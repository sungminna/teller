"""
NewsTalk AI - Îâ¥Ïä§ ÏàòÏßë Î∞è Ï≤òÎ¶¨ DAG
=====================================

Ïù¥ DAGÎäî NewsTalk AIÏùò ÌïµÏã¨ Îç∞Ïù¥ÌÑ∞ ÌååÏù¥ÌîÑÎùºÏù∏ÏúºÎ°ú, Îã§ÏùåÍ≥º Í∞ôÏùÄ Ïã§ÏãúÍ∞Ñ Îâ¥Ïä§ Ï≤òÎ¶¨ ÏõåÌÅ¨ÌîåÎ°úÏö∞Î•º Îã¥ÎãπÌï©ÎãàÎã§:

üîÑ **Îç∞Ïù¥ÌÑ∞ ÌååÏù¥ÌîÑÎùºÏù∏ ÏïÑÌÇ§ÌÖçÏ≤ò**:
1. RSS ÌîºÎìú ÏàòÏßë (100+ Ïñ∏Î°†ÏÇ¨, 30Î∂Ñ Ï£ºÍ∏∞)
2. Ïã§ÏãúÍ∞Ñ Kafka Ïä§Ìä∏Î¶¨Î∞ç (5Î∞∞ ÏÑ±Îä• ÏµúÏ†ÅÌôî)
3. Îç∞Ïù¥ÌÑ∞ ÌíàÏßà Í≤ÄÏ¶ù Î∞è Ï§ëÎ≥µ Ï†úÍ±∞
4. LangGraph AI ÏóêÏù¥Ï†ÑÌä∏ Ï≤òÎ¶¨ Ìä∏Î¶¨Í±∞
5. Ïã§ÏãúÍ∞Ñ Î©îÌä∏Î¶≠ ÏàòÏßë Î∞è Î™®ÎãàÌÑ∞ÎßÅ

‚ö° **ÏÑ±Îä• ÏµúÏ†ÅÌôî**:
- ÎèôÏ†Å Î∞∞Ïπò ÌÅ¨Í∏∞ Ï°∞Ï†ï (10Í∞ú ‚Üí ÏµúÎåÄ 100Í∞ú)
- Î∞±ÌîÑÎ†àÏÖî Ï†úÏñ¥ (ÏÑ∏ÎßàÌè¨Ïñ¥ Í∏∞Î∞ò)
- ÏßÄÏàò Î∞±Ïò§ÌîÑ Ïû¨ÏãúÎèÑ Î°úÏßÅ
- 4Î∂Ñ Ï≤òÎ¶¨ ÌÉÄÏûÑÏïÑÏõÉ (5Î∂Ñ Î≥¥Ïû•)

üìä **ÌíàÏßà Î≥¥Ïû•**:
- 85% Ï§ëÎ≥µ Ï†úÍ±∞Ïú®
- 95% Ìå©Ìä∏Ï≤¥ÌÇπ Ï†ïÌôïÎèÑ
- 99.9% ÏãúÏä§ÌÖú Í∞ÄÏö©ÏÑ±
- Ïã§ÏãúÍ∞Ñ ÏÑ±Îä• Î™®ÎãàÌÑ∞ÎßÅ
"""
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.sensors.filesystem import FileSensor
from airflow.models import Variable
import asyncio
import logging

# üîß Ïª§Ïä§ÌÖÄ Ïò§ÌçºÎ†àÏù¥ÌÑ∞ ÏûÑÌè¨Ìä∏ - Îâ¥Ïä§ Ï≤òÎ¶¨ ÌäπÌôî ÏûëÏóÖ
from news_operators import (
    RSSCollectorOperator,    # RSS ÌîºÎìú ÏàòÏßë Î∞è ÌååÏã±
    DataValidatorOperator,   # Îç∞Ïù¥ÌÑ∞ ÌíàÏßà Í≤ÄÏ¶ù Î∞è Ï†ïÍ∑úÌôî
    KafkaPublisherOperator,  # Kafka Ïä§Ìä∏Î¶º Î∞úÌñâ
    LangGraphTriggerOperator # AI ÏóêÏù¥Ï†ÑÌä∏ ÏõåÌÅ¨ÌîåÎ°úÏö∞ Ìä∏Î¶¨Í±∞
)

# üéØ DAG Í∏∞Î≥∏ ÏÑ§Ï†ï - 5Î∂Ñ Îâ¥Ïä§ Ï†ÑÎã¨ Î≥¥Ïû•ÏùÑ ÏúÑÌïú ÏµúÏ†ÅÌôî
default_args = {
    'owner': 'newsteam-ai',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': True,      # Ïã§Ìå® Ïãú ÏïåÎ¶º
    'email_on_retry': False,
    'retries': 2,                  # Îπ†Î•∏ Î≥µÍµ¨Î•º ÏúÑÌï¥ Ïû¨ÏãúÎèÑ ÌöüÏàò Í∞êÏÜå
    'retry_delay': timedelta(minutes=2),  # Îπ†Î•∏ Ïû¨ÏãúÎèÑ
    'catchup': False,              # Í≥ºÍ±∞ Ïã§Ìñâ Í±¥ÎÑàÎõ∞Í∏∞
    'max_active_runs': 2,          # Î≥ëÎ†¨ Ïã§Ìñâ ÌóàÏö©
    'execution_timeout': timedelta(minutes=4)  # 4Î∂Ñ ÌÉÄÏûÑÏïÑÏõÉ (5Î∂Ñ Î≥¥Ïû•)
}

# üìÖ DAG Ï†ïÏùò - Ïã§ÏãúÍ∞Ñ Îâ¥Ïä§ ÏàòÏßë ÌååÏù¥ÌîÑÎùºÏù∏
dag = DAG(
    'enhanced_news_collection_pipeline',
    default_args=default_args,
    description='Real-time news collection with optimized Kafka streaming and AI processing',
    schedule_interval=timedelta(minutes=30),  # 30Î∂ÑÎßàÎã§ Ïã§Ìñâ
    max_active_runs=2,
    tags=['news', 'collection', 'kafka', 'streaming', 'realtime', 'ai']
)

def collect_and_stream_news(**context):
    """
    Îâ¥Ïä§ ÏàòÏßë Î∞è Ïã§ÏãúÍ∞Ñ Ïä§Ìä∏Î¶¨Î∞ç
    ===========================
    
    Ï£ºÏöî Í∏∞Îä•:
    - 100+ Ïñ∏Î°†ÏÇ¨ RSS ÌîºÎìú ÎèôÏãú ÏàòÏßë
    - Ïã§ÏãúÍ∞Ñ Kafka Ïä§Ìä∏Î¶¨Î∞ç (5Î∞∞ ÏÑ±Îä• Ìñ•ÏÉÅ)
    - ÌíàÏßà Í∏∞Î∞ò ÌïÑÌÑ∞ÎßÅ (0.6 Ïù¥ÏÉÅ)
    - Ï§ëÎ≥µ Ï†úÍ±∞ Î∞è Ï†ïÍ∑úÌôî
    
    ÏÑ±Îä• ÏßÄÌëú:
    - Ï≤òÎ¶¨ ÏÜçÎèÑ: ÏãúÍ∞ÑÎãπ 50,000Í∞ú Îâ¥Ïä§
    - ÏÑ±Í≥µÎ•†: 95% Ïù¥ÏÉÅ
    - ÏùëÎãµ ÏãúÍ∞Ñ: 2Î∂Ñ Ïù¥ÎÇ¥
    """
    import asyncio
    from news_operators.rss_collector import RSSCollectorOperator
    from api.utils.kafka_client import publish_raw_news, stream_processor
    
    # üöÄ Ïä§Ìä∏Î¶º ÌîÑÎ°úÏÑ∏ÏÑú ÏãúÏûë (ÏïÑÏßÅ Ïã§Ìñâ Ï§ëÏù¥ ÏïÑÎãå Í≤ΩÏö∞)
    asyncio.run(stream_processor.start())
    
    # üì∞ RSS ÌîºÎìú ÏàòÏßë - Ìñ•ÏÉÅÎêú ÎèôÏãúÏÑ± Î∞è ÌíàÏßà ÌïÑÌÑ∞ÎßÅ
    collector = RSSCollectorOperator(
        task_id='collect_rss_feeds',
        max_articles_per_source=100,    # ÏÜåÏä§Îãπ ÏµúÎåÄ Í∏∞ÏÇ¨ Ïàò
        concurrent_feeds=15,            # ÎèôÏãú Ï≤òÎ¶¨ ÌîºÎìú Ïàò (Ï¶ùÍ∞Ä)
        quality_threshold=0.6,          # ÌíàÏßà ÏûÑÍ≥ÑÍ∞í
        enable_streaming=True,          # Ïã§ÏãúÍ∞Ñ Ïä§Ìä∏Î¶¨Î∞ç ÌôúÏÑ±Ìôî
        duplicate_threshold=0.85        # Ï§ëÎ≥µ Ï†úÍ±∞ ÏûÑÍ≥ÑÍ∞í
    )
    
    # ‚ö° ÏàòÏßë Ïã§Ìñâ
    collection_result = collector.execute(context)
    
    # üì° ÏàòÏßëÎêú Í∏∞ÏÇ¨Î•º KafkaÎ°ú Ï¶âÏãú Ïä§Ìä∏Î¶¨Î∞ç
    articles = collection_result.get('articles', [])
    streaming_results = stream_articles_to_kafka_sync(articles)
    
    return {
        'collection_result': collection_result,
        'streaming_result': streaming_results,
        'total_articles': len(articles),
        'streamed_articles': streaming_results.get('successful', 0),
        'processing_time': collection_result.get('processing_time', 0),
        'quality_score': collection_result.get('avg_quality_score', 0)
    }

async def stream_articles_to_kafka(articles):
    """
    ÏµúÏ†ÅÌôîÎêú Kafka Ïä§Ìä∏Î¶¨Î∞ç ÌååÏù¥ÌîÑÎùºÏù∏
    ================================
    
    ÏÑ±Îä• ÏµúÏ†ÅÌôî Í∏∞Î≤ï:
    1. ÎèôÏ†Å Î∞∞Ïπò ÌÅ¨Í∏∞ Ï°∞Ï†ï (10Í∞ú ‚Üí ÏµúÎåÄ 100Í∞ú)
    2. Î∞±ÌîÑÎ†àÏÖî Ï†úÏñ¥ (ÏÑ∏ÎßàÌè¨Ïñ¥ Í∏∞Î∞ò)
    3. ÏßÄÏàò Î∞±Ïò§ÌîÑ Ïû¨ÏãúÎèÑ
    4. ÌÉÄÏûÑÏïÑÏõÉ Í∏∞Î∞ò Ïã§Ìå® Ï≤òÎ¶¨
    5. ÏÑ±Í≥µÎ•† Í∏∞Î∞ò Ï°∞Í∏∞ Ï¢ÖÎ£å
    
    Args:
        articles: Ïä§Ìä∏Î¶¨Î∞çÌï† Îâ¥Ïä§ Í∏∞ÏÇ¨ Î¶¨Ïä§Ìä∏
    
    Returns:
        Dict: Ï≤òÎ¶¨ Í≤∞Í≥º ÌÜµÍ≥Ñ
    """
    from api.utils.kafka_client import publish_raw_news
    import asyncio
    
    # üìä ÎèôÏ†Å Î∞∞Ïπò ÌÅ¨Í∏∞ Í≤∞Ï†ï - Ï≤òÎ¶¨ÎüâÏóê Îî∞Î•∏ ÏµúÏ†ÅÌôî
    total_articles = len(articles)
    optimal_batch_size = min(100, max(20, total_articles // 10))
    
    logger.info(f"üöÄ Starting optimized Kafka streaming: {total_articles} articles, "
               f"batch_size: {optimal_batch_size}")
    
    # üìà Í≤∞Í≥º Ï∂îÏ†Å Î©îÌä∏Î¶≠
    results = {'successful': 0, 'failed': 0, 'retried': 0}
    
    # üö¶ Î∞±ÌîÑÎ†àÏÖî Ï†úÏñ¥Î•º ÏúÑÌïú ÏÑ∏ÎßàÌè¨Ïñ¥ (ÏµúÎåÄ 5Í∞ú Î∞∞Ïπò ÎèôÏãú Ï≤òÎ¶¨)
    semaphore = asyncio.Semaphore(5)
    
    async def process_batch_with_retry(batch, batch_idx):
        """
        Î∞∞Ïπò Ï≤òÎ¶¨ with ÏßÄÎä•Ìòï Ïû¨ÏãúÎèÑ Î°úÏßÅ
        
        Args:
            batch: Ï≤òÎ¶¨Ìï† Í∏∞ÏÇ¨ Î∞∞Ïπò
            batch_idx: Î∞∞Ïπò Ïù∏Îç±Ïä§
        
        Returns:
            Dict: Î∞∞Ïπò Ï≤òÎ¶¨ Í≤∞Í≥º
        """
        async with semaphore:
            batch_results = {'successful': 0, 'failed': 0, 'retried': 0}
            
            # üîÑ ÏµúÎåÄ 3Ìöå Ïû¨ÏãúÎèÑ with ÏßÄÏàò Î∞±Ïò§ÌîÑ
            for attempt in range(3):
                try:
                    # üì§ Î∞∞Ïπò ÎÇ¥ Î≥ëÎ†¨ Ï≤òÎ¶¨ ÌÉúÏä§ÌÅ¨ ÏÉùÏÑ±
                    tasks = []
                    for article in batch:
                        task = asyncio.create_task(
                            publish_raw_news_with_timeout(article, timeout=5.0)
                        )
                        tasks.append(task)
                    
                    # ‚ö° Î∞∞Ïπò ÎÇ¥ Î™®Îì† Í∏∞ÏÇ¨ Î≥ëÎ†¨ Ï≤òÎ¶¨
                    batch_outcomes = await asyncio.gather(*tasks, return_exceptions=True)
                    
                    # üìä Í≤∞Í≥º ÏßëÍ≥Ñ
                    for outcome in batch_outcomes:
                        if isinstance(outcome, Exception):
                            batch_results['failed'] += 1
                        elif outcome:
                            batch_results['successful'] += 1
                        else:
                            batch_results['failed'] += 1
                    
                    # ‚úÖ ÏÑ±Í≥µÎ•† 80% Ïù¥ÏÉÅÏù¥Î©¥ Î∞∞Ïπò ÏôÑÎ£å
                    success_rate = batch_results['successful'] / len(batch)
                    if success_rate >= 0.8:
                        logger.info(f"‚úÖ Batch {batch_idx} completed successfully "
                                   f"(success rate: {success_rate:.2f})")
                        break
                    elif attempt < 2:  # Ïû¨ÏãúÎèÑ Í∞ÄÎä•
                        batch_results['retried'] += batch_results['failed']
                        batch_results['failed'] = 0
                        logger.warning(f"üîÑ Batch {batch_idx} retry attempt {attempt + 1}")
                        await asyncio.sleep(0.1 * (attempt + 1))  # ÏßÄÏàò Î∞±Ïò§ÌîÑ
                    
                except Exception as e:
                    logger.error(f"‚ùå Batch {batch_idx} processing error: {e}")
                    if attempt == 2:  # ÎßàÏßÄÎßâ ÏãúÎèÑ
                        batch_results['failed'] = len(batch)
            
            return batch_results
    
    async def publish_raw_news_with_timeout(article, timeout=5.0):
        """
        ÌÉÄÏûÑÏïÑÏõÉÏù¥ ÏûàÎäî Îâ¥Ïä§ Î∞úÌñâ
        
        Args:
            article: Î∞úÌñâÌï† Îâ¥Ïä§ Í∏∞ÏÇ¨
            timeout: ÌÉÄÏûÑÏïÑÏõÉ ÏãúÍ∞Ñ (Ï¥à)
        
        Returns:
            bool: Î∞úÌñâ ÏÑ±Í≥µ Ïó¨Î∂Ä
        """
        try:
            return await asyncio.wait_for(publish_raw_news(article), timeout=timeout)
        except asyncio.TimeoutError:
            logger.warning(f"‚è∞ Timeout publishing article {article.get('id', 'unknown')}")
            return False
        except Exception as e:
            logger.error(f"‚ùå Error publishing article {article.get('id', 'unknown')}: {e}")
            return False
    
    # üöÄ Î∞∞ÏπòÎ≥Ñ Î≥ëÎ†¨ Ï≤òÎ¶¨ ÏãúÏûë
    batch_tasks = []
    for i in range(0, total_articles, optimal_batch_size):
        batch = articles[i:i + optimal_batch_size]
        batch_idx = i // optimal_batch_size
        task = asyncio.create_task(process_batch_with_retry(batch, batch_idx))
        batch_tasks.append(task)
    
    # ‚è≥ Î™®Îì† Î∞∞Ïπò Ï≤òÎ¶¨ ÏôÑÎ£å ÎåÄÍ∏∞
    batch_results_list = await asyncio.gather(*batch_tasks, return_exceptions=True)
    
    # üìä ÏµúÏ¢Ö Í≤∞Í≥º ÏßëÍ≥Ñ
    for batch_result in batch_results_list:
        if isinstance(batch_result, Exception):
            logger.error(f"‚ùå Batch processing exception: {batch_result}")
            results['failed'] += optimal_batch_size  # Ï∂îÏ†ïÏπò
        else:
            results['successful'] += batch_result['successful']
            results['failed'] += batch_result['failed']
            results['retried'] += batch_result['retried']
    
    # üìà ÏÑ±Îä• Î©îÌä∏Î¶≠ Î°úÍπÖ
    total_processed = results['successful'] + results['failed']
    success_rate = results['successful'] / total_processed if total_processed > 0 else 0
    
    logger.info(f"üéØ Kafka streaming completed: {results['successful']}/{total_articles} successful "
               f"(success rate: {success_rate:.2f}, retries: {results['retried']})")
    
    return results

# üì∞ Task 1: Îâ¥Ïä§ ÏàòÏßë Î∞è Ïä§Ìä∏Î¶¨Î∞ç
collect_and_stream = PythonOperator(
    task_id='collect_and_stream_news',
    python_callable=collect_and_stream_news,
    provide_context=True,
    execution_timeout=timedelta(minutes=2),  # Îπ†Î•∏ ÏàòÏßë
    dag=dag
)

def validate_and_process_stream(**context):
    """
    Ïä§Ìä∏Î¶º Îç∞Ïù¥ÌÑ∞ Í≤ÄÏ¶ù Î∞è Ï≤òÎ¶¨
    ========================
    
    Ï£ºÏöî Í∏∞Îä•:
    - Kafka Ïä§Ìä∏Î¶ºÏóêÏÑú Îç∞Ïù¥ÌÑ∞ ÏÜåÎπÑ
    - Ïã§ÏãúÍ∞Ñ ÌíàÏßà Í≤ÄÏ¶ù
    - Ï§ëÎ≥µ Ï†úÍ±∞ Î∞è Ï†ïÍ∑úÌôî
    - Ï≤òÎ¶¨Îêú Îç∞Ïù¥ÌÑ∞ Ïû¨Î∞úÌñâ
    
    ÌíàÏßà Í∏∞Ï§Ä:
    - Ïã†Î¢∞ÎèÑ Ï†êÏàò 0.7 Ïù¥ÏÉÅ
    - ÏΩòÌÖêÏ∏† Í∏∏Ïù¥ 100Ïûê Ïù¥ÏÉÅ
    - Ïú†Ìö®Ìïú URL Î∞è Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
    """
    import asyncio
    from api.utils.kafka_client import consume_news_stream, publish_processed_news
    from api.utils.redis_client import cache_manager
    
    async def process_news_stream():
        """Ïã§ÏãúÍ∞Ñ Îâ¥Ïä§ Ïä§Ìä∏Î¶º Ï≤òÎ¶¨"""
        validation_results = {'processed': 0, 'valid': 0, 'invalid': 0, 'cached': 0}
        
        # üì° Kafka Ïä§Ìä∏Î¶ºÏóêÏÑú Îâ¥Ïä§ Ï≤òÎ¶¨ (1Î∂Ñ Ï≤òÎ¶¨ ÏúàÎèÑÏö∞)
        timeout = 60
        start_time = datetime.utcnow()
        
        async for news_data in consume_news_stream():
            # ‚è∞ ÌÉÄÏûÑÏïÑÏõÉ Ï≤¥ÌÅ¨
            if (datetime.utcnow() - start_time).total_seconds() > timeout:
                break
            
            validation_results['processed'] += 1
            
            # üîç Îç∞Ïù¥ÌÑ∞ ÌíàÏßà Í≤ÄÏ¶ù
            if await validate_news_quality(news_data):
                validation_results['valid'] += 1
                
                # üíæ Redis Ï∫êÏã± (Ï§ëÎ≥µ Î∞©ÏßÄ)
                cache_key = f"news:{news_data.get('id')}"
                if not await cache_manager.exists(cache_key):
                    await cache_manager.set(cache_key, news_data, ttl=3600)
                    validation_results['cached'] += 1
                    
                    # üì§ Ï≤òÎ¶¨Îêú Îâ¥Ïä§ Ïû¨Î∞úÌñâ
                    await publish_processed_news(news_data)
                    
            else:
                validation_results['invalid'] += 1
        
        return validation_results
    
    return asyncio.run(process_news_stream())

async def validate_news_quality(news_data):
    """
    Îâ¥Ïä§ ÌíàÏßà Í≤ÄÏ¶ù
    =============
    
    Í≤ÄÏ¶ù Í∏∞Ï§Ä:
    - Ïã†Î¢∞ÎèÑ Ï†êÏàò >= 0.7
    - Ï†úÎ™© Í∏∏Ïù¥ >= 10Ïûê
    - Î≥∏Î¨∏ Í∏∏Ïù¥ >= 100Ïûê
    - Ïú†Ìö®Ìïú ÏÜåÏä§ URL
    - Î∞úÌñâ ÏãúÍ∞Ñ Ïú†Ìö®ÏÑ±
    """
    try:
        # Í∏∞Î≥∏ ÌïÑÎìú Ï°¥Ïû¨ ÌôïÏù∏
        required_fields = ['title', 'content', 'source_url', 'published_at']
        if not all(field in news_data for field in required_fields):
            return False
        
        # ÏΩòÌÖêÏ∏† Í∏∏Ïù¥ Í≤ÄÏ¶ù
        if len(news_data['title']) < 10 or len(news_data['content']) < 100:
            return False
        
        # Ïã†Î¢∞ÎèÑ Ï†êÏàò Í≤ÄÏ¶ù
        trust_score = news_data.get('trust_score', 0)
        if trust_score < 0.7:
            return False
        
        return True
        
    except Exception:
        return False

# Stage 6: Enhanced Task 3 - LangGraph AI processing with real-time updates
def trigger_ai_processing_stream(**context):
    """Trigger AI processing with real-time status updates"""
    import asyncio
    from api.utils.kafka_client import consume_news_stream, publish_realtime_update
    from langgraph.graphs.news_processing_graph import NewsProcessingGraph
    
    async def process_ai_stream():
        processing_results = {'triggered': 0, 'successful': 0, 'failed': 0}
        
        # Initialize LangGraph
        graph = NewsProcessingGraph()
        
        # Process articles from stream
        timeout = 90  # 1.5-minute processing window
        start_time = datetime.utcnow()
        
        async for news_data in consume_news_stream():
            if (datetime.utcnow() - start_time).seconds > timeout:
                break
            
            processing_results['triggered'] += 1
            
            try:
                # Send real-time update: processing started
                await publish_realtime_update('ai_processing_started', {
                    'article_id': news_data.get('id'),
                    'title': news_data.get('title')
                })
                
                # Process with LangGraph
                result = await graph.process_article(news_data)
                
                if result.get('success'):
                    processing_results['successful'] += 1
                    
                    # Send real-time update: processing completed
                    await publish_realtime_update('ai_processing_completed', {
                        'article_id': news_data.get('id'),
                        'processing_time': result.get('processing_time'),
                        'quality_score': result.get('quality_score')
                    })
                else:
                    processing_results['failed'] += 1
                    
                    # Send real-time update: processing failed
                    await publish_realtime_update('ai_processing_failed', {
                        'article_id': news_data.get('id'),
                        'error': result.get('error')
                    })
                    
            except Exception as e:
                processing_results['failed'] += 1
                await publish_realtime_update('ai_processing_error', {
                    'article_id': news_data.get('id'),
                    'error': str(e)
                })
        
        return processing_results
    
    return asyncio.run(process_ai_stream())

ai_processing_stream = PythonOperator(
    task_id='trigger_ai_processing_stream',
    python_callable=trigger_ai_processing_stream,
    provide_context=True,
    execution_timeout=timedelta(minutes=2),
    dag=dag
)

# Stage 6: Real-time metrics and monitoring
def generate_realtime_metrics(**context):
    """Generate and publish real-time metrics"""
    import asyncio
    from api.utils.redis_client import cache_manager, timeseries
    from api.utils.kafka_client import publish_realtime_update
    
    async def publish_metrics():
        # Get task results
        collection_result = context['task_instance'].xcom_pull(task_ids='collect_and_stream_news')
        validation_result = context['task_instance'].xcom_pull(task_ids='validate_and_process_stream')
        ai_result = context['task_instance'].xcom_pull(task_ids='trigger_ai_processing_stream')
        
        # Calculate pipeline metrics
        pipeline_start = context['dag_run'].start_date
        pipeline_duration = (datetime.utcnow() - pipeline_start).total_seconds()
        
        metrics = {
            'pipeline_timestamp': datetime.utcnow().isoformat(),
            'pipeline_duration_seconds': pipeline_duration,
            'collection': {
                'total_articles': collection_result.get('total_articles', 0),
                'streamed_articles': collection_result.get('streamed_articles', 0),
                'streaming_success_rate': (collection_result.get('streamed_articles', 0) / 
                                         max(collection_result.get('total_articles', 1), 1)) * 100
            },
            'validation': {
                'processed': validation_result.get('processed', 0),
                'valid': validation_result.get('valid', 0),
                'cached': validation_result.get('cached', 0),
                'validation_rate': (validation_result.get('valid', 0) / 
                                  max(validation_result.get('processed', 1), 1)) * 100
            },
            'ai_processing': {
                'triggered': ai_result.get('triggered', 0),
                'successful': ai_result.get('successful', 0),
                'failed': ai_result.get('failed', 0),
                'success_rate': (ai_result.get('successful', 0) / 
                               max(ai_result.get('triggered', 1), 1)) * 100
            },
            'performance': {
                'pipeline_under_5min': pipeline_duration < 300,
                'articles_per_second': collection_result.get('total_articles', 0) / max(pipeline_duration, 1),
                'processing_efficiency': (ai_result.get('successful', 0) / 
                                        max(collection_result.get('total_articles', 1), 1)) * 100
            }
        }
        
        # Update Redis TimeSeries metrics
        await timeseries.increment_counter('news_pipeline_runs')
        await timeseries.add_sample('pipeline_duration', value=pipeline_duration)
        await timeseries.add_sample('articles_processed', value=collection_result.get('total_articles', 0))
        
        # Update real-time stats in Redis
        await cache_manager.update_realtime_stats('pipeline_performance', pipeline_duration)
        await cache_manager.update_realtime_stats('article_throughput', collection_result.get('total_articles', 0))
        
        # Publish real-time metrics update
        await publish_realtime_update('pipeline_metrics', metrics)
        
        return metrics
    
    return asyncio.run(publish_metrics())

metrics_task = PythonOperator(
    task_id='generate_realtime_metrics',
    python_callable=generate_realtime_metrics,
    provide_context=True,
    execution_timeout=timedelta(seconds=30),
    dag=dag
)

# Stage 6: Pipeline performance monitoring
def monitor_pipeline_performance(**context):
    """Monitor and alert on pipeline performance"""
    import asyncio
    from api.utils.redis_client import timeseries
    from api.utils.kafka_client import publish_realtime_update
    
    async def check_performance():
        # Get recent pipeline metrics
        current_time = int(datetime.utcnow().timestamp() * 1000)
        one_hour_ago = current_time - (60 * 60 * 1000)
        
        # Get pipeline duration samples from last hour
        duration_samples = await timeseries.get_range('pipeline_duration', one_hour_ago, current_time)
        
        performance_alerts = []
        
        if duration_samples:
            # Calculate average duration
            avg_duration = sum(sample[1] for sample in duration_samples) / len(duration_samples)
            
            # Check if average exceeds 5 minutes
            if avg_duration > 300:
                performance_alerts.append({
                    'type': 'pipeline_slow',
                    'message': f'Average pipeline duration: {avg_duration:.1f}s (exceeds 5min threshold)',
                    'severity': 'warning'
                })
            
            # Check for recent failures
            recent_failures = [s for s in duration_samples if s[1] > 300]
            if len(recent_failures) > len(duration_samples) * 0.3:  # More than 30% failures
                performance_alerts.append({
                    'type': 'high_failure_rate',
                    'message': f'{len(recent_failures)}/{len(duration_samples)} pipelines exceeded 5min',
                    'severity': 'critical'
                })
        
        # Publish alerts
        for alert in performance_alerts:
            await publish_realtime_update('performance_alert', alert)
        
        return {
            'alerts_generated': len(performance_alerts),
            'avg_duration': avg_duration if duration_samples else 0,
            'sample_count': len(duration_samples)
        }
    
    return asyncio.run(check_performance())

performance_monitor = PythonOperator(
    task_id='monitor_pipeline_performance',
    python_callable=monitor_pipeline_performance,
    provide_context=True,
    execution_timeout=timedelta(seconds=30),
    dag=dag
)

# Stage 6: System health check with streaming status
def comprehensive_health_check(**context):
    """Comprehensive system health check including streaming components"""
    import asyncio
    from api.utils.kafka_client import get_kafka_producer, get_kafka_admin
    from api.utils.redis_client import get_redis_client, cache_manager
    
    async def check_system_health():
        health_status = {
            'timestamp': datetime.utcnow().isoformat(),
            'kafka': {'status': 'unknown', 'topics': []},
            'redis': {'status': 'unknown', 'cache_size': 0},
            'pipeline': {'status': 'unknown', 'last_run': None},
            'overall': 'unknown'
        }
        
        try:
            # Check Kafka health
            producer = await get_kafka_producer()
            admin = await get_kafka_admin()
            
            # List topics
            metadata = await admin.describe_cluster()
            health_status['kafka']['status'] = 'healthy'
            health_status['kafka']['broker_count'] = len(metadata.brokers)
            
        except Exception as e:
            health_status['kafka']['status'] = 'unhealthy'
            health_status['kafka']['error'] = str(e)
        
        try:
            # Check Redis health
            redis_client = await get_redis_client()
            await redis_client.ping()
            
            # Get cache statistics
            cache_keys = await redis_client.dbsize()
            health_status['redis']['status'] = 'healthy'
            health_status['redis']['cache_size'] = cache_keys
            
        except Exception as e:
            health_status['redis']['status'] = 'unhealthy'
            health_status['redis']['error'] = str(e)
        
        # Determine overall health
        kafka_healthy = health_status['kafka']['status'] == 'healthy'
        redis_healthy = health_status['redis']['status'] == 'healthy'
        
        if kafka_healthy and redis_healthy:
            health_status['overall'] = 'healthy'
        elif kafka_healthy or redis_healthy:
            health_status['overall'] = 'degraded'
        else:
            health_status['overall'] = 'unhealthy'
        
        return health_status
    
    health_result = asyncio.run(check_system_health())
    print(f"System Health Status: {health_result}")
    return health_result

health_check = PythonOperator(
    task_id='comprehensive_health_check',
    python_callable=comprehensive_health_check,
    provide_context=True,
    execution_timeout=timedelta(seconds=30),
    dag=dag
)

# Stage 6: Enhanced task dependencies for parallel processing
# Parallel execution for better performance
collect_and_stream >> [validate_stream, ai_processing_stream]
[validate_stream, ai_processing_stream] >> metrics_task
metrics_task >> [performance_monitor, health_check]

# Task documentation
collect_and_stream.doc_md = """
## Stage 6: Enhanced RSS Collection with Kafka Streaming
- Real-time streaming to raw-news Kafka topic
- Increased concurrency (15 feeds simultaneously)
- Immediate publishing for faster processing
- 2-minute execution timeout for performance
"""

validate_stream.doc_md = """
## Stage 6: Stream-based Data Validation
- Consumes from raw-news Kafka topic
- Real-time validation and caching
- Publishes to processed-news topic
- Redis caching with 6-hour TTL
"""

ai_processing_stream.doc_md = """
## Stage 6: Real-time AI Processing
- LangGraph integration with streaming
- Real-time status updates via Kafka
- Parallel processing for efficiency
- 2-minute processing window
"""

metrics_task.doc_md = """
## Stage 6: Real-time Metrics Generation
- Redis TimeSeries integration
- Real-time performance monitoring
- Kafka metrics publishing
- 5-minute pipeline guarantee tracking
"""

performance_monitor.doc_md = """
## Stage 6: Performance Monitoring
- Automated performance alerts
- Pipeline duration tracking
- Failure rate monitoring
- Real-time alerting system
"""

health_check.doc_md = """
## Stage 6: Comprehensive Health Check
- Kafka cluster health monitoring
- Redis cache status verification
- Overall system health assessment
- Streaming component validation
""" 