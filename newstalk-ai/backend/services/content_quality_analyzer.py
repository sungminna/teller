"""
ğŸ“Š Content Quality Analyzer Service
===================================

ë‰´ìŠ¤ ì½˜í…ì¸ ì˜ í’ˆì§ˆì„ ë¶„ì„í•˜ëŠ” ì„œë¹„ìŠ¤
"""

import logging
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
from datetime import datetime

logger = logging.getLogger(__name__)


@dataclass
class QualityMetrics:
    """í’ˆì§ˆ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤"""
    
    overall_score: float
    readability_score: float
    factual_accuracy: float
    bias_score: float
    source_credibility: float
    freshness_score: float
    engagement_potential: float
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "overall_score": self.overall_score,
            "readability_score": self.readability_score,
            "factual_accuracy": self.factual_accuracy,
            "bias_score": self.bias_score,
            "source_credibility": self.source_credibility,
            "freshness_score": self.freshness_score,
            "engagement_potential": self.engagement_potential
        }


class ContentQualityAnalyzer:
    """ì½˜í…ì¸  í’ˆì§ˆ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.quality_threshold = 0.7
        self.bias_threshold = 0.3
        self.credible_sources = {
            "reuters.com": 0.95,
            "bbc.com": 0.90,
            "cnn.com": 0.85,
            "nytimes.com": 0.90,
            "washingtonpost.com": 0.88
        }
        
    async def analyze_content_quality(self, content: Dict[str, Any]) -> QualityMetrics:
        """
        ì½˜í…ì¸  í’ˆì§ˆ ë¶„ì„
        
        Args:
            content: ë¶„ì„í•  ì½˜í…ì¸  ë°ì´í„°
            
        Returns:
            QualityMetrics: í’ˆì§ˆ ë©”íŠ¸ë¦­ ê²°ê³¼
        """
        try:
            # ê¸°ë³¸ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            readability = self._calculate_readability(content.get("content", ""))
            factual_accuracy = await self._assess_factual_accuracy(content)
            bias_score = self._analyze_bias(content.get("content", ""))
            source_credibility = self._evaluate_source_credibility(content.get("source", ""))
            freshness = self._calculate_freshness(content.get("published_at"))
            engagement = self._predict_engagement(content)
            
            # ì „ì²´ ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
            overall_score = (
                readability * 0.15 +
                factual_accuracy * 0.25 +
                (1 - bias_score) * 0.20 +  # í¸í–¥ì„±ì´ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
                source_credibility * 0.20 +
                freshness * 0.10 +
                engagement * 0.10
            )
            
            metrics = QualityMetrics(
                overall_score=overall_score,
                readability_score=readability,
                factual_accuracy=factual_accuracy,
                bias_score=bias_score,
                source_credibility=source_credibility,
                freshness_score=freshness,
                engagement_potential=engagement
            )
            
            logger.info(f"Content quality analysis completed. Overall score: {overall_score:.3f}")
            return metrics
            
        except Exception as e:
            logger.error(f"Content quality analysis failed: {str(e)}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return QualityMetrics(
                overall_score=0.5,
                readability_score=0.5,
                factual_accuracy=0.5,
                bias_score=0.5,
                source_credibility=0.5,
                freshness_score=0.5,
                engagement_potential=0.5
            )
    
    def _calculate_readability(self, content: str) -> float:
        """
        ê°€ë…ì„± ì ìˆ˜ ê³„ì‚°
        
        Args:
            content: ì½˜í…ì¸  í…ìŠ¤íŠ¸
            
        Returns:
            float: ê°€ë…ì„± ì ìˆ˜ (0-1)
        """
        if not content:
            return 0.0
            
        # ê°„ë‹¨í•œ ê°€ë…ì„± ë©”íŠ¸ë¦­
        words = content.split()
        sentences = content.split('.')
        
        if len(sentences) == 0:
            return 0.0
            
        avg_words_per_sentence = len(words) / len(sentences)
        avg_word_length = sum(len(word) for word in words) / len(words) if words else 0
        
        # ê°€ë…ì„± ì ìˆ˜ ê³„ì‚° (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        readability = max(0, min(1, 1 - (avg_words_per_sentence - 15) / 20))
        readability *= max(0, min(1, 1 - (avg_word_length - 5) / 5))
        
        return readability
    
    async def _assess_factual_accuracy(self, content: Dict[str, Any]) -> float:
        """
        íŒ©íŠ¸ ì •í™•ì„± í‰ê°€
        
        Args:
            content: ì½˜í…ì¸  ë°ì´í„°
            
        Returns:
            float: íŒ©íŠ¸ ì •í™•ì„± ì ìˆ˜ (0-1)
        """
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì™¸ë¶€ íŒ©íŠ¸ì²´í‚¹ API ì‚¬ìš©
        # í˜„ì¬ëŠ” ëª¨ì˜ ì ìˆ˜ ë°˜í™˜
        
        text = content.get("content", "")
        
        # ê¸°ë³¸ì ì¸ íŒ©íŠ¸ì²´í‚¹ íœ´ë¦¬ìŠ¤í‹±
        fact_indicators = [
            "according to", "statistics show", "research indicates",
            "study found", "data reveals", "official report"
        ]
        
        fact_score = sum(1 for indicator in fact_indicators if indicator in text.lower())
        max_score = len(fact_indicators)
        
        # ì •ê·œí™” ë° ê¸°ë³¸ ì ìˆ˜ ì¶”ê°€
        normalized_score = (fact_score / max_score) * 0.3 + 0.7
        
        return min(1.0, normalized_score)
    
    def _analyze_bias(self, content: str) -> float:
        """
        í¸í–¥ì„± ë¶„ì„
        
        Args:
            content: ì½˜í…ì¸  í…ìŠ¤íŠ¸
            
        Returns:
            float: í¸í–¥ì„± ì ìˆ˜ (0-1, ë†’ì„ìˆ˜ë¡ í¸í–¥ì )
        """
        if not content:
            return 0.5
            
        # í¸í–¥ì„± ì§€í‘œ ë‹¨ì–´ë“¤
        bias_indicators = [
            "obviously", "clearly", "undoubtedly", "without question",
            "everyone knows", "it's obvious", "definitely", "absolutely"
        ]
        
        emotional_words = [
            "shocking", "outrageous", "incredible", "unbelievable",
            "devastating", "amazing", "terrible", "wonderful"
        ]
        
        content_lower = content.lower()
        bias_count = sum(1 for indicator in bias_indicators if indicator in content_lower)
        emotional_count = sum(1 for word in emotional_words if word in content_lower)
        
        total_words = len(content.split())
        if total_words == 0:
            return 0.5
            
        bias_ratio = (bias_count + emotional_count) / total_words
        
        # í¸í–¥ì„± ì ìˆ˜ ê³„ì‚° (0-1, ë†’ì„ìˆ˜ë¡ í¸í–¥ì )
        bias_score = min(1.0, bias_ratio * 10)
        
        return bias_score
    
    def _evaluate_source_credibility(self, source: str) -> float:
        """
        ì¶œì²˜ ì‹ ë¢°ë„ í‰ê°€
        
        Args:
            source: ë‰´ìŠ¤ ì¶œì²˜
            
        Returns:
            float: ì‹ ë¢°ë„ ì ìˆ˜ (0-1)
        """
        if not source:
            return 0.5
            
        # ì•Œë ¤ì§„ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜ í™•ì¸
        for domain, credibility in self.credible_sources.items():
            if domain in source.lower():
                return credibility
                
        # ê¸°ë³¸ ì‹ ë¢°ë„ ì ìˆ˜
        return 0.6
    
    def _calculate_freshness(self, published_at: Optional[str]) -> float:
        """
        ë‰´ìŠ¤ ì‹ ì„ ë„ ê³„ì‚°
        
        Args:
            published_at: ë°œí–‰ ì‹œê°„
            
        Returns:
            float: ì‹ ì„ ë„ ì ìˆ˜ (0-1)
        """
        if not published_at:
            return 0.5
            
        try:
            # ë°œí–‰ ì‹œê°„ íŒŒì‹± (ê°„ë‹¨í•œ êµ¬í˜„)
            if isinstance(published_at, str):
                # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ë‚ ì§œ íŒŒì‹± í•„ìš”
                published_date = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
            else:
                published_date = published_at
                
            now = datetime.now()
            time_diff = (now - published_date).total_seconds()
            
            # 24ì‹œê°„ ì´ë‚´ë©´ 1.0, ì¼ì£¼ì¼ í›„ë©´ 0.0
            hours_diff = time_diff / 3600
            freshness = max(0, min(1, 1 - hours_diff / (24 * 7)))
            
            return freshness
            
        except Exception:
            return 0.5
    
    def _predict_engagement(self, content: Dict[str, Any]) -> float:
        """
        ì°¸ì—¬ë„ ì˜ˆì¸¡
        
        Args:
            content: ì½˜í…ì¸  ë°ì´í„°
            
        Returns:
            float: ì°¸ì—¬ë„ ì ìˆ˜ (0-1)
        """
        text = content.get("content", "")
        title = content.get("title", "")
        
        # ì°¸ì—¬ë„ ë†’ì€ í‚¤ì›Œë“œë“¤
        engagement_keywords = [
            "breaking", "exclusive", "revealed", "secret", "shocking",
            "new study", "research", "discovery", "trend", "viral"
        ]
        
        combined_text = (title + " " + text).lower()
        engagement_score = sum(1 for keyword in engagement_keywords if keyword in combined_text)
        
        # ì œëª© ê¸¸ì´ë„ ê³ ë ¤ (ë„ˆë¬´ ê¸¸ê±°ë‚˜ ì§§ìœ¼ë©´ ì°¸ì—¬ë„ ë‚®ìŒ)
        title_length_score = 1.0
        if title:
            title_words = len(title.split())
            if title_words < 5 or title_words > 15:
                title_length_score = 0.7
                
        # ì •ê·œí™”
        normalized_engagement = min(1.0, (engagement_score / len(engagement_keywords)) * 0.8 + 0.2)
        
        return normalized_engagement * title_length_score
    
    def is_quality_content(self, metrics: QualityMetrics) -> bool:
        """
        ì½˜í…ì¸ ê°€ í’ˆì§ˆ ê¸°ì¤€ì„ ë§Œì¡±í•˜ëŠ”ì§€ í™•ì¸
        
        Args:
            metrics: í’ˆì§ˆ ë©”íŠ¸ë¦­
            
        Returns:
            bool: í’ˆì§ˆ ê¸°ì¤€ ë§Œì¡± ì—¬ë¶€
        """
        return (
            metrics.overall_score >= self.quality_threshold and
            metrics.bias_score <= self.bias_threshold and
            metrics.source_credibility >= 0.6
        )
    
    def get_quality_summary(self, metrics: QualityMetrics) -> Dict[str, Any]:
        """
        í’ˆì§ˆ ë¶„ì„ ìš”ì•½ ì •ë³´ ë°˜í™˜
        
        Args:
            metrics: í’ˆì§ˆ ë©”íŠ¸ë¦­
            
        Returns:
            Dict[str, Any]: í’ˆì§ˆ ìš”ì•½ ì •ë³´
        """
        return {
            "overall_grade": self._get_grade(metrics.overall_score),
            "is_quality_content": self.is_quality_content(metrics),
            "strengths": self._identify_strengths(metrics),
            "weaknesses": self._identify_weaknesses(metrics),
            "recommendations": self._generate_recommendations(metrics)
        }
    
    def _get_grade(self, score: float) -> str:
        """ì ìˆ˜ë¥¼ ë“±ê¸‰ìœ¼ë¡œ ë³€í™˜"""
        if score >= 0.9:
            return "A+"
        elif score >= 0.8:
            return "A"
        elif score >= 0.7:
            return "B"
        elif score >= 0.6:
            return "C"
        else:
            return "D"
    
    def _identify_strengths(self, metrics: QualityMetrics) -> List[str]:
        """ê°•ì  ì‹ë³„"""
        strengths = []
        
        if metrics.readability_score >= 0.8:
            strengths.append("ë†’ì€ ê°€ë…ì„±")
        if metrics.factual_accuracy >= 0.8:
            strengths.append("ë†’ì€ íŒ©íŠ¸ ì •í™•ì„±")
        if metrics.bias_score <= 0.3:
            strengths.append("ë‚®ì€ í¸í–¥ì„±")
        if metrics.source_credibility >= 0.8:
            strengths.append("ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜")
        if metrics.freshness_score >= 0.8:
            strengths.append("ìµœì‹  ì •ë³´")
        if metrics.engagement_potential >= 0.8:
            strengths.append("ë†’ì€ ì°¸ì—¬ë„ ì ì¬ë ¥")
            
        return strengths
    
    def _identify_weaknesses(self, metrics: QualityMetrics) -> List[str]:
        """ì•½ì  ì‹ë³„"""
        weaknesses = []
        
        if metrics.readability_score < 0.5:
            weaknesses.append("ë‚®ì€ ê°€ë…ì„±")
        if metrics.factual_accuracy < 0.6:
            weaknesses.append("íŒ©íŠ¸ ì •í™•ì„± ë¶€ì¡±")
        if metrics.bias_score > 0.7:
            weaknesses.append("ë†’ì€ í¸í–¥ì„±")
        if metrics.source_credibility < 0.5:
            weaknesses.append("ì¶œì²˜ ì‹ ë¢°ì„± ë¶€ì¡±")
        if metrics.freshness_score < 0.3:
            weaknesses.append("ì˜¤ë˜ëœ ì •ë³´")
        if metrics.engagement_potential < 0.4:
            weaknesses.append("ë‚®ì€ ì°¸ì—¬ë„")
            
        return weaknesses
    
    def _generate_recommendations(self, metrics: QualityMetrics) -> List[str]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if metrics.readability_score < 0.6:
            recommendations.append("ë¬¸ì¥ ê¸¸ì´ë¥¼ ì¤„ì´ê³  ë³µì¡í•œ ìš©ì–´ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•˜ì„¸ìš”")
        if metrics.factual_accuracy < 0.7:
            recommendations.append("ë” ë§ì€ íŒ©íŠ¸ì™€ ë°ì´í„°ë¥¼ í¬í•¨í•˜ì„¸ìš”")
        if metrics.bias_score > 0.5:
            recommendations.append("ì¤‘ë¦½ì ì¸ í‘œí˜„ì„ ì‚¬ìš©í•˜ê³  ê°ì •ì  ì–¸ì–´ë¥¼ ì¤„ì´ì„¸ìš”")
        if metrics.source_credibility < 0.6:
            recommendations.append("ë” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜ë¥¼ ì¸ìš©í•˜ì„¸ìš”")
        if metrics.engagement_potential < 0.5:
            recommendations.append("ë” í¥ë¯¸ë¡œìš´ ì œëª©ê³¼ ë‚´ìš©ì„ ì‘ì„±í•˜ì„¸ìš”")
            
        return recommendations 