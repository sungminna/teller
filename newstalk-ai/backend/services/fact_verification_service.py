"""
ğŸ” Fact Verification Service
============================

ë‰´ìŠ¤ ì½˜í…ì¸ ì˜ íŒ©íŠ¸ë¥¼ ê²€ì¦í•˜ëŠ” ì„œë¹„ìŠ¤
"""

import logging
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
from datetime import datetime
import asyncio

logger = logging.getLogger(__name__)


@dataclass
class FactCheckResult:
    """íŒ©íŠ¸ì²´í¬ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    
    claim: str
    verdict: str  # "TRUE", "FALSE", "PARTIALLY_TRUE", "UNVERIFIED"
    confidence: float
    sources: List[str]
    explanation: str
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "claim": self.claim,
            "verdict": self.verdict,
            "confidence": self.confidence,
            "sources": self.sources,
            "explanation": self.explanation
        }


class FactVerificationService:
    """íŒ©íŠ¸ ê²€ì¦ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.confidence_threshold = 0.7
        self.known_facts = {
            "earth is round": {"verdict": "TRUE", "confidence": 1.0},
            "water boils at 100 celsius": {"verdict": "TRUE", "confidence": 1.0},
            "humans can fly without assistance": {"verdict": "FALSE", "confidence": 1.0}
        }
        
    async def verify_facts(self, content: str) -> List[FactCheckResult]:
        """
        ì½˜í…ì¸ ì˜ íŒ©íŠ¸ë¥¼ ê²€ì¦
        
        Args:
            content: ê²€ì¦í•  ì½˜í…ì¸ 
            
        Returns:
            List[FactCheckResult]: íŒ©íŠ¸ì²´í¬ ê²°ê³¼ ëª©ë¡
        """
        try:
            # í´ë ˆì„ ì¶”ì¶œ
            claims = self._extract_claims(content)
            
            # ê° í´ë ˆì„ ê²€ì¦
            results = []
            for claim in claims:
                result = await self._verify_single_claim(claim)
                results.append(result)
                
            logger.info(f"Fact verification completed for {len(claims)} claims")
            return results
            
        except Exception as e:
            logger.error(f"Fact verification failed: {str(e)}")
            return []
    
    def _extract_claims(self, content: str) -> List[str]:
        """
        ì½˜í…ì¸ ì—ì„œ íŒ©íŠ¸ì²´í¬ ê°€ëŠ¥í•œ í´ë ˆì„ ì¶”ì¶œ
        
        Args:
            content: ì½˜í…ì¸  í…ìŠ¤íŠ¸
            
        Returns:
            List[str]: ì¶”ì¶œëœ í´ë ˆì„ ëª©ë¡
        """
        # ê°„ë‹¨í•œ í´ë ˆì„ ì¶”ì¶œ ë¡œì§
        sentences = content.split('.')
        
        # íŒ©íŠ¸ì„± ë¬¸ì¥ ì‹ë³„ í‚¤ì›Œë“œ
        fact_indicators = [
            "according to", "statistics show", "research indicates",
            "study found", "data reveals", "reports that", "confirmed that"
        ]
        
        claims = []
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 10:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
                # íŒ©íŠ¸ì„± ì§€í‘œê°€ í¬í•¨ëœ ë¬¸ì¥ ìš°ì„  ì„ íƒ
                if any(indicator in sentence.lower() for indicator in fact_indicators):
                    claims.append(sentence)
                elif len(claims) < 3:  # ìµœëŒ€ 3ê°œê¹Œì§€ ì¶”ê°€ í´ë ˆì„ í¬í•¨
                    claims.append(sentence)
                    
        return claims[:5]  # ìµœëŒ€ 5ê°œ í´ë ˆì„
    
    async def _verify_single_claim(self, claim: str) -> FactCheckResult:
        """
        ë‹¨ì¼ í´ë ˆì„ ê²€ì¦
        
        Args:
            claim: ê²€ì¦í•  í´ë ˆì„
            
        Returns:
            FactCheckResult: íŒ©íŠ¸ì²´í¬ ê²°ê³¼
        """
        # ì‹œë®¬ë ˆì´ì…˜ëœ ê²€ì¦ í”„ë¡œì„¸ìŠ¤
        await asyncio.sleep(0.1)  # API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
        
        # ì•Œë ¤ì§„ íŒ©íŠ¸ í™•ì¸
        claim_lower = claim.lower()
        for known_fact, fact_data in self.known_facts.items():
            if known_fact in claim_lower:
                return FactCheckResult(
                    claim=claim,
                    verdict=fact_data["verdict"],
                    confidence=fact_data["confidence"],
                    sources=["Known Fact Database"],
                    explanation=f"This claim matches a verified fact in our database."
                )
        
        # ê¸°ë³¸ íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ê²€ì¦
        verdict, confidence = self._heuristic_verification(claim)
        
        return FactCheckResult(
            claim=claim,
            verdict=verdict,
            confidence=confidence,
            sources=self._get_mock_sources(),
            explanation=self._generate_explanation(claim, verdict, confidence)
        )
    
    def _heuristic_verification(self, claim: str) -> tuple[str, float]:
        """
        íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ê²€ì¦
        
        Args:
            claim: ê²€ì¦í•  í´ë ˆì„
            
        Returns:
            tuple[str, float]: (verdict, confidence)
        """
        claim_lower = claim.lower()
        
        # ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì§€í‘œë“¤
        reliable_indicators = [
            "according to official", "government data", "peer-reviewed study",
            "scientific research", "official statistics", "verified by"
        ]
        
        # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ì§€í‘œë“¤
        suspicious_indicators = [
            "some say", "it is believed", "rumors suggest",
            "unconfirmed reports", "allegedly", "supposedly"
        ]
        
        # ê³¼ì¥ëœ í‘œí˜„ë“¤
        exaggerated_indicators = [
            "100% effective", "completely safe", "never fails",
            "always works", "miracle", "revolutionary breakthrough"
        ]
        
        # ì ìˆ˜ ê³„ì‚°
        reliability_score = sum(1 for indicator in reliable_indicators if indicator in claim_lower)
        suspicion_score = sum(1 for indicator in suspicious_indicators if indicator in claim_lower)
        exaggeration_score = sum(1 for indicator in exaggerated_indicators if indicator in claim_lower)
        
        # ìˆ«ìë‚˜ í†µê³„ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        has_numbers = any(char.isdigit() for char in claim)
        
        # íŒì • ë¡œì§
        if reliability_score > 0 and suspicion_score == 0:
            if has_numbers:
                return "TRUE", 0.8
            else:
                return "PARTIALLY_TRUE", 0.7
        elif suspicion_score > 0 or exaggeration_score > 0:
            return "UNVERIFIED", 0.4
        elif has_numbers and len(claim.split()) > 10:
            return "PARTIALLY_TRUE", 0.6
        else:
            return "UNVERIFIED", 0.5
    
    def _get_mock_sources(self) -> List[str]:
        """ëª¨ì˜ ì†ŒìŠ¤ ëª©ë¡ ë°˜í™˜"""
        return [
            "Reuters Fact Check Database",
            "Associated Press Verification",
            "Scientific Journal Database",
            "Government Statistics Portal"
        ]
    
    def _generate_explanation(self, claim: str, verdict: str, confidence: float) -> str:
        """
        ê²€ì¦ ê²°ê³¼ ì„¤ëª… ìƒì„±
        
        Args:
            claim: ì›ë³¸ í´ë ˆì„
            verdict: íŒì • ê²°ê³¼
            confidence: ì‹ ë¢°ë„
            
        Returns:
            str: ì„¤ëª… í…ìŠ¤íŠ¸
        """
        explanations = {
            "TRUE": f"This claim has been verified with {confidence:.1%} confidence based on reliable sources and factual indicators.",
            "FALSE": f"This claim contradicts verified information with {confidence:.1%} confidence.",
            "PARTIALLY_TRUE": f"This claim contains some accurate elements but may lack complete context or precision. Confidence: {confidence:.1%}.",
            "UNVERIFIED": f"Insufficient reliable evidence to verify this claim. Confidence in assessment: {confidence:.1%}."
        }
        
        return explanations.get(verdict, "Unable to determine the veracity of this claim.")
    
    def get_overall_credibility(self, results: List[FactCheckResult]) -> Dict[str, Any]:
        """
        ì „ì²´ ì½˜í…ì¸ ì˜ ì‹ ë¢°ë„ í‰ê°€
        
        Args:
            results: íŒ©íŠ¸ì²´í¬ ê²°ê³¼ ëª©ë¡
            
        Returns:
            Dict[str, Any]: ì „ì²´ ì‹ ë¢°ë„ í‰ê°€
        """
        if not results:
            return {
                "overall_credibility": 0.5,
                "grade": "C",
                "summary": "No verifiable claims found",
                "recommendations": ["Add more factual content with verifiable sources"]
            }
        
        # ì ìˆ˜ ê³„ì‚°
        total_score = 0
        for result in results:
            if result.verdict == "TRUE":
                total_score += result.confidence
            elif result.verdict == "PARTIALLY_TRUE":
                total_score += result.confidence * 0.7
            elif result.verdict == "FALSE":
                total_score -= result.confidence * 0.5
            # UNVERIFIEDëŠ” ì¤‘ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬
        
        overall_credibility = max(0, min(1, total_score / len(results)))
        
        # ë“±ê¸‰ ê³„ì‚°
        grade = self._calculate_grade(overall_credibility)
        
        # ìš”ì•½ ë° ê¶Œì¥ì‚¬í•­
        summary = self._generate_summary(results, overall_credibility)
        recommendations = self._generate_recommendations(results, overall_credibility)
        
        return {
            "overall_credibility": overall_credibility,
            "grade": grade,
            "summary": summary,
            "recommendations": recommendations,
            "total_claims_checked": len(results),
            "verified_claims": len([r for r in results if r.verdict == "TRUE"]),
            "false_claims": len([r for r in results if r.verdict == "FALSE"]),
            "unverified_claims": len([r for r in results if r.verdict == "UNVERIFIED"])
        }
    
    def _calculate_grade(self, credibility: float) -> str:
        """ì‹ ë¢°ë„ ì ìˆ˜ë¥¼ ë“±ê¸‰ìœ¼ë¡œ ë³€í™˜"""
        if credibility >= 0.9:
            return "A+"
        elif credibility >= 0.8:
            return "A"
        elif credibility >= 0.7:
            return "B"
        elif credibility >= 0.6:
            return "C"
        else:
            return "D"
    
    def _generate_summary(self, results: List[FactCheckResult], credibility: float) -> str:
        """ì‹ ë¢°ë„ ìš”ì•½ ìƒì„±"""
        if credibility >= 0.8:
            return "High credibility content with well-verified claims"
        elif credibility >= 0.6:
            return "Moderately credible content with some verified information"
        elif credibility >= 0.4:
            return "Mixed credibility with both verified and unverified claims"
        else:
            return "Low credibility content requiring additional verification"
    
    def _generate_recommendations(self, results: List[FactCheckResult], credibility: float) -> List[str]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        false_claims = [r for r in results if r.verdict == "FALSE"]
        unverified_claims = [r for r in results if r.verdict == "UNVERIFIED"]
        
        if false_claims:
            recommendations.append("Remove or correct false claims identified in the fact-check")
        
        if len(unverified_claims) > len(results) * 0.5:
            recommendations.append("Add more verifiable sources and citations")
        
        if credibility < 0.6:
            recommendations.append("Include more factual content from reliable sources")
            recommendations.append("Provide specific data and statistics to support claims")
        
        if not recommendations:
            recommendations.append("Continue maintaining high factual standards")
        
        return recommendations
    
    async def real_time_fact_check(self, claim: str) -> FactCheckResult:
        """
        ì‹¤ì‹œê°„ íŒ©íŠ¸ì²´í¬
        
        Args:
            claim: ì‹¤ì‹œê°„ìœ¼ë¡œ ì²´í¬í•  í´ë ˆì„
            
        Returns:
            FactCheckResult: íŒ©íŠ¸ì²´í¬ ê²°ê³¼
        """
        return await self._verify_single_claim(claim)
    
    def is_credible_content(self, results: List[FactCheckResult]) -> bool:
        """
        ì½˜í…ì¸ ê°€ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ”ì§€ íŒë‹¨
        
        Args:
            results: íŒ©íŠ¸ì²´í¬ ê²°ê³¼ ëª©ë¡
            
        Returns:
            bool: ì‹ ë¢°ì„± ì—¬ë¶€
        """
        if not results:
            return False
            
        credibility = self.get_overall_credibility(results)
        return credibility["overall_credibility"] >= self.confidence_threshold 