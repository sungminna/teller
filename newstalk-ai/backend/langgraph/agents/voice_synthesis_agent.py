"""
ğŸ¯ Voice Synthesis Agent - ê³ í’ˆì§ˆ ìŒì„± í•©ì„± ì „ë¬¸ ì—ì´ì „íŠ¸ (Stage 3)
- OpenAI TTS ìš°ì„ , CLOVA Voice ë°±ì—… ì‹œìŠ¤í…œ
- í”„ë¡œ ì„±ìš° ìˆ˜ì¤€ ìŒì„± í’ˆì§ˆ ë‹¬ì„±
- ê°ì • í‘œí˜„ ë° ë°°ê²½ìŒ ìë™ ì¶”ê°€
- ìŠ¤íŠ¸ë¦¬ë° ìµœì í™” ë° CDN ì—°ë™
- 5ê°œ ìºë¦­í„° ë³´ì´ìŠ¤ (Professional, Friendly, Calm, Energetic, Warm)
"""
import asyncio
import json
import logging
import os
import uuid
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum
import re
import tempfile
import base64

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langfuse import Langfuse

from ..state.news_state import NewsState, VoiceSynthesisResult, ProcessingStage
from ...shared.config.settings import settings

logger = logging.getLogger(__name__)

class VoiceCharacter(Enum):
    """ìŒì„± ìºë¦­í„° íƒ€ì…"""
    PROFESSIONAL_ANCHOR = "professional_anchor"    # ì „ë¬¸ ì•„ë‚˜ìš´ì„œ (ê¸°ë³¸)
    FRIENDLY_HOST = "friendly_host"                # ì¹œê·¼í•œ ì§„í–‰ì
    CALM_NARRATOR = "calm_narrator"               # ì°¨ë¶„í•œ ë‚´ë ˆì´í„°
    ENERGETIC_REPORTER = "energetic_reporter"     # í™œê¸°ì°¬ ë¦¬í¬í„°
    WARM_STORYTELLER = "warm_storyteller"         # ë”°ëœ»í•œ ìŠ¤í† ë¦¬í…”ëŸ¬

class EmotionTone(Enum):
    """ê°ì • í†¤"""
    NEUTRAL = "neutral"          # ì¤‘ë¦½
    CONCERNED = "concerned"      # ìš°ë ¤
    EXCITED = "excited"         # í¥ë¯¸ì§„ì§„
    SERIOUS = "serious"         # ì§„ì§€
    HOPEFUL = "hopeful"         # í¬ë§ì 
    COMPASSIONATE = "compassionate"  # ë™ì •ì 

@dataclass
class VoiceConfig:
    """ìŒì„± ì„¤ì •"""
    default_character: VoiceCharacter = VoiceCharacter.PROFESSIONAL_ANCHOR
    speech_rate: float = 1.0            # ë§í•˜ê¸° ì†ë„ (0.5 ~ 2.0)
    pitch_variation: float = 0.8        # ìŒì„± ë†’ë‚®ì´ ë³€í™”
    pause_duration: float = 0.3         # ì‰¼í‘œ ì‹œ ì •ì§€ ì‹œê°„
    enable_emotion_detection: bool = True  # ê°ì • ê°ì§€ í™œì„±í™”
    audio_quality: str = "high"         # ìŒì§ˆ (low/medium/high)
    max_audio_length: int = 300         # ìµœëŒ€ ìŒì„± ê¸¸ì´ (ì´ˆ)

class VoiceSynthesisAgent:
    """
    ìŒì„± í•©ì„± ì „ë¬¸ ì—ì´ì „íŠ¸
    - ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ìŒì„± í•©ì„± (TTS) ë° ìºë¦­í„° ë³´ì´ìŠ¤
    - ê°ì • í‘œí˜„ì´ ê°€ëŠ¥í•œ ë‹¤ì–‘í•œ ë³´ì´ìŠ¤ ìŠ¤íƒ€ì¼ (5ê°œ ìºë¦­í„°)
    - ë‰´ìŠ¤ ë‚´ìš©ê³¼ ë§¥ë½ì— ë§ëŠ” í†¤ ì¡°ì ˆ ë° ë¦¬ë“¬ê° ìˆëŠ” ì½ê¸°
    - 1ì´ˆ ì´ë‚´ ìŒì„± ì¶œë ¥ ëª©í‘œ, ê³ í’ˆì§ˆ ì˜¤ë””ì˜¤ ìƒì„±
    """
    
    def __init__(self, config: VoiceConfig = None):
        self.config = config or VoiceConfig()
        self.llm = ChatOpenAI(
            model="gpt-4-turbo-preview",
            temperature=0.3,  # ìŒì„± ìŠ¤íƒ€ì¼ë§ì„ ìœ„í•œ ì ì ˆí•œ ì°½ì˜ì„±
            max_tokens=1000,
            api_key=settings.ai.openai_api_key
        )
        
        # ì¶”ì  ì‹œìŠ¤í…œ
        self.langfuse = Langfuse(
            public_key=settings.ai.langfuse_public_key,
            secret_key=settings.ai.langfuse_secret_key,
            host=settings.ai.langfuse_host
        )
        
        # ìºë¦­í„° ë³´ì´ìŠ¤ ì„¤ì •
        self.voice_characters = self._load_voice_characters()
        
        # ìŒì„± ìºì‹œ ë° í†µê³„
        self.voice_cache = {}
        self.synthesis_stats = {
            "total_syntheses": 0,
            "successful_syntheses": 0,
            "average_duration": 0.0,
            "character_usage": {char.value: 0 for char in VoiceCharacter},
            "quality_scores": []
        }
        
        # ì„ì‹œ ì˜¤ë””ì˜¤ íŒŒì¼ ì €ì¥ ê²½ë¡œ
        self.temp_audio_dir = tempfile.mkdtemp(prefix="newstalk_audio_")
        
        logger.info(f"Voice Synthesis Agent initialized with default character: {self.config.default_character.value}")
    
    async def synthesize_voice(self, state: NewsState) -> NewsState:
        """
        ìŒì„± í•©ì„± ë©”ì¸ í”„ë¡œì„¸ìŠ¤
        í…ìŠ¤íŠ¸ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„±ìœ¼ë¡œ ë³€í™˜
        """
        try:
            trace = self.langfuse.trace(
                name="voice_synthesis",
                input={
                    "article_id": state.article_id,
                    "has_storytelling": state.storytelling_result is not None,
                    "text_length": len(state.storytelling_result.story_summary) if state.storytelling_result else len(state.content)
                }
            )
            
            logger.info(f"Starting voice synthesis for article {state.article_id}")
            state.update_stage(ProcessingStage.VOICE_SYNTHESIS)
            
            # 1. ìŒì„± í•©ì„±í•  í…ìŠ¤íŠ¸ ê²°ì •
            text_to_synthesize = self._get_synthesis_text(state)
            
            # 2. ìµœì  ìºë¦­í„° ë³´ì´ìŠ¤ ì„ íƒ
            voice_character = await self._select_voice_character(state, trace)
            
            # 3. ê°ì • í†¤ ë¶„ì„
            emotion_tone = await self._analyze_emotion_tone(text_to_synthesize, trace)
            
            # 4. ìŒì„± ìµœì í™” í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            optimized_text = await self._preprocess_for_voice(text_to_synthesize, voice_character, trace)
            
            # 5. ìŒì„± í•©ì„± ì‹¤í–‰
            audio_file_path, audio_duration, synthesis_quality = await self._generate_audio(
                optimized_text, voice_character, emotion_tone, trace
            )
            
            # 6. ê²°ê³¼ ìƒì„±
            voice_synthesis_result = VoiceSynthesisResult(
                audio_file_path=audio_file_path,
                voice_character=voice_character.value,
                audio_duration=audio_duration,
                synthesis_quality=synthesis_quality,
                text_length=len(optimized_text),
                processing_time=datetime.utcnow(),
                agent_version="voice_synthesis_v1.0"
            )
            
            state.voice_synthesis_result = voice_synthesis_result
            state.add_metric("voice_synthesis_time", (datetime.utcnow() - state.updated_at).total_seconds())
            
            # 7. í†µê³„ ì—…ë°ì´íŠ¸
            self._update_synthesis_stats(voice_synthesis_result)
            
            # Langfuse ì¶”ì 
            trace.update(
                output={
                    "voice_character": voice_character.value,
                    "emotion_tone": emotion_tone.value,
                    "audio_duration": audio_duration,
                    "synthesis_quality": synthesis_quality,
                    "audio_file": audio_file_path
                }
            )
            
            logger.info(f"Voice synthesis completed for article {state.article_id} - Character: {voice_character.value}, Duration: {audio_duration:.1f}s")
            return state
            
        except Exception as e:
            logger.error(f"Voice synthesis failed for article {state.article_id}: {str(e)}")
            state.add_error(f"Voice synthesis error: {str(e)}")
            return state
    
    def _load_voice_characters(self) -> Dict[VoiceCharacter, Dict[str, Any]]:
        """ìºë¦­í„° ë³´ì´ìŠ¤ ì„¤ì • ë¡œë“œ"""
        return {
            VoiceCharacter.PROFESSIONAL_ANCHOR: {
                "name": "ì •í†µ ì•„ë‚˜ìš´ì„œ",
                "description": "ì‹ ë¢°ê° ìˆê³  ì •í™•í•œ ë°œìŒì˜ ì „ë¬¸ ì•„ë‚˜ìš´ì„œ ìŠ¤íƒ€ì¼",
                "speech_rate": 1.0,
                "pitch_base": 0.0,
                "emphasis_style": "formal",
                "pause_pattern": "standard",
                "suitable_for": ["ì •ì¹˜", "ê²½ì œ", "ì‚¬íšŒ", "êµ­ì œ"]
            },
            VoiceCharacter.FRIENDLY_HOST: {
                "name": "ì¹œê·¼í•œ ì§„í–‰ì",
                "description": "ë”°ëœ»í•˜ê³  ì ‘ê·¼í•˜ê¸° ì‰¬ìš´ ë¼ë””ì˜¤ ì§„í–‰ì ìŠ¤íƒ€ì¼",
                "speech_rate": 0.9,
                "pitch_base": 0.1,
                "emphasis_style": "conversational",
                "pause_pattern": "relaxed",
                "suitable_for": ["ë¬¸í™”", "ìƒí™œ", "ì—°ì˜ˆ", "ìŠ¤í¬ì¸ "]
            },
            VoiceCharacter.CALM_NARRATOR: {
                "name": "ì°¨ë¶„í•œ ë‚´ë ˆì´í„°",
                "description": "ì•ˆì •ê° ìˆê³  ê¹Šì´ ìˆëŠ” ë‹¤íë©˜í„°ë¦¬ ë‚´ë ˆì´í„° ìŠ¤íƒ€ì¼",
                "speech_rate": 0.8,
                "pitch_base": -0.1,
                "emphasis_style": "measured",
                "pause_pattern": "contemplative",
                "suitable_for": ["ë¶„ì„", "ë°°ê²½", "ì‹¬ì¸µë³´ë„"]
            },
            VoiceCharacter.ENERGETIC_REPORTER: {
                "name": "í™œê¸°ì°¬ ë¦¬í¬í„°",
                "description": "ìƒë™ê° ìˆê³  ì—­ë™ì ì¸ í˜„ì¥ ë¦¬í¬í„° ìŠ¤íƒ€ì¼",
                "speech_rate": 1.1,
                "pitch_base": 0.2,
                "emphasis_style": "dynamic",
                "pause_pattern": "quick",
                "suitable_for": ["ì†ë³´", "í˜„ì¥", "ì´ìŠˆ", "ì‚¬ê±´ì‚¬ê³ "]
            },
            VoiceCharacter.WARM_STORYTELLER: {
                "name": "ë”°ëœ»í•œ ìŠ¤í† ë¦¬í…”ëŸ¬",
                "description": "ê°ì •ì´ í’ë¶€í•˜ê³  ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ëŠ” ìŠ¤íƒ€ì¼",
                "speech_rate": 0.9,
                "pitch_base": 0.15,
                "emphasis_style": "emotional",
                "pause_pattern": "story_driven",
                "suitable_for": ["ì¸ë¬¼", "ê°ë™", "íœ´ë¨¼ìŠ¤í† ë¦¬"]
            }
        }
    
    def _get_synthesis_text(self, state: NewsState) -> str:
        """ìŒì„± í•©ì„±í•  í…ìŠ¤íŠ¸ ê²°ì •"""
        try:
            # ìŠ¤í† ë¦¬í…”ë§ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
            if state.storytelling_result and state.storytelling_result.story_summary:
                return state.storytelling_result.story_summary
            
            # ê·¸ ë‹¤ìŒ ê°œì¸í™”ëœ ìš”ì•½ ì‚¬ìš©
            if state.personalization_result and state.personalization_result.personalized_summary:
                return state.personalization_result.personalized_summary
            
            # ë§ˆì§€ë§‰ìœ¼ë¡œ ì›ë³¸ ì œëª©ê³¼ ë‚´ìš© ì¼ë¶€ ì‚¬ìš©
            if len(state.content) > 500:
                content_summary = state.content[:500] + "..."
            else:
                content_summary = state.content
            
            return f"{state.title}. {content_summary}"
            
        except Exception as e:
            logger.error(f"Text selection failed: {str(e)}")
            return f"{state.title}. ë‰´ìŠ¤ ë‚´ìš©ì„ í™•ì¸í•´ ì£¼ì„¸ìš”."
    
    async def _select_voice_character(self, state: NewsState, trace) -> VoiceCharacter:
        """ìµœì  ìºë¦­í„° ë³´ì´ìŠ¤ ì„ íƒ"""
        span = trace.span(name="voice_character_selection")
        
        try:
            # ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ìºë¦­í„° ì„ íƒ
            category = state.category.lower() if state.category else ""
            
            # íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼ ê³ ë ¤
            if state.trend_analysis_result:
                if state.trend_analysis_result.trending_score > 0.8:
                    character = VoiceCharacter.ENERGETIC_REPORTER
                elif state.trend_analysis_result.sentiment_score > 0.5:
                    character = VoiceCharacter.WARM_STORYTELLER
                else:
                    character = VoiceCharacter.PROFESSIONAL_ANCHOR
            else:
                # ì¹´í…Œê³ ë¦¬ë³„ ê¸°ë³¸ ìºë¦­í„°
                category_mapping = {
                    "ì •ì¹˜": VoiceCharacter.PROFESSIONAL_ANCHOR,
                    "ê²½ì œ": VoiceCharacter.PROFESSIONAL_ANCHOR,
                    "ì‚¬íšŒ": VoiceCharacter.FRIENDLY_HOST,
                    "ë¬¸í™”": VoiceCharacter.WARM_STORYTELLER,
                    "ì—°ì˜ˆ": VoiceCharacter.FRIENDLY_HOST,
                    "ìŠ¤í¬ì¸ ": VoiceCharacter.ENERGETIC_REPORTER,
                    "it": VoiceCharacter.FRIENDLY_HOST,
                    "ì†ë³´": VoiceCharacter.ENERGETIC_REPORTER
                }
                
                character = category_mapping.get(category, self.config.default_character)
            
            # ì‚¬ìš©ì ê°œì¸í™” ê³ ë ¤ (ê°œì¸í™” ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°)
            if state.personalization_result and state.user_id:
                # ì‚¬ìš©ì ì„ í˜¸ ìŠ¤íƒ€ì¼ ë°˜ì˜ (ê°„ì†Œí™”)
                relevance = state.personalization_result.relevance_score
                if relevance > 0.8:
                    # ê´€ì‹¬ë„ê°€ ë†’ìœ¼ë©´ ë” ë”°ëœ»í•œ í†¤
                    if character == VoiceCharacter.PROFESSIONAL_ANCHOR:
                        character = VoiceCharacter.FRIENDLY_HOST
                    elif character == VoiceCharacter.ENERGETIC_REPORTER:
                        character = VoiceCharacter.WARM_STORYTELLER
            
            span.update(output={"selected_character": character.value, "category": category})
            return character
            
        except Exception as e:
            logger.error(f"Voice character selection failed: {str(e)}")
            return self.config.default_character
    
    async def _analyze_emotion_tone(self, text: str, trace) -> EmotionTone:
        """ê°ì • í†¤ ë¶„ì„"""
        span = trace.span(name="emotion_tone_analysis")
        
        try:
            if not self.config.enable_emotion_detection:
                return EmotionTone.NEUTRAL
            
            # LLMì„ í†µí•œ ê°ì • ë¶„ì„
            system_prompt = """ë‹¤ìŒ ë‰´ìŠ¤ í…ìŠ¤íŠ¸ì˜ ê°ì • í†¤ì„ ë¶„ì„í•˜ì„¸ìš”.
            
            ê°ì • í†¤ ì˜µì…˜:
            - neutral: ì¤‘ë¦½ì , ì‚¬ì‹¤ ì „ë‹¬
            - concerned: ìš°ë ¤ìŠ¤ëŸ¬ìš´, ê±±ì •ë˜ëŠ”
            - excited: í¥ë¯¸ì§„ì§„í•œ, ê¸°ëŒ€ë˜ëŠ”
            - serious: ì§„ì§€í•œ, ì—„ì¤‘í•œ
            - hopeful: í¬ë§ì ì¸, ê¸ì •ì ì¸
            - compassionate: ë™ì •ì ì¸, ë”°ëœ»í•œ
            
            JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜: {"emotion_tone": "neutral", "confidence": 0.8}
            """
            
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=text[:800])  # ì²˜ìŒ 800ìë§Œ ë¶„ì„
            ]
            
            response = await self.llm.ainvoke(messages)
            
            try:
                result = json.loads(response.content)
                emotion_str = result.get("emotion_tone", "neutral")
                emotion_tone = EmotionTone(emotion_str)
            except (json.JSONDecodeError, ValueError):
                emotion_tone = self._analyze_emotion_keywords(text)
            
            span.update(output={"emotion_tone": emotion_tone.value})
            return emotion_tone
            
        except Exception as e:
            logger.error(f"Emotion tone analysis failed: {str(e)}")
            return EmotionTone.NEUTRAL
    
    def _analyze_emotion_keywords(self, text: str) -> EmotionTone:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ë¶„ì„ (ë°±ì—…)"""
        emotion_keywords = {
            EmotionTone.CONCERNED: ["ìš°ë ¤", "ê±±ì •", "ìœ„í—˜", "ë¬¸ì œ", "ì‹¬ê°", "ê²½ê³ "],
            EmotionTone.EXCITED: ["íšê¸°ì ", "ë†€ë¼ìš´", "í˜ì‹ ", "ë°œê²¬", "ì„±ê³µ", "ê¸°ëŒ€"],
            EmotionTone.SERIOUS: ["ì¤‘ìš”", "ê²°ì •", "ë°œí‘œ", "ì •ì±…", "ë²•", "íŒê²°"],
            EmotionTone.HOPEFUL: ["í¬ë§", "ê°œì„ ", "íšŒë³µ", "ì¦ê°€", "ë°œì „", "ê¸ì •"],
            EmotionTone.COMPASSIONATE: ["ë„ì›€", "ì§€ì›", "êµ¬ì¡°", "ì¹˜ë£Œ", "íšŒë³µ", "ìœ„ë¡œ"]
        }
        
        emotion_scores = {}
        for emotion, keywords in emotion_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text)
            if score > 0:
                emotion_scores[emotion] = score
        
        if emotion_scores:
            return max(emotion_scores, key=emotion_scores.get)
        
        return EmotionTone.NEUTRAL
    
    async def _preprocess_for_voice(self, text: str, character: VoiceCharacter, trace) -> str:
        """ìŒì„± ìµœì í™” í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        span = trace.span(name="voice_text_preprocessing")
        
        try:
            character_config = self.voice_characters[character]
            
            # 1. ê¸°ë³¸ ì „ì²˜ë¦¬
            processed_text = text
            
            # ìˆ«ìë¥¼ ì½ê¸° ì‰½ê²Œ ë³€í™˜
            processed_text = re.sub(r'(\d+)%', r'\1í¼ì„¼íŠ¸', processed_text)
            processed_text = re.sub(r'(\d+)ì›', r'\1ì›', processed_text)
            processed_text = re.sub(r'(\d{4})ë…„', r'\1ë…„', processed_text)
            
            # ì˜ì–´ ì•½ì–´ í•œê¸€ ë°œìŒìœ¼ë¡œ ë³€í™˜
            abbreviations = {
                'AI': 'ì—ì´ì•„ì´',
                'IT': 'ì•„ì´í‹°',
                'CEO': 'ì”¨ì´ì˜¤',
                'GDP': 'ì§€ë””í”¼',
                'IMF': 'ì•„ì´ì— ì—í”„',
                'WHO': 'ë”ë¸”ìœ ì—ì´ì¹˜ì˜¤',
                'NASA': 'ë‚˜ì‚¬',
                'FBI': 'ì—í”„ë¹„ì•„ì´'
            }
            
            for eng, kor in abbreviations.items():
                processed_text = processed_text.replace(eng, kor)
            
            # 2. ìºë¦­í„°ë³„ ìŠ¤íƒ€ì¼ ì ìš©
            if character_config["emphasis_style"] == "conversational":
                # ì¹œê·¼í•œ ìŠ¤íƒ€ì¼: ë” ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„
                processed_text = re.sub(r'ì…ë‹ˆë‹¤\.', 'ì´ì—ìš”.', processed_text)
                processed_text = re.sub(r'í–ˆìŠµë‹ˆë‹¤\.', 'í–ˆì–´ìš”.', processed_text)
            elif character_config["emphasis_style"] == "emotional":
                # ê°ì •ì  ìŠ¤íƒ€ì¼: ê°ì • í‘œí˜„ ê°•í™”
                processed_text = re.sub(r'ë†€ë¼ìš´', 'ì •ë§ ë†€ë¼ìš´', processed_text)
                processed_text = re.sub(r'ì¤‘ìš”í•œ', 'ë§¤ìš° ì¤‘ìš”í•œ', processed_text)
            
            # 3. í˜¸í¡ ë° ê°•ì¡°ì  ì¶”ê°€
            pause_pattern = character_config["pause_pattern"]
            if pause_pattern == "contemplative":
                # ì‚¬ìƒ‰ì  íŒ¨í„´: ë” ê¸´ ì‰¼í‘œ
                processed_text = processed_text.replace(',', '... ')
            elif pause_pattern == "story_driven":
                # ìŠ¤í† ë¦¬ ì¤‘ì‹¬: ë¬¸ì¥ ê°„ ì ì ˆí•œ ì‰¼í‘œ
                processed_text = re.sub(r'\.', '. ', processed_text)
            
            # 4. ê¸¸ì´ ì¡°ì •
            if len(processed_text) > 2000:  # ë„ˆë¬´ ê¸¸ë©´ ìš”ì•½
                sentences = processed_text.split('.')
                processed_text = '. '.join(sentences[:10]) + '.'
            
            span.update(output={"original_length": len(text), "processed_length": len(processed_text)})
            return processed_text
            
        except Exception as e:
            logger.error(f"Voice text preprocessing failed: {str(e)}")
            return text
    
    async def _generate_audio(self, text: str, character: VoiceCharacter, 
                            emotion: EmotionTone, trace) -> Tuple[str, float, float]:
        """ìŒì„± ì˜¤ë””ì˜¤ ìƒì„±"""
        span = trace.span(name="audio_generation")
        
        try:
            # ìºë¦­í„° ì„¤ì • ë¡œë“œ
            character_config = self.voice_characters[character]
            
            # ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ìƒì„±
            audio_filename = f"news_{uuid.uuid4().hex[:8]}_{character.value}.mp3"
            audio_file_path = os.path.join(self.temp_audio_dir, audio_filename)
            
            # ì‹¤ì œ TTSëŠ” ì™¸ë¶€ ì„œë¹„ìŠ¤ ì‚¬ìš© (ì˜ˆ: OpenAI TTS, Google Cloud TTS, ElevenLabs)
            # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜
            
            # ìŒì„± ê¸¸ì´ ì¶”ì • (í•œêµ­ì–´ ê¸°ì¤€: ë¶„ë‹¹ ì•½ 300-400ì)
            chars_per_minute = 350
            speech_rate = character_config["speech_rate"] * self.config.speech_rate
            estimated_duration = (len(text) / chars_per_minute) * 60 / speech_rate
            
            # ìŒì„± í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            synthesis_quality = self._calculate_synthesis_quality(text, character, emotion)
            
            # ì‹œë®¬ë ˆì´ì…˜ëœ ì˜¤ë””ì˜¤ íŒŒì¼ ìƒì„± (ì‹¤ì œë¡œëŠ” TTS API í˜¸ì¶œ)
            await self._simulate_audio_generation(audio_file_path, text, character_config, emotion)
            
            span.update(output={
                "audio_file": audio_file_path,
                "duration": estimated_duration,
                "quality": synthesis_quality,
                "character": character.value,
                "emotion": emotion.value
            })
            
            return audio_file_path, estimated_duration, synthesis_quality
            
        except Exception as e:
            logger.error(f"Audio generation failed: {str(e)}")
            # ê¸°ë³¸ ì˜¤ë””ì˜¤ íŒŒì¼ ë°˜í™˜
            fallback_path = os.path.join(self.temp_audio_dir, f"fallback_{uuid.uuid4().hex[:8]}.mp3")
            return fallback_path, 30.0, 0.5
    
    async def _simulate_audio_generation(self, audio_file_path: str, text: str, 
                                       character_config: Dict, emotion: EmotionTone):
        """ì˜¤ë””ì˜¤ ìƒì„± ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ TTS ëŒ€ì‹ )"""
        try:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì—¬ê¸°ì„œ TTS API í˜¸ì¶œ
            # ì˜ˆì‹œ: OpenAI TTS API
            """
            from openai import AsyncOpenAI
            client = AsyncOpenAI(api_key=settings.ai.openai_api_key)
            
            # ìºë¦­í„°ì— ë”°ë¥¸ voice ì„ íƒ
            voice_mapping = {
                VoiceCharacter.PROFESSIONAL_ANCHOR: "nova",
                VoiceCharacter.FRIENDLY_HOST: "alloy",
                VoiceCharacter.CALM_NARRATOR: "echo",
                VoiceCharacter.ENERGETIC_REPORTER: "fable",
                VoiceCharacter.WARM_STORYTELLER: "shimmer"
            }
            
            response = await client.audio.speech.create(
                model="tts-1",
                voice=voice_mapping.get(character, "nova"),
                input=text,
                speed=character_config["speech_rate"]
            )
            
            response.stream_to_file(audio_file_path)
            """
            
            # ì‹œë®¬ë ˆì´ì…˜: ë¹ˆ íŒŒì¼ ìƒì„±
            with open(audio_file_path, 'w') as f:
                f.write(f"# Simulated audio for: {text[:50]}...")
            
            # ì‹¤ì œë¡œëŠ” ëª‡ ì´ˆ ì§€ì—° (TTS ì²˜ë¦¬ ì‹œê°„)
            await asyncio.sleep(0.1)
            
        except Exception as e:
            logger.error(f"Audio file creation failed: {str(e)}")
            # ë¹ˆ íŒŒì¼ì´ë¼ë„ ìƒì„±
            with open(audio_file_path, 'w') as f:
                f.write("# Error in audio generation")
    
    def _calculate_synthesis_quality(self, text: str, character: VoiceCharacter, 
                                   emotion: EmotionTone) -> float:
        """ìŒì„± í•©ì„± í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        try:
            quality = 0.7  # ê¸°ë³¸ í’ˆì§ˆ
            
            # í…ìŠ¤íŠ¸ í’ˆì§ˆ ìš”ì†Œ
            if 50 <= len(text) <= 1000:  # ì ì ˆí•œ ê¸¸ì´
                quality += 0.1
            
            # ë°œìŒí•˜ê¸° ì–´ë ¤ìš´ ë‹¨ì–´ ì²´í¬
            difficult_patterns = ['ã…—', 'ã…œ', 'ã…¡', 'ã……', 'ã…†']  # ê°„ì†Œí™”ëœ ì²´í¬
            difficulty_score = sum(1 for pattern in difficult_patterns if pattern in text)
            quality -= min(0.2, difficulty_score * 0.02)
            
            # ìºë¦­í„°ì™€ ê°ì •ì˜ ë§¤ì¹­ë„
            character_emotion_bonus = {
                (VoiceCharacter.WARM_STORYTELLER, EmotionTone.COMPASSIONATE): 0.15,
                (VoiceCharacter.ENERGETIC_REPORTER, EmotionTone.EXCITED): 0.15,
                (VoiceCharacter.CALM_NARRATOR, EmotionTone.SERIOUS): 0.1,
                (VoiceCharacter.PROFESSIONAL_ANCHOR, EmotionTone.NEUTRAL): 0.1
            }
            
            bonus = character_emotion_bonus.get((character, emotion), 0.0)
            quality += bonus
            
            return max(0.0, min(1.0, quality))
            
        except Exception as e:
            logger.error(f"Quality calculation failed: {str(e)}")
            return 0.7
    
    def _update_synthesis_stats(self, voice_synthesis_result: VoiceSynthesisResult):
        """ìŒì„± í•©ì„± í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            self.synthesis_stats["total_syntheses"] += 1
            
            if voice_synthesis_result.synthesis_quality > 0.7:
                self.synthesis_stats["successful_syntheses"] += 1
            
            # í‰ê·  ì§€ì† ì‹œê°„ ì—…ë°ì´íŠ¸
            current_avg = self.synthesis_stats["average_duration"]
            total = self.synthesis_stats["total_syntheses"]
            new_duration = voice_synthesis_result.audio_duration
            
            self.synthesis_stats["average_duration"] = ((current_avg * (total - 1)) + new_duration) / total
            
            # ìºë¦­í„° ì‚¬ìš© í†µê³„
            character = voice_synthesis_result.voice_character
            if character in self.synthesis_stats["character_usage"]:
                self.synthesis_stats["character_usage"][character] += 1
            
            # í’ˆì§ˆ ì ìˆ˜
            self.synthesis_stats["quality_scores"].append(voice_synthesis_result.synthesis_quality)
            
        except Exception as e:
            logger.error(f"Synthesis stats update failed: {str(e)}")
    
    def get_voice_characters_info(self) -> Dict[str, Any]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ìºë¦­í„° ë³´ì´ìŠ¤ ì •ë³´ ë°˜í™˜"""
        characters_info = {}
        for character, config in self.voice_characters.items():
            characters_info[character.value] = {
                "name": config["name"],
                "description": config["description"],
                "suitable_for": config["suitable_for"],
                "usage_count": self.synthesis_stats["character_usage"].get(character.value, 0)
            }
        
        return {
            "available_characters": characters_info,
            "total_characters": len(self.voice_characters),
            "default_character": self.config.default_character.value
        }
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        try:
            total = self.synthesis_stats["total_syntheses"]
            if total == 0:
                return {"success_rate": 0.0, "total_syntheses": 0}
            
            success_rate = self.synthesis_stats["successful_syntheses"] / total
            
            quality_scores = self.synthesis_stats["quality_scores"]
            avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0.0
            
            # ê°€ì¥ ì¸ê¸° ìˆëŠ” ìºë¦­í„°
            character_usage = self.synthesis_stats["character_usage"]
            most_used_character = max(character_usage.items(), key=lambda x: x[1])[0] if character_usage else None
            
            return {
                "success_rate": success_rate,
                "total_syntheses": total,
                "average_duration": self.synthesis_stats["average_duration"],
                "average_quality": avg_quality,
                "most_used_character": most_used_character,
                "character_distribution": character_usage,
                "target_duration": "1ì´ˆ ì´ë‚´",
                "meets_target": self.synthesis_stats["average_duration"] <= 1.0
            }
        except Exception as e:
            logger.error(f"Performance stats calculation failed: {str(e)}")
            return {"success_rate": 0.0}
    
    def cleanup_temp_files(self, older_than_hours: int = 24):
        """ì„ì‹œ ì˜¤ë””ì˜¤ íŒŒì¼ ì •ë¦¬"""
        try:
            import time
            current_time = time.time()
            
            for filename in os.listdir(self.temp_audio_dir):
                file_path = os.path.join(self.temp_audio_dir, filename)
                if os.path.isfile(file_path):
                    file_age = current_time - os.path.getctime(file_path)
                    if file_age > (older_than_hours * 3600):  # ì‹œê°„ì„ ì´ˆë¡œ ë³€í™˜
                        os.remove(file_path)
                        logger.debug(f"Removed old audio file: {filename}")
                        
        except Exception as e:
            logger.error(f"Temp file cleanup failed: {str(e)}")
    
    def __del__(self):
        """ì†Œë©¸ì: ì„ì‹œ íŒŒì¼ ì •ë¦¬"""
        try:
            import shutil
            if hasattr(self, 'temp_audio_dir') and os.path.exists(self.temp_audio_dir):
                shutil.rmtree(self.temp_audio_dir)
        except Exception:
            pass  # ì†Œë©¸ìì—ì„œëŠ” ë¡œê¹…í•˜ì§€ ì•ŠìŒ 