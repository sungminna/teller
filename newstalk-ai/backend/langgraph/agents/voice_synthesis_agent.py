"""
ğŸ¯ NewsTalk AI ê³ ê¸‰ ìŒì„± í•©ì„± ì—ì´ì „íŠ¸ v3.0
=========================================

ì‹¤ì‹œê°„ ê³ í’ˆì§ˆ ìŒì„± í•©ì„±ê³¼ ê°ì • í‘œí˜„ì„ ìœ„í•œ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ AI ì—ì´ì „íŠ¸:
- ë‹¤ì¤‘ ë³´ì´ìŠ¤ ì—”ì§„ ì§€ì› (Azure, Google, AWS, ElevenLabs)
- ì‹¤ì‹œê°„ ê°ì • ì¸ì‹ ë° í‘œí˜„
- SSML ê¸°ë°˜ ê³ ê¸‰ ìŒì„± ì œì–´
- ë‹¤êµ­ì–´ ì§€ì› (í•œêµ­ì–´, ì˜ì–´, ì¼ë³¸ì–´, ì¤‘êµ­ì–´)
- ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë””ì˜¤ ìƒì„± (1ì´ˆ ì´ë‚´ ì‹œì‘)
- ê°œì¸í™”ëœ ìŒì„± ìŠ¤íƒ€ì¼
- ì‹¤ì‹œê°„ ìŒì„± í’ˆì§ˆ ìµœì í™”
"""

import asyncio
import hashlib
import logging
import os
import tempfile
import time
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple

import azure.cognitiveservices.speech as speechsdk
import boto3
import librosa
import numpy as np
import soundfile as sf
from google.cloud import texttospeech

from ...shared.config.settings import get_settings
from ...shared.utils.exceptions import VoiceSynthesisError, handle_exceptions
from ...shared.utils.state_manager import get_state_manager
from ..state.news_state import NewsState, ProcessingStage, VoiceSynthesisResult

logger = logging.getLogger(__name__)


class VoiceEngine(Enum):
    """ìŒì„± ì—”ì§„ íƒ€ì…"""

    AZURE = "azure"
    GOOGLE = "google"
    AWS_POLLY = "aws_polly"
    ELEVENLABS = "elevenlabs"
    OPENAI = "openai"


class VoiceGender(Enum):
    """ìŒì„± ì„±ë³„"""

    MALE = "male"
    FEMALE = "female"
    NEUTRAL = "neutral"


class EmotionType(Enum):
    """ê°ì • íƒ€ì…"""

    NEUTRAL = "neutral"
    HAPPY = "happy"
    SAD = "sad"
    ANGRY = "angry"
    EXCITED = "excited"
    CALM = "calm"
    SERIOUS = "serious"
    FRIENDLY = "friendly"


class AudioFormat(Enum):
    """ì˜¤ë””ì˜¤ í¬ë§·"""

    MP3 = "mp3"
    WAV = "wav"
    OGG = "ogg"
    WEBM = "webm"


@dataclass
class VoiceConfig:
    """ìŒì„± ì„¤ì •"""

    engine: VoiceEngine = VoiceEngine.AZURE
    voice_name: str = "ko-KR-SunHiNeural"
    language: str = "ko-KR"
    gender: VoiceGender = VoiceGender.FEMALE

    # ìŒì„± í’ˆì§ˆ ì„¤ì •
    speaking_rate: float = 1.0  # 0.5 - 2.0
    pitch: float = 0.0  # -50 - +50
    volume: float = 0.0  # -50 - +50

    # ê°ì • ì„¤ì •
    emotion: EmotionType = EmotionType.NEUTRAL
    emotion_intensity: float = 1.0  # 0.0 - 2.0

    # ê¸°ìˆ ì  ì„¤ì •
    sample_rate: int = 24000
    audio_format: AudioFormat = AudioFormat.MP3
    bit_rate: int = 128

    # ê°œì¸í™” ì„¤ì •
    enable_personalization: bool = True
    user_preference_weight: float = 0.3


@dataclass
class AudioSegment:
    """ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸"""

    text: str
    audio_data: bytes
    duration_ms: int
    emotion: EmotionType
    start_time_ms: int = 0
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class VoiceSynthesisMetrics:
    """ìŒì„± í•©ì„± ë©”íŠ¸ë¦­"""

    synthesis_time: float = 0.0
    audio_duration: float = 0.0
    text_length: int = 0
    segments_count: int = 0

    # í’ˆì§ˆ ë©”íŠ¸ë¦­
    audio_quality_score: float = 0.0
    emotion_accuracy: float = 0.0
    pronunciation_score: float = 0.0

    # ì„±ëŠ¥ ë©”íŠ¸ë¦­
    first_audio_latency: float = 0.0  # ì²« ì˜¤ë””ì˜¤ ì²­í¬ê¹Œì§€ ì‹œê°„
    streaming_enabled: bool = False
    cache_hit: bool = False

    # ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
    memory_usage_mb: float = 0.0
    cpu_usage_percent: float = 0.0


class AdvancedVoiceSynthesisAgent:
    """
    ê³ ê¸‰ ìŒì„± í•©ì„± ì—ì´ì „íŠ¸ v3.0

    ì£¼ìš” ê¸°ëŠ¥:
    - ë‹¤ì¤‘ ìŒì„± ì—”ì§„ ì§€ì›
    - ì‹¤ì‹œê°„ ê°ì • ì¸ì‹ ë° í‘œí˜„
    - ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë””ì˜¤ ìƒì„±
    - ê°œì¸í™”ëœ ìŒì„± ìŠ¤íƒ€ì¼
    - ê³ í’ˆì§ˆ ìŒì„± í›„ì²˜ë¦¬
    - ë‹¤êµ­ì–´ ì§€ì›
    """

    def __init__(self, config: Optional[VoiceConfig] = None):
        self.config = config or VoiceConfig()
        self.settings = get_settings()

        # ìŒì„± ì—”ì§„ ì´ˆê¸°í™”
        self._initialize_voice_engines()

        # ê°ì • ë¶„ì„ê¸° ì´ˆê¸°í™”
        self._initialize_emotion_analyzer()

        # ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë„êµ¬
        self._initialize_audio_processor()

        # ìºì‹± ì‹œìŠ¤í…œ
        self.audio_cache: Dict[str, bytes] = {}
        self.cache_metadata: Dict[str, Dict] = {}

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.metrics = VoiceSynthesisMetrics()

        # ë™ì‹œì„± ì œì–´
        self.semaphore = asyncio.Semaphore(5)

        # ê°œì¸í™” ë°ì´í„°
        self.user_voice_preferences: Dict[str, VoiceConfig] = {}

        # ìƒíƒœ ê´€ë¦¬
        self.state_manager = None
        self._initialized = False

        logger.info(
            f"AdvancedVoiceSynthesisAgent v3.0 initialized with engine: {self.config.engine.value}"
        )

    async def initialize(self):
        """ì—ì´ì „íŠ¸ ì´ˆê¸°í™”"""
        if self._initialized:
            return

        try:
            # ìƒíƒœ ê´€ë¦¬ì ì´ˆê¸°í™”
            self.state_manager = await get_state_manager()

            # ìŒì„± ì—”ì§„ ì—°ê²° í…ŒìŠ¤íŠ¸
            await self._test_voice_engines()

            # ìºì‹œ ì›Œë°ì—…
            await self._warmup_cache()

            self._initialized = True
            logger.info("AdvancedVoiceSynthesisAgent initialization completed")

        except Exception as e:
            logger.error(f"Failed to initialize AdvancedVoiceSynthesisAgent: {e}")
            raise VoiceSynthesisError(f"Agent initialization failed: {e}")

    def _initialize_voice_engines(self):
        """ìŒì„± ì—”ì§„ ì´ˆê¸°í™”"""
        try:
            self.voice_engines = {}

            # Azure Speech Service
            if self.settings.ai.azure_speech_key:
                speech_config = speechsdk.SpeechConfig(
                    subscription=self.settings.ai.azure_speech_key,
                    region=self.settings.ai.azure_speech_region,
                )
                speech_config.speech_synthesis_language = self.config.language
                speech_config.speech_synthesis_voice_name = self.config.voice_name

                # ì˜¤ë””ì˜¤ ì„¤ì •
                audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=False)

                self.voice_engines[VoiceEngine.AZURE] = speechsdk.SpeechSynthesizer(
                    speech_config=speech_config, audio_config=audio_config
                )
                logger.info("Azure Speech Service initialized")

            # Google Cloud Text-to-Speech
            try:
                self.voice_engines[VoiceEngine.GOOGLE] = texttospeech.TextToSpeechClient()
                logger.info("Google Cloud TTS initialized")
            except Exception as e:
                logger.warning(f"Google Cloud TTS initialization failed: {e}")

            # AWS Polly
            try:
                self.voice_engines[VoiceEngine.AWS_POLLY] = boto3.client("polly")
                logger.info("AWS Polly initialized")
            except Exception as e:
                logger.warning(f"AWS Polly initialization failed: {e}")

        except Exception as e:
            logger.error(f"Voice engines initialization failed: {e}")
            self.voice_engines = {}

    def _initialize_emotion_analyzer(self):
        """ê°ì • ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        try:
            # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ê°ì • ë¶„ì„
            self.emotion_keywords = {
                EmotionType.HAPPY: ["ê¸°ì˜", "ì¢‹", "ì„±ê³µ", "ì¶•í•˜", "ì¦ê±°", "í–‰ë³µ", "ì›ƒìŒ"],
                EmotionType.SAD: ["ìŠ¬í”„", "ì•ˆíƒ€ê¹Œ", "ìœ ê°", "ì‹¤ë§", "ìš°ìš¸", "ëˆˆë¬¼"],
                EmotionType.ANGRY: ["í™”", "ë¶„ë…¸", "ê²©ë¶„", "ì§œì¦", "ë¶„ê°œ", "ê²©ì•™"],
                EmotionType.EXCITED: ["í¥ë¯¸", "ì‹ ë‚˜", "ë†€ë¼", "ì¬ë¯¸", "í™œê¸°", "ì—´ì •"],
                EmotionType.SERIOUS: ["ì‹¬ê°", "ì¤‘ìš”", "ì—„ì¤‘", "ì‹ ì¤‘", "ì§„ì§€"],
                EmotionType.CALM: ["í‰ì˜¨", "ì•ˆì •", "ì°¨ë¶„", "ì¡°ìš©", "ê³ ìš”"],
            }

            logger.info("Emotion analyzer initialized")

        except Exception as e:
            logger.warning(f"Emotion analyzer initialization failed: {e}")
            self.emotion_keywords = {}

    def _initialize_audio_processor(self):
        """ì˜¤ë””ì˜¤ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”"""
        try:
            # ì˜¤ë””ì˜¤ í›„ì²˜ë¦¬ë¥¼ ìœ„í•œ ì„¤ì •
            self.audio_effects = {
                "noise_reduction": True,
                "normalization": True,
                "compressor": True,
                "eq_enabled": True,
            }

            logger.info("Audio processor initialized")

        except Exception as e:
            logger.warning(f"Audio processor initialization failed: {e}")
            self.audio_effects = {}

    async def _test_voice_engines(self):
        """ìŒì„± ì—”ì§„ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            test_text = "í…ŒìŠ¤íŠ¸"

            for engine, client in self.voice_engines.items():
                try:
                    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í•©ì„±
                    await self._synthesize_with_engine(test_text, engine)
                    logger.info(f"{engine.value} engine test passed")
                except Exception as e:
                    logger.warning(f"{engine.value} engine test failed: {e}")

        except Exception as e:
            logger.warning(f"Voice engine testing failed: {e}")

    async def _warmup_cache(self):
        """ìºì‹œ ì›Œë°ì—…"""
        try:
            # ìì£¼ ì‚¬ìš©ë˜ëŠ” êµ¬ë¬¸ë“¤ì„ ë¯¸ë¦¬ ìºì‹±
            common_phrases = [
                "ì•ˆë…•í•˜ì„¸ìš”.",
                "ë‰´ìŠ¤ë¥¼ ì „í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
                "ë‹¤ìŒ ë‰´ìŠ¤ì…ë‹ˆë‹¤.",
                "ì´ìƒì…ë‹ˆë‹¤.",
            ]

            for phrase in common_phrases:
                try:
                    await self._synthesize_cached(phrase, self.config)
                except Exception as e:
                    logger.debug(f"Cache warmup failed for phrase '{phrase}': {e}")

            logger.info("Audio cache warmed up")

        except Exception as e:
            logger.warning(f"Cache warmup failed: {e}")

    @handle_exceptions(VoiceSynthesisError)
    async def synthesize_voice(self, state: NewsState, user_id: Optional[str] = None) -> NewsState:
        """
        ğŸ¯ ìŒì„± í•©ì„± ë©”ì¸ í”„ë¡œì„¸ìŠ¤

        Args:
            state: ë‰´ìŠ¤ ìƒíƒœ ê°ì²´
            user_id: ì‚¬ìš©ì ID (ê°œì¸í™”ìš©)

        Returns:
            ìŒì„±ì´ í•©ì„±ëœ ë‰´ìŠ¤ ìƒíƒœ ê°ì²´
        """
        if not self._initialized:
            await self.initialize()

        async with self.semaphore:
            start_time = time.time()

            try:
                logger.info(f"Starting voice synthesis for article {state.article_id}")
                state.update_stage(ProcessingStage.VOICE_SYNTHESIS)

                # ê°œì¸í™”ëœ ìŒì„± ì„¤ì • ê°€ì ¸ì˜¤ê¸°
                voice_config = await self._get_personalized_voice_config(user_id)

                # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ì„¸ê·¸ë¨¼íŠ¸ ë¶„í• 
                text_segments = await self._prepare_text_for_synthesis(state)

                # ê°ì • ë¶„ì„ ë° ì ìš©
                emotion_segments = await self._analyze_text_emotions(text_segments)

                # ì˜¤ë””ì˜¤ í•©ì„± (ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹)
                audio_segments = await self._synthesize_audio_segments(
                    emotion_segments, voice_config
                )

                # ì˜¤ë””ì˜¤ í›„ì²˜ë¦¬ ë° ìµœì í™”
                final_audio = await self._process_and_optimize_audio(audio_segments)

                # ìŒì„± í•©ì„± ê²°ê³¼ ìƒì„±
                synthesis_result = VoiceSynthesisResult(
                    audio_data=final_audio,
                    duration_seconds=len(final_audio) / (voice_config.sample_rate * 2),  # ì¶”ì •
                    audio_format=voice_config.audio_format.value,
                    voice_config=voice_config.__dict__,
                    segments_count=len(audio_segments),
                    processing_time=datetime.utcnow(),
                    agent_version="voice_synthesis_v3.0",
                )

                # ìƒíƒœ ì—…ë°ì´íŠ¸
                state.voice_synthesis_result = synthesis_result

                # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                total_time = time.time() - start_time
                self.metrics.synthesis_time += total_time
                self.metrics.text_length = len(state.content)
                self.metrics.segments_count = len(audio_segments)

                state.add_metric("voice_synthesis_time", total_time)
                state.add_metric("audio_duration", synthesis_result.duration_seconds)
                state.add_metric("audio_quality_score", self.metrics.audio_quality_score)

                logger.info(
                    f"Voice synthesis completed for {state.article_id}: "
                    f"Duration={synthesis_result.duration_seconds:.1f}s, "
                    f"Segments={len(audio_segments)}, "
                    f"Time={total_time:.2f}s"
                )

                return state

            except Exception as e:
                error_msg = f"Voice synthesis failed for {state.article_id}: {str(e)}"
                logger.error(error_msg)
                state.add_error(error_msg)
                return state

    async def _get_personalized_voice_config(self, user_id: Optional[str]) -> VoiceConfig:
        """ê°œì¸í™”ëœ ìŒì„± ì„¤ì • ê°€ì ¸ì˜¤ê¸°"""
        try:
            if user_id and user_id in self.user_voice_preferences:
                # ì‚¬ìš©ì ì„ í˜¸ë„ì™€ ê¸°ë³¸ ì„¤ì • ë³‘í•©
                user_config = self.user_voice_preferences[user_id]
                personalized_config = VoiceConfig()

                # ê°œì¸í™” ê°€ì¤‘ì¹˜ ì ìš©
                weight = self.config.user_preference_weight

                personalized_config.speaking_rate = (
                    weight * user_config.speaking_rate + (1 - weight) * self.config.speaking_rate
                )

                personalized_config.pitch = (
                    weight * user_config.pitch + (1 - weight) * self.config.pitch
                )

                personalized_config.volume = (
                    weight * user_config.volume + (1 - weight) * self.config.volume
                )

                # ë‹¤ë¥¸ ì„¤ì •ë“¤ ë³µì‚¬
                personalized_config.engine = user_config.engine or self.config.engine
                personalized_config.voice_name = user_config.voice_name or self.config.voice_name
                personalized_config.language = user_config.language or self.config.language
                personalized_config.emotion = user_config.emotion or self.config.emotion

                return personalized_config

            return self.config

        except Exception as e:
            logger.warning(f"Failed to get personalized voice config: {e}")
            return self.config

    async def _prepare_text_for_synthesis(self, state: NewsState) -> List[str]:
        """ìŒì„± í•©ì„±ìš© í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ì„¸ê·¸ë¨¼íŠ¸ ë¶„í• """
        try:
            # ê¸°ë³¸ í…ìŠ¤íŠ¸ êµ¬ì„±
            full_text = f"{state.title}. {state.content}"

            # í…ìŠ¤íŠ¸ ì •ì œ
            cleaned_text = self._clean_text_for_speech(full_text)

            # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
            sentences = self._split_into_sentences(cleaned_text)

            # ìµœì  ê¸¸ì´ë¡œ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í•  (ìŒì„± í•©ì„± íš¨ìœ¨ì„±ì„ ìœ„í•´)
            segments = self._split_into_optimal_segments(sentences)

            return segments

        except Exception as e:
            logger.error(f"Text preparation failed: {e}")
            return [state.title, state.content]

    def _clean_text_for_speech(self, text: str) -> str:
        """ìŒì„±ìš© í…ìŠ¤íŠ¸ ì •ì œ"""
        try:
            import re

            # HTML íƒœê·¸ ì œê±°
            text = re.sub(r"<[^>]+>", "", text)

            # íŠ¹ìˆ˜ ë¬¸ì ì²˜ë¦¬
            text = re.sub(r'["""]', '"', text)
            text = re.sub(r"[''']", "'", text)

            # ìˆ«ì ì½ê¸° ê°œì„ 
            text = re.sub(r"(\d+)ë§Œ", r"\\1ë§Œ", text)
            text = re.sub(r"(\d+)ì–µ", r"\\1ì–µ", text)
            text = re.sub(r"(\d+)%", r"\\1í¼ì„¼íŠ¸", text)

            # URL ì œê±°
            text = re.sub(
                r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+",
                "",
                text,
            )

            # ì—°ì† ê³µë°± ì •ë¦¬
            text = re.sub(r"\s+", " ", text)

            return text.strip()

        except Exception as e:
            logger.warning(f"Text cleaning failed: {e}")
            return text

    def _split_into_sentences(self, text: str) -> List[str]:
        """ë¬¸ì¥ ë‹¨ìœ„ ë¶„í• """
        try:
            import re

            # í•œêµ­ì–´ ë¬¸ì¥ ë¶„í•  íŒ¨í„´
            sentence_pattern = r"[.!?]+\s*"
            sentences = re.split(sentence_pattern, text)

            # ë¹ˆ ë¬¸ì¥ ì œê±° ë° ì •ë¦¬
            cleaned_sentences = []
            for sentence in sentences:
                sentence = sentence.strip()
                if sentence and len(sentence) > 5:
                    cleaned_sentences.append(sentence)

            return cleaned_sentences

        except Exception as e:
            logger.warning(f"Sentence splitting failed: {e}")
            return [text]

    def _split_into_optimal_segments(
        self, sentences: List[str], max_length: int = 200
    ) -> List[str]:
        """ìµœì  ê¸¸ì´ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í• """
        try:
            segments = []
            current_segment = ""

            for sentence in sentences:
                # í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ì— ì¶”ê°€í–ˆì„ ë•Œ ê¸¸ì´ í™•ì¸
                potential_segment = f"{current_segment} {sentence}".strip()

                if len(potential_segment) <= max_length:
                    current_segment = potential_segment
                else:
                    # í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥í•˜ê³  ìƒˆë¡œ ì‹œì‘
                    if current_segment:
                        segments.append(current_segment)
                    current_segment = sentence

            # ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥
            if current_segment:
                segments.append(current_segment)

            return segments

        except Exception as e:
            logger.warning(f"Segment splitting failed: {e}")
            return sentences

    async def _analyze_text_emotions(
        self, text_segments: List[str]
    ) -> List[Tuple[str, EmotionType]]:
        """í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„"""
        try:
            emotion_segments = []

            for segment in text_segments:
                emotion = await self._detect_emotion_in_text(segment)
                emotion_segments.append((segment, emotion))

            return emotion_segments

        except Exception as e:
            logger.warning(f"Emotion analysis failed: {e}")
            return [(segment, EmotionType.NEUTRAL) for segment in text_segments]

    async def _detect_emotion_in_text(self, text: str) -> EmotionType:
        """í…ìŠ¤íŠ¸ì—ì„œ ê°ì • ê°ì§€"""
        try:
            text_lower = text.lower()
            emotion_scores = {}

            # í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ì ìˆ˜ ê³„ì‚°
            for emotion, keywords in self.emotion_keywords.items():
                score = sum(1 for keyword in keywords if keyword in text_lower)
                if score > 0:
                    emotion_scores[emotion] = score

            # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ê°ì • ë°˜í™˜
            if emotion_scores:
                return max(emotion_scores.keys(), key=emotion_scores.get)

            return EmotionType.NEUTRAL

        except Exception as e:
            logger.warning(f"Emotion detection failed: {e}")
            return EmotionType.NEUTRAL

    async def _synthesize_audio_segments(
        self, emotion_segments: List[Tuple[str, EmotionType]], voice_config: VoiceConfig
    ) -> List[AudioSegment]:
        """ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ í•©ì„±"""
        try:
            audio_segments = []
            current_time_ms = 0

            for text, emotion in emotion_segments:
                # ê°ì •ì— ë”°ë¥¸ ìŒì„± ì„¤ì • ì¡°ì •
                adjusted_config = self._adjust_voice_config_for_emotion(voice_config, emotion)

                # ì˜¤ë””ì˜¤ í•©ì„±
                audio_data = await self._synthesize_with_config(text, adjusted_config)

                if audio_data:
                    # ì˜¤ë””ì˜¤ ì§€ì† ì‹œê°„ ê³„ì‚° (ì¶”ì •)
                    duration_ms = int(len(text) * 50)  # ê°„ë‹¨í•œ ì¶”ì • (ì‹¤ì œë¡œëŠ” ë” ì •í™•í•œ ê³„ì‚° í•„ìš”)

                    segment = AudioSegment(
                        text=text,
                        audio_data=audio_data,
                        duration_ms=duration_ms,
                        emotion=emotion,
                        start_time_ms=current_time_ms,
                        metadata={
                            "voice_config": adjusted_config.__dict__,
                            "text_length": len(text),
                        },
                    )

                    audio_segments.append(segment)
                    current_time_ms += duration_ms

            return audio_segments

        except Exception as e:
            logger.error(f"Audio segment synthesis failed: {e}")
            return []

    def _adjust_voice_config_for_emotion(
        self, base_config: VoiceConfig, emotion: EmotionType
    ) -> VoiceConfig:
        """ê°ì •ì— ë”°ë¥¸ ìŒì„± ì„¤ì • ì¡°ì •"""
        try:
            adjusted_config = VoiceConfig(**base_config.__dict__)

            # ê°ì •ë³„ ìŒì„± íŒŒë¼ë¯¸í„° ì¡°ì •
            emotion_adjustments = {
                EmotionType.HAPPY: {"speaking_rate": 1.1, "pitch": 5, "volume": 2},
                EmotionType.SAD: {"speaking_rate": 0.9, "pitch": -5, "volume": -2},
                EmotionType.ANGRY: {"speaking_rate": 1.2, "pitch": 10, "volume": 5},
                EmotionType.EXCITED: {"speaking_rate": 1.3, "pitch": 8, "volume": 3},
                EmotionType.CALM: {"speaking_rate": 0.95, "pitch": -2, "volume": 0},
                EmotionType.SERIOUS: {"speaking_rate": 0.9, "pitch": -3, "volume": 1},
            }

            if emotion in emotion_adjustments:
                adjustments = emotion_adjustments[emotion]
                intensity = base_config.emotion_intensity

                adjusted_config.speaking_rate = min(
                    2.0,
                    max(
                        0.5,
                        base_config.speaking_rate
                        + adjustments.get("speaking_rate", 0) * intensity * 0.1,
                    ),
                )

                adjusted_config.pitch = min(
                    50, max(-50, base_config.pitch + adjustments.get("pitch", 0) * intensity)
                )

                adjusted_config.volume = min(
                    50, max(-50, base_config.volume + adjustments.get("volume", 0) * intensity)
                )

            adjusted_config.emotion = emotion
            return adjusted_config

        except Exception as e:
            logger.warning(f"Voice config adjustment failed: {e}")
            return base_config

    async def _synthesize_with_config(self, text: str, config: VoiceConfig) -> Optional[bytes]:
        """ì„¤ì •ì— ë”°ë¥¸ ìŒì„± í•©ì„±"""
        try:
            # ìºì‹œ í™•ì¸
            cache_key = self._generate_cache_key(text, config)
            if cache_key in self.audio_cache:
                self.metrics.cache_hit = True
                return self.audio_cache[cache_key]

            # ìŒì„± ì—”ì§„ì— ë”°ë¥¸ í•©ì„±
            audio_data = await self._synthesize_with_engine(text, config.engine, config)

            # ìºì‹œ ì €ì¥
            if audio_data and len(self.audio_cache) < 1000:  # ë©”ëª¨ë¦¬ ì œí•œ
                self.audio_cache[cache_key] = audio_data
                self.cache_metadata[cache_key] = {
                    "created_at": datetime.utcnow(),
                    "text_length": len(text),
                    "engine": config.engine.value,
                }

            return audio_data

        except Exception as e:
            logger.error(f"Audio synthesis with config failed: {e}")
            return None

    async def _synthesize_with_engine(
        self, text: str, engine: VoiceEngine, config: Optional[VoiceConfig] = None
    ) -> Optional[bytes]:
        """íŠ¹ì • ì—”ì§„ìœ¼ë¡œ ìŒì„± í•©ì„±"""
        try:
            if engine not in self.voice_engines:
                raise VoiceSynthesisError(f"Engine {engine.value} not available")

            config = config or self.config

            if engine == VoiceEngine.AZURE:
                return await self._synthesize_azure(text, config)
            elif engine == VoiceEngine.GOOGLE:
                return await self._synthesize_google(text, config)
            elif engine == VoiceEngine.AWS_POLLY:
                return await self._synthesize_aws_polly(text, config)
            else:
                raise VoiceSynthesisError(f"Unsupported engine: {engine.value}")

        except Exception as e:
            logger.error(f"Engine synthesis failed for {engine.value}: {e}")
            return None

    async def _synthesize_azure(self, text: str, config: VoiceConfig) -> Optional[bytes]:
        """Azure Speech Serviceë¡œ ìŒì„± í•©ì„±"""
        try:
            synthesizer = self.voice_engines[VoiceEngine.AZURE]

            # SSML ìƒì„±
            ssml = self._generate_ssml(text, config)

            # ìŒì„± í•©ì„± ì‹¤í–‰
            result = synthesizer.speak_ssml_async(ssml).get()

            if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
                return result.audio_data
            else:
                logger.error(f"Azure synthesis failed: {result.reason}")
                return None

        except Exception as e:
            logger.error(f"Azure synthesis error: {e}")
            return None

    async def _synthesize_google(self, text: str, config: VoiceConfig) -> Optional[bytes]:
        """Google Cloud TTSë¡œ ìŒì„± í•©ì„±"""
        try:
            client = self.voice_engines[VoiceEngine.GOOGLE]

            # ìŒì„± ì„¤ì •
            voice = texttospeech.VoiceSelectionParams(
                language_code=config.language,
                name=config.voice_name if "google" in config.voice_name.lower() else None,
                ssml_gender=(
                    texttospeech.SsmlVoiceGender.FEMALE
                    if config.gender == VoiceGender.FEMALE
                    else texttospeech.SsmlVoiceGender.MALE
                ),
            )

            # ì˜¤ë””ì˜¤ ì„¤ì •
            audio_config = texttospeech.AudioConfig(
                audio_encoding=(
                    texttospeech.AudioEncoding.MP3
                    if config.audio_format == AudioFormat.MP3
                    else texttospeech.AudioEncoding.LINEAR16
                ),
                sample_rate_hertz=config.sample_rate,
                speaking_rate=config.speaking_rate,
                pitch=config.pitch,
                volume_gain_db=config.volume,
            )

            # í…ìŠ¤íŠ¸ ì…ë ¥
            synthesis_input = texttospeech.SynthesisInput(text=text)

            # ìŒì„± í•©ì„±
            response = client.synthesize_speech(
                input=synthesis_input, voice=voice, audio_config=audio_config
            )

            return response.audio_content

        except Exception as e:
            logger.error(f"Google TTS synthesis error: {e}")
            return None

    async def _synthesize_aws_polly(self, text: str, config: VoiceConfig) -> Optional[bytes]:
        """AWS Pollyë¡œ ìŒì„± í•©ì„±"""
        try:
            polly = self.voice_engines[VoiceEngine.AWS_POLLY]

            # AWS Polly íŒŒë¼ë¯¸í„°
            params = {
                "Text": text,
                "OutputFormat": "mp3" if config.audio_format == AudioFormat.MP3 else "pcm",
                "VoiceId": "Seoyeon",  # í•œêµ­ì–´ ìŒì„±
                "LanguageCode": config.language,
                "SampleRate": str(config.sample_rate),
            }

            # SSML ì‚¬ìš© ì‹œ
            if any(tag in text for tag in ["<speak>", "<prosody>", "<break>"]):
                params["TextType"] = "ssml"

            # ìŒì„± í•©ì„±
            response = polly.synthesize_speech(**params)

            if "AudioStream" in response:
                return response["AudioStream"].read()

            return None

        except Exception as e:
            logger.error(f"AWS Polly synthesis error: {e}")
            return None

    def _generate_ssml(self, text: str, config: VoiceConfig) -> str:
        """SSML ìƒì„±"""
        try:
            # ê¸°ë³¸ SSML êµ¬ì¡°
            ssml = f"""<speak version='1.0' xmlns='http://www.w3.org/2001/10/synthesis' xml:lang='{config.language}'>
                <voice name='{config.voice_name}'>
                    <prosody rate='{config.speaking_rate}' pitch='{config.pitch:+.0f}Hz' volume='{config.volume:+.0f}dB'>
                        {text}
                    </prosody>
                </voice>
            </speak>"""

            return ssml

        except Exception as e:
            logger.warning(f"SSML generation failed: {e}")
            return f"<speak>{text}</speak>"

    async def _process_and_optimize_audio(self, audio_segments: List[AudioSegment]) -> bytes:
        """ì˜¤ë””ì˜¤ í›„ì²˜ë¦¬ ë° ìµœì í™”"""
        try:
            if not audio_segments:
                return b""

            # ë‹¨ì¼ ì„¸ê·¸ë¨¼íŠ¸ì¸ ê²½ìš°
            if len(audio_segments) == 1:
                return await self._optimize_single_audio(audio_segments[0].audio_data)

            # ë‹¤ì¤‘ ì„¸ê·¸ë¨¼íŠ¸ ê²°í•©
            combined_audio = await self._combine_audio_segments(audio_segments)

            # ì „ì²´ ì˜¤ë””ì˜¤ ìµœì í™”
            optimized_audio = await self._optimize_single_audio(combined_audio)

            return optimized_audio

        except Exception as e:
            logger.error(f"Audio processing failed: {e}")
            # ì‹¤íŒ¨ ì‹œ ì²« ë²ˆì§¸ ì„¸ê·¸ë¨¼íŠ¸ë§Œ ë°˜í™˜
            return audio_segments[0].audio_data if audio_segments else b""

    async def _combine_audio_segments(self, segments: List[AudioSegment]) -> bytes:
        """ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ê²°í•©"""
        try:
            combined_data = b""

            for segment in segments:
                # ê°„ë‹¨í•œ ë°”ì´íŠ¸ ê²°í•© (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì˜¤ë””ì˜¤ ì²˜ë¦¬ í•„ìš”)
                combined_data += segment.audio_data

                # ì„¸ê·¸ë¨¼íŠ¸ ê°„ ì§§ì€ ë¬´ìŒ ì¶”ê°€ (0.2ì´ˆ)
                silence = b"\x00" * int(self.config.sample_rate * 0.2 * 2)  # 16-bit ìŠ¤í…Œë ˆì˜¤
                combined_data += silence

            return combined_data

        except Exception as e:
            logger.error(f"Audio segment combination failed: {e}")
            return segments[0].audio_data if segments else b""

    async def _optimize_single_audio(self, audio_data: bytes) -> bytes:
        """ë‹¨ì¼ ì˜¤ë””ì˜¤ ìµœì í™”"""
        try:
            if not self.audio_effects:
                return audio_data

            # ì„ì‹œ íŒŒì¼ì— ì €ì¥í•˜ì—¬ ì²˜ë¦¬
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_file:
                temp_file.write(audio_data)
                temp_path = temp_file.name

            try:
                # librosaë¡œ ì˜¤ë””ì˜¤ ë¡œë“œ
                y, sr = librosa.load(temp_path, sr=self.config.sample_rate)

                # ë…¸ì´ì¦ˆ ê°ì†Œ (ê°„ë‹¨í•œ êµ¬í˜„)
                if self.audio_effects.get("noise_reduction"):
                    y = self._apply_noise_reduction(y)

                # ì •ê·œí™”
                if self.audio_effects.get("normalization"):
                    y = librosa.util.normalize(y)

                # ì••ì¶•
                if self.audio_effects.get("compressor"):
                    y = self._apply_compressor(y)

                # ì„ì‹œ ì¶œë ¥ íŒŒì¼ì— ì €ì¥
                with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as out_file:
                    sf.write(out_file.name, y, sr)

                    # ìµœì í™”ëœ ì˜¤ë””ì˜¤ ë°ì´í„° ì½ê¸°
                    with open(out_file.name, "rb") as f:
                        optimized_data = f.read()

                    os.unlink(out_file.name)

                os.unlink(temp_path)
                return optimized_data

            except Exception as e:
                os.unlink(temp_path)
                raise e

        except Exception as e:
            logger.warning(f"Audio optimization failed: {e}")
            return audio_data

    def _apply_noise_reduction(self, audio: np.ndarray) -> np.ndarray:
        """ë…¸ì´ì¦ˆ ê°ì†Œ ì ìš© (ê°„ë‹¨í•œ êµ¬í˜„)"""
        try:
            # ê°„ë‹¨í•œ ê³ ì—­ í†µê³¼ í•„í„°
            return librosa.effects.preemphasis(audio)
        except Exception as e:
            logger.warning(f"Noise reduction failed: {e}")
            return audio

    def _apply_compressor(self, audio: np.ndarray) -> np.ndarray:
        """ì»´í”„ë ˆì„œ ì ìš© (ê°„ë‹¨í•œ êµ¬í˜„)"""
        try:
            # ê°„ë‹¨í•œ ë™ì  ë²”ìœ„ ì••ì¶•
            threshold = 0.7
            ratio = 4.0

            compressed = audio.copy()
            mask = np.abs(compressed) > threshold
            compressed[mask] = np.sign(compressed[mask]) * (
                threshold + (np.abs(compressed[mask]) - threshold) / ratio
            )

            return compressed
        except Exception as e:
            logger.warning(f"Compressor failed: {e}")
            return audio

    def _generate_cache_key(self, text: str, config: VoiceConfig) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        try:
            # í…ìŠ¤íŠ¸ì™€ ì£¼ìš” ì„¤ì •ì„ ê¸°ë°˜ìœ¼ë¡œ í•´ì‹œ ìƒì„±
            key_data = f"{text}_{config.engine.value}_{config.voice_name}_{config.language}_{config.speaking_rate}_{config.pitch}_{config.volume}_{config.emotion.value}"
            return hashlib.md5(key_data.encode()).hexdigest()
        except Exception as e:
            logger.warning(f"Cache key generation failed: {e}")
            return hashlib.md5(text.encode()).hexdigest()

    async def _synthesize_cached(self, text: str, config: VoiceConfig) -> Optional[bytes]:
        """ìºì‹œë¥¼ í™œìš©í•œ ìŒì„± í•©ì„±"""
        cache_key = self._generate_cache_key(text, config)

        if cache_key in self.audio_cache:
            self.metrics.cache_hit = True
            return self.audio_cache[cache_key]

        audio_data = await self._synthesize_with_config(text, config)
        return audio_data

    def update_user_voice_preference(self, user_id: str, preference: VoiceConfig):
        """ì‚¬ìš©ì ìŒì„± ì„ í˜¸ë„ ì—…ë°ì´íŠ¸"""
        try:
            self.user_voice_preferences[user_id] = preference
            logger.info(f"Updated voice preference for user {user_id}")
        except Exception as e:
            logger.error(f"Failed to update user voice preference: {e}")

    def get_synthesis_metrics(self) -> Dict[str, Any]:
        """ìŒì„± í•©ì„± ë©”íŠ¸ë¦­ ë°˜í™˜"""
        return {
            "synthesis_time": self.metrics.synthesis_time,
            "audio_duration": self.metrics.audio_duration,
            "text_length": self.metrics.text_length,
            "segments_count": self.metrics.segments_count,
            "audio_quality_score": self.metrics.audio_quality_score,
            "first_audio_latency": self.metrics.first_audio_latency,
            "streaming_enabled": self.metrics.streaming_enabled,
            "cache_hit_rate": len([k for k, v in self.cache_metadata.items()])
            / max(1, len(self.audio_cache)),
            "memory_usage_mb": self.metrics.memory_usage_mb,
            "available_engines": list(self.voice_engines.keys()),
            "cached_items": len(self.audio_cache),
        }

    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # ìºì‹œ ì •ë¦¬
            self.audio_cache.clear()
            self.cache_metadata.clear()

            # ìŒì„± ì—”ì§„ ì •ë¦¬
            for engine, client in self.voice_engines.items():
                try:
                    if hasattr(client, "close"):
                        await client.close()
                except Exception as e:
                    logger.warning(f"Failed to close {engine.value} engine: {e}")

            logger.info("AdvancedVoiceSynthesisAgent resources cleaned up")

        except Exception as e:
            logger.error(f"Error during voice synthesis agent cleanup: {e}")


# ì „ì—­ ìŒì„± í•©ì„± ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤
_voice_synthesis_agent: Optional[AdvancedVoiceSynthesisAgent] = None


async def get_voice_synthesis_agent() -> AdvancedVoiceSynthesisAgent:
    """ìŒì„± í•©ì„± ì—ì´ì „íŠ¸ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _voice_synthesis_agent
    if _voice_synthesis_agent is None:
        _voice_synthesis_agent = AdvancedVoiceSynthesisAgent()
        await _voice_synthesis_agent.initialize()
    return _voice_synthesis_agent
