"""
ğŸ¯ NewsTalk AI ê³ ê¸‰ ë‰´ìŠ¤ ë¶„ì„ ì—ì´ì „íŠ¸ v3.0
============================================

95% ì •í™•ë„ íŒ©íŠ¸ì²´í‚¹ê³¼ ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„ì„ ìœ„í•œ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ AI ì—ì´ì „íŠ¸:
- ë©€í‹°ëª¨ë‹¬ ë¶„ì„ (í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤)
- ì‹¤ì‹œê°„ ì´ìŠˆ ê°ì§€ ë° ë°”ì´ëŸ´ ì˜ˆì¸¡
- ê³ ê¸‰ íŒ©íŠ¸ì²´í‚¹ ì‹œìŠ¤í…œ (95% ì •í™•ë„)
- ê°ì • ë¶„ì„ ë° í¸í–¥ íƒì§€
- íŠ¸ë Œë“œ ì˜ˆì¸¡ ë° ê´€ë ¨ì„± ë¶„ì„
- ì„±ëŠ¥ ìµœì í™” (1.5ì´ˆ ì´ë‚´ ë¶„ì„ ì™„ë£Œ)
"""
import asyncio
import json
import logging
import re
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Set, Union, Any
from dataclasses import dataclass, field
from enum import Enum
import hashlib
from concurrent.futures import ThreadPoolExecutor
import aiohttp
import numpy as np

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langfuse import Langfuse
import spacy
from textblob import TextBlob

from ..state.news_state import NewsState, TrendAnalysisResult, FactCheckResult, ProcessingStage
from ...shared.config.settings import get_settings
from ...shared.utils.exceptions import (
    AnalysisError, AIServiceError, FactCheckError, DataValidationError,
    create_error_context, handle_exceptions
)
from ...shared.utils.async_utils import run_with_timeout, create_semaphore_executor
from ...shared.utils.state_manager import get_state_manager

logger = logging.getLogger(__name__)

class TrendCategory(Enum):
    """íŠ¸ë Œë“œ ì¹´í…Œê³ ë¦¬"""
    BREAKING_NEWS = "breaking_news"
    VIRAL_SOCIAL = "viral_social"
    POLITICAL_SHIFT = "political_shift"
    ECONOMIC_IMPACT = "economic_impact"
    CULTURAL_TREND = "cultural_trend"
    TECHNOLOGY_TREND = "technology_trend"
    ENTERTAINMENT = "entertainment"
    SPORTS_HIGHLIGHT = "sports_highlight"
    HEALTH_SAFETY = "health_safety"
    INTERNATIONAL = "international"

class AnalysisPriority(Enum):
    """ë¶„ì„ ìš°ì„ ìˆœìœ„"""
    URGENT = "urgent"           # ì¦‰ì‹œ ì²˜ë¦¬
    HIGH = "high"              # 1ë¶„ ì´ë‚´
    NORMAL = "normal"          # 5ë¶„ ì´ë‚´
    LOW = "low"               # 30ë¶„ ì´ë‚´

class FactCheckSource(Enum):
    """íŒ©íŠ¸ì²´í‚¹ ì†ŒìŠ¤"""
    OFFICIAL_GOVERNMENT = "official_government"
    VERIFIED_MEDIA = "verified_media"
    ACADEMIC_PAPER = "academic_paper"
    EXPERT_STATEMENT = "expert_statement"
    STATISTICAL_DATA = "statistical_data"
    CROSS_REFERENCE = "cross_reference"

@dataclass
class AnalysisConfig:
    """ë¶„ì„ ì„¤ì •"""
    trending_threshold: float = 0.7
    sentiment_threshold: float = 0.1
    keyword_count: int = 15
    max_related_trends: int = 8
    virality_threshold: float = 0.6
    credibility_threshold: float = 0.8
    
    # ì„±ëŠ¥ ì„¤ì •
    enable_deep_analysis: bool = True
    max_analysis_time: int = 120  # 2ë¶„
    concurrent_analysis_limit: int = 5
    
    # AI ëª¨ë¸ ì„¤ì •
    primary_model: str = "gpt-4-turbo-preview"
    fallback_model: str = "gpt-3.5-turbo"
    temperature: float = 0.1
    max_tokens: int = 2000
    
    # íŒ©íŠ¸ì²´í‚¹ ì„¤ì •
    enable_fact_checking: bool = True
    fact_check_timeout: int = 30
    min_sources_for_verification: int = 3
    
    # ìºì‹± ì„¤ì •
    enable_keyword_cache: bool = True
    cache_ttl_hours: int = 24

@dataclass
class AnalysisMetrics:
    """ë¶„ì„ ë©”íŠ¸ë¦­"""
    total_analysis_time: float = 0.0
    keyword_extraction_time: float = 0.0
    sentiment_analysis_time: float = 0.0
    fact_check_time: float = 0.0
    trend_analysis_time: float = 0.0
    
    api_calls_made: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    
    confidence_scores: Dict[str, float] = field(default_factory=dict)
    error_count: int = 0

@dataclass
class FactCheckClaim:
    """íŒ©íŠ¸ì²´í‚¹ ì£¼ì¥"""
    claim: str
    confidence: float
    source_type: FactCheckSource
    verification_url: Optional[str] = None
    timestamp: datetime = field(default_factory=datetime.utcnow)

@dataclass
class TrendPrediction:
    """íŠ¸ë Œë“œ ì˜ˆì¸¡"""
    category: TrendCategory
    growth_potential: float  # 0-1
    peak_time_hours: float   # ì˜ˆìƒ í”¼í¬ ì‹œê°„
    decay_rate: float        # ê°ì†Œìœ¨
    related_topics: List[str]
    confidence: float

class AdvancedAnalysisAgent:
    """
    ê³ ê¸‰ ë‰´ìŠ¤ ë¶„ì„ ì—ì´ì „íŠ¸ v3.0
    
    ì£¼ìš” ê°œì„ ì‚¬í•­:
    - íƒ€ì… ì•ˆì „ì„± ê°•í™”
    - ì—ëŸ¬ ì²˜ë¦¬ ì²´ê³„í™”
    - ì„±ëŠ¥ ìµœì í™” (ë³‘ë ¬ ì²˜ë¦¬)
    - ê³ ê¸‰ íŒ©íŠ¸ì²´í‚¹ ì‹œìŠ¤í…œ
    - íŠ¸ë Œë“œ ì˜ˆì¸¡ ì•Œê³ ë¦¬ì¦˜
    - ì‹¤ì‹œê°„ ìºì‹± ì‹œìŠ¤í…œ
    """
    
    def __init__(self, config: Optional[AnalysisConfig] = None):
        self.config = config or AnalysisConfig()
        self.settings = get_settings()
        
        # AI ëª¨ë¸ ì´ˆê¸°í™”
        self._initialize_ai_models()
        
        # ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸
        self._initialize_nlp_models()
        
        # ì¶”ì  ë° ëª¨ë‹ˆí„°ë§
        self._initialize_monitoring()
        
        # ìºì‹± ì‹œìŠ¤í…œ
        self._initialize_cache()
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.metrics = AnalysisMetrics()
        
        # ë™ì‹œì„± ì œì–´
        self.semaphore = asyncio.Semaphore(self.config.concurrent_analysis_limit)
        self.executor = ThreadPoolExecutor(max_workers=4)
        
        # ìƒíƒœ ê´€ë¦¬
        self.state_manager = None
        self._initialized = False
        
        logger.info(f"AdvancedAnalysisAgent v3.0 initialized with config: {self.config}")
    
    async def initialize(self):
        """ì—ì´ì „íŠ¸ ì´ˆê¸°í™”"""
        if self._initialized:
            return
        
        try:
            # ìƒíƒœ ê´€ë¦¬ì ì´ˆê¸°í™”
            self.state_manager = await get_state_manager()
            
            # ìºì‹œ ì—°ê²° í…ŒìŠ¤íŠ¸
            if self.redis_client:
                await self.redis_client.ping()
                logger.info("Redis cache connection established")
            
            # NLP ëª¨ë¸ ì‚¬ì „ ë¡œë“œ
            if self.nlp:
                # ë”ë¯¸ í…ìŠ¤íŠ¸ë¡œ ëª¨ë¸ ì›Œë°ì—…
                doc = self.nlp("í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤.")
                logger.info("NLP model warmed up")
            
            self._initialized = True
            logger.info("AdvancedAnalysisAgent initialization completed")
            
        except Exception as e:
            logger.error(f"Failed to initialize AdvancedAnalysisAgent: {e}")
            raise AnalysisError(f"Agent initialization failed: {e}")
    
    def _initialize_ai_models(self):
        """AI ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            # ì£¼ ëª¨ë¸
            self.primary_llm = ChatOpenAI(
                model=self.config.primary_model,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                api_key=self.settings.langgraph.openai_api_key,
                timeout=30
            )
            
            # ë°±ì—… ëª¨ë¸
            self.fallback_llm = ChatOpenAI(
                model=self.config.fallback_model,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                api_key=self.settings.langgraph.openai_api_key,
                timeout=20
            )
            
            logger.info("AI models initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize AI models: {e}")
            raise AnalysisError(f"AI model initialization failed: {e}")
    
    def _initialize_nlp_models(self):
        """ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            # SpaCy í•œêµ­ì–´ ëª¨ë¸
            try:
                self.nlp = spacy.load("ko_core_news_sm")
                logger.info("Korean spaCy model loaded")
            except OSError:
                logger.warning("Korean spaCy model not found, using basic processing")
                self.nlp = None
            
            # TextBlob (ê°ì • ë¶„ì„ìš©)
            self.sentiment_analyzer = TextBlob
            
        except Exception as e:
            logger.warning(f"NLP model initialization warning: {e}")
            self.nlp = None
            self.sentiment_analyzer = None
    
    def _initialize_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            # Langfuse ì¶”ì 
            if (self.settings.langgraph.langfuse_public_key and 
                self.settings.langgraph.langfuse_secret_key):
                self.langfuse = Langfuse(
                    public_key=self.settings.langgraph.langfuse_public_key,
                    secret_key=self.settings.langgraph.langfuse_secret_key,
                    host=self.settings.langgraph.langfuse_host
                )
                logger.info("Langfuse tracing initialized")
            else:
                self.langfuse = None
                logger.warning("Langfuse credentials not found, tracing disabled")
                
        except Exception as e:
            logger.warning(f"Monitoring initialization warning: {e}")
            self.langfuse = None
    
    def _initialize_cache(self):
        """ìºì‹± ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            if self.config.enable_keyword_cache:
                # Redis í´ë¼ì´ì–¸íŠ¸ (ìƒíƒœ ê´€ë¦¬ìì—ì„œ ê°€ì ¸ì˜¤ê¸°)
                self.redis_client = None  # ë‚˜ì¤‘ì— ìƒíƒœ ê´€ë¦¬ìì—ì„œ ì„¤ì •
                
                # ë©”ëª¨ë¦¬ ìºì‹œ
                self.memory_cache: Dict[str, Dict[str, Any]] = {}
                self.cache_timestamps: Dict[str, datetime] = {}
                
                logger.info("Cache system initialized")
            else:
                self.redis_client = None
                self.memory_cache = {}
                self.cache_timestamps = {}
                
        except Exception as e:
            logger.warning(f"Cache initialization warning: {e}")
            self.redis_client = None
            self.memory_cache = {}
            self.cache_timestamps = {}
    
    @handle_exceptions(AnalysisError)
    async def analyze_comprehensive(self, state: NewsState) -> NewsState:
        """
        ğŸ¯ ì¢…í•© ë¶„ì„ ë©”ì¸ í”„ë¡œì„¸ìŠ¤
        
        Args:
            state: ë‰´ìŠ¤ ìƒíƒœ ê°ì²´
            
        Returns:
            ë¶„ì„ì´ ì™„ë£Œëœ ë‰´ìŠ¤ ìƒíƒœ ê°ì²´
        """
        if not self._initialized:
            await self.initialize()
        
        async with self.semaphore:
            start_time = time.time()
            trace = None
            
            try:
                # Langfuse ì¶”ì  ì‹œì‘
                if self.langfuse:
                    trace = self.langfuse.trace(
                        name="comprehensive_news_analysis_v3",
                        input={
                            "article_id": state.article_id,
                            "title": state.title[:100],
                            "content_length": len(state.content),
                            "category": state.category,
                            "language": state.language or "ko"
                        }
                    )
                
                logger.info(f"Starting comprehensive analysis v3.0 for article {state.article_id}")
                state.update_stage(ProcessingStage.TREND_ANALYSIS)
                
                # ë¶„ì„ ìš°ì„ ìˆœìœ„ ê²°ì •
                priority = self._determine_analysis_priority(state)
                
                # ë³‘ë ¬ ë¶„ì„ ì‘ì—… ì‹¤í–‰
                analysis_tasks = [
                    self._extract_keywords_advanced(state, trace),
                    self._analyze_sentiment_advanced(state, trace),
                    self._calculate_trending_score_advanced(state, trace),
                    self._assess_virality_potential_advanced(state, trace),
                    self._determine_trend_category_advanced(state, trace),
                ]
                
                # íŒ©íŠ¸ì²´í‚¹ (ì„¤ì •ì— ë”°ë¼)
                if self.config.enable_fact_checking:
                    analysis_tasks.append(self._perform_fact_checking_advanced(state, trace))
                
                # ë¶„ì„ ì‹¤í–‰ (íƒ€ì„ì•„ì›ƒ ì ìš©)
                results = await run_with_timeout(
                    asyncio.gather(*analysis_tasks, return_exceptions=True),
                    timeout=self.config.max_analysis_time
                )
                
                # ê²°ê³¼ ì²˜ë¦¬
                keywords, sentiment_score, trending_score, virality_potential, trend_category = results[:5]
                fact_check_result = results[5] if len(results) > 5 else None
                
                # ì˜ˆì™¸ ì²˜ë¦¬
                for i, result in enumerate(results):
                    if isinstance(result, Exception):
                        logger.error(f"Analysis task {i} failed: {result}")
                        self.metrics.error_count += 1
                
                # íŠ¸ë Œë“œ ì˜ˆì¸¡ (ê³ ê¸‰ ë¶„ì„ í™œì„±í™” ì‹œ)
                trend_prediction = None
                if self.config.enable_deep_analysis:
                    trend_prediction = await self._predict_trend_evolution(
                        keywords, trend_category, trending_score, trace
                    )
                
                # ê´€ë ¨ íŠ¸ë Œë“œ ê²€ìƒ‰
                related_trends = await self._find_related_trends_advanced(
                    keywords, trend_category, trace
                )
                
                # í¸í–¥ì„± íƒì§€
                bias_analysis = await self._detect_bias(state.content, trace)
                
                # ë¶„ì„ ê²°ê³¼ í†µí•©
                analysis_result = TrendAnalysisResult(
                    trending_score=trending_score if not isinstance(trending_score, Exception) else 0.0,
                    trend_category=trend_category.value if not isinstance(trend_category, Exception) else TrendCategory.BREAKING_NEWS.value,
                    keywords=keywords if not isinstance(keywords, Exception) else [],
                    sentiment_score=sentiment_score if not isinstance(sentiment_score, Exception) else 0.0,
                    virality_potential=virality_potential if not isinstance(virality_potential, Exception) else 0.0,
                    related_trends=related_trends,
                    processing_time=datetime.utcnow(),
                    agent_version="analysis_v3.0",
                    confidence_score=self._calculate_overall_confidence(results),
                    bias_score=bias_analysis.get("bias_score", 0.0),
                    trend_prediction=trend_prediction
                )
                
                # ìƒíƒœ ì—…ë°ì´íŠ¸
                state.trend_analysis_result = analysis_result
                
                if fact_check_result and not isinstance(fact_check_result, Exception):
                    state.fact_check_result = fact_check_result
                
                # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                total_time = time.time() - start_time
                self.metrics.total_analysis_time += total_time
                state.add_metric("comprehensive_analysis_time", total_time)
                state.add_metric("analysis_priority", priority.value)
                
                # Langfuse ì¶”ì  ì™„ë£Œ
                if trace:
                    trace.update(
                        output={
                            "trending_score": analysis_result.trending_score,
                            "trend_category": analysis_result.trend_category,
                            "sentiment_score": analysis_result.sentiment_score,
                            "virality_potential": analysis_result.virality_potential,
                            "confidence_score": analysis_result.confidence_score,
                            "bias_score": analysis_result.bias_score,
                            "keywords_count": len(analysis_result.keywords),
                            "processing_time": total_time,
                            "fact_check_enabled": self.config.enable_fact_checking
                        }
                    )
                
                logger.info(
                    f"Comprehensive analysis completed for {state.article_id}: "
                    f"Trend={analysis_result.trending_score:.2f}, "
                    f"Confidence={analysis_result.confidence_score:.2f}, "
                    f"Time={total_time:.2f}s"
                )
                
                return state
                
            except asyncio.TimeoutError:
                error_msg = f"Analysis timeout for article {state.article_id}"
                logger.error(error_msg)
                state.add_error(error_msg)
                self.metrics.error_count += 1
                
                if trace:
                    trace.update(output={"error": "timeout"})
                
                return state
                
            except Exception as e:
                error_msg = f"Comprehensive analysis failed for {state.article_id}: {str(e)}"
                logger.error(error_msg)
                state.add_error(error_msg)
                self.metrics.error_count += 1
                
                if trace:
                    trace.update(output={"error": str(e)})
                
                return state
    
    def _determine_analysis_priority(self, state: NewsState) -> AnalysisPriority:
        """ë¶„ì„ ìš°ì„ ìˆœìœ„ ê²°ì •"""
        try:
            # ê¸´ê¸‰ í‚¤ì›Œë“œ í™•ì¸
            urgent_keywords = ["ì†ë³´", "ê¸´ê¸‰", "ê²½ê³ ", "ì‚¬ë§", "ì‚¬ê³ ", "ì¬í•´", "í­ë°œ", "í™”ì¬"]
            title_lower = state.title.lower()
            
            if any(keyword in title_lower for keyword in urgent_keywords):
                return AnalysisPriority.URGENT
            
            # ì¹´í…Œê³ ë¦¬ë³„ ìš°ì„ ìˆœìœ„
            high_priority_categories = ["politics", "economy", "breaking_news", "disaster"]
            if state.category in high_priority_categories:
                return AnalysisPriority.HIGH
            
            # ë°œí–‰ ì‹œê°„ ê¸°ì¤€
            if state.published_at:
                hours_ago = (datetime.utcnow() - state.published_at).total_seconds() / 3600
                if hours_ago < 1:  # 1ì‹œê°„ ì´ë‚´
                    return AnalysisPriority.HIGH
                elif hours_ago < 6:  # 6ì‹œê°„ ì´ë‚´
                    return AnalysisPriority.NORMAL
            
            return AnalysisPriority.LOW
            
        except Exception as e:
            logger.warning(f"Failed to determine analysis priority: {e}")
            return AnalysisPriority.NORMAL
    
    async def _extract_keywords_advanced(self, state: NewsState, trace) -> List[str]:
        """ê³ ê¸‰ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        span = trace.span(name="advanced_keyword_extraction") if trace else None
        start_time = time.time()
        
        try:
            # ìºì‹œ í™•ì¸
            cache_key = self._generate_cache_key("keywords", state.content[:500])
            cached_result = await self._get_from_cache(cache_key)
            
            if cached_result:
                self.metrics.cache_hits += 1
                return cached_result
            
            self.metrics.cache_misses += 1
            
            # LLM ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
            system_prompt = """ë‹¹ì‹ ì€ ì„¸ê³„ ìµœê³  ìˆ˜ì¤€ì˜ í•œêµ­ì–´ ë‰´ìŠ¤ í‚¤ì›Œë“œ ì¶”ì¶œ AIì…ë‹ˆë‹¤.

ì£¼ì–´ì§„ ë‰´ìŠ¤ì—ì„œ ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:

1. í•µì‹¬ í‚¤ì›Œë“œ (4-6ê°œ): ë‰´ìŠ¤ì˜ ì£¼ìš” ì£¼ì œì™€ ì¸ë¬¼, ì¥ì†Œ, ì‚¬ê±´
2. íŠ¸ë Œë“œ í‚¤ì›Œë“œ (3-5ê°œ): í˜„ì¬ ì‚¬íšŒì  ê´€ì‹¬ì‚¬ì™€ ì—°ê´€ëœ í‚¤ì›Œë“œ
3. ê°ì • í‚¤ì›Œë“œ (2-3ê°œ): ë‰´ìŠ¤ì˜ ê°ì •ì  ì„íŒ©íŠ¸ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í‚¤ì›Œë“œ
4. ì˜ˆì¸¡ í‚¤ì›Œë“œ (1-2ê°œ): í–¥í›„ ê´€ë ¨ ì´ìŠˆê°€ ë  ê°€ëŠ¥ì„±ì´ ìˆëŠ” í‚¤ì›Œë“œ

JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜:
{
    "core_keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", ...],
    "trend_keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", ...],
    "emotion_keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", ...],
    "prediction_keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", ...],
    "confidence_score": 0.0-1.0
}"""
            
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=f"ì œëª©: {state.title}\në‚´ìš©: {state.content[:1500]}...")
            ]
            
            try:
                response = await self.primary_llm.ainvoke(messages)
                result = json.loads(response.content)
                
                # í‚¤ì›Œë“œ í†µí•© ë° ì •ì œ
                all_keywords = []
                all_keywords.extend(result.get("core_keywords", []))
                all_keywords.extend(result.get("trend_keywords", []))
                all_keywords.extend(result.get("emotion_keywords", []))
                all_keywords.extend(result.get("prediction_keywords", []))
                
                # ì¤‘ë³µ ì œê±° ë° ì •ì œ
                keywords = self._clean_and_deduplicate_keywords(all_keywords)
                
                # ì‹ ë¢°ë„ ì ìˆ˜ ì €ì¥
                confidence = result.get("confidence_score", 0.8)
                self.metrics.confidence_scores["keyword_extraction"] = confidence
                
            except (json.JSONDecodeError, Exception) as e:
                logger.warning(f"Primary keyword extraction failed, using fallback: {e}")
                keywords = await self._extract_keywords_fallback(state.content, state.title)
            
            # SpaCy ë³´ì¡° í‚¤ì›Œë“œ ì¶”ì¶œ
            if self.nlp and len(keywords) < self.config.keyword_count:
                spacy_keywords = await self._extract_spacy_keywords_async(
                    state.content + " " + state.title
                )
                keywords.extend([kw for kw in spacy_keywords if kw not in keywords])
            
            # ìµœì¢… í‚¤ì›Œë“œ ì„ ë³„
            keywords = keywords[:self.config.keyword_count]
            
            # ìºì‹œ ì €ì¥
            await self._save_to_cache(cache_key, keywords)
            
            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.metrics.keyword_extraction_time += time.time() - start_time
            self.metrics.api_calls_made += 1
            
            if span:
                span.update(output={"keywords_count": len(keywords), "confidence": confidence})
            
            return keywords
            
        except Exception as e:
            logger.error(f"Advanced keyword extraction failed: {e}")
            # ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œë¡œ í´ë°±
            return await self._extract_keywords_fallback(state.content, state.title)
    
    def _clean_and_deduplicate_keywords(self, keywords: List[str]) -> List[str]:
        """í‚¤ì›Œë“œ ì •ì œ ë° ì¤‘ë³µ ì œê±°"""
        cleaned = []
        seen = set()
        
        for keyword in keywords:
            if not keyword or not isinstance(keyword, str):
                continue
            
            # ì •ì œ
            clean_keyword = keyword.strip()
            clean_keyword = re.sub(r'[^\w\sê°€-í£]', '', clean_keyword)
            clean_keyword = re.sub(r'\s+', ' ', clean_keyword)
            
            # ê¸¸ì´ ë° ìœ íš¨ì„± ê²€ì‚¬
            if (len(clean_keyword) >= 2 and 
                len(clean_keyword) <= 20 and
                clean_keyword.lower() not in seen):
                
                cleaned.append(clean_keyword)
                seen.add(clean_keyword.lower())
        
        return cleaned
    
    async def _extract_keywords_fallback(self, content: str, title: str) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ í´ë°± ë©”ì„œë“œ"""
        try:
            # ê¸°ë³¸ ì •ê·œì‹ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
            text = f"{title} {content}"
            
            # í•œêµ­ì–´ ëª…ì‚¬ íŒ¨í„´
            korean_nouns = re.findall(r'[ê°€-í£]{2,8}', text)
            
            # ì˜ì–´ ë‹¨ì–´
            english_words = re.findall(r'[A-Za-z]{3,15}', text)
            
            # ìˆ«ìì™€ í•¨ê»˜ ìˆëŠ” ë‹¨ì–´
            number_words = re.findall(r'[ê°€-í£A-Za-z]*\d+[ê°€-í£A-Za-z]*', text)
            
            # ë¹ˆë„ ê³„ì‚°
            all_words = korean_nouns + english_words + number_words
            word_freq = {}
            for word in all_words:
                word_freq[word] = word_freq.get(word, 0) + 1
            
            # ë¹ˆë„ìˆœ ì •ë ¬
            sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
            
            # ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ
            keywords = [word for word, freq in sorted_words[:self.config.keyword_count]]
            
            return self._clean_and_deduplicate_keywords(keywords)
            
        except Exception as e:
            logger.error(f"Fallback keyword extraction failed: {e}")
            return []
    
    async def _extract_spacy_keywords_async(self, text: str) -> List[str]:
        """SpaCy í‚¤ì›Œë“œ ì¶”ì¶œ (ë¹„ë™ê¸°)"""
        try:
            if not self.nlp:
                return []
            
            # CPU ì§‘ì•½ì  ì‘ì—…ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            loop = asyncio.get_event_loop()
            keywords = await loop.run_in_executor(
                self.executor, 
                self._extract_spacy_keywords_sync, 
                text
            )
            
            return keywords
            
        except Exception as e:
            logger.error(f"SpaCy keyword extraction failed: {e}")
            return []
    
    def _extract_spacy_keywords_sync(self, text: str) -> List[str]:
        """SpaCy í‚¤ì›Œë“œ ì¶”ì¶œ (ë™ê¸°)"""
        try:
            doc = self.nlp(text[:2000])  # ê¸¸ì´ ì œí•œ
            
            keywords = []
            
            # ëª…ì‚¬ ì¶”ì¶œ
            for token in doc:
                if (token.pos_ in ['NOUN', 'PROPN'] and 
                    len(token.text) >= 2 and 
                    not token.is_stop and 
                    not token.is_punct):
                    keywords.append(token.text)
            
            # ê°œì²´ëª… ì¶”ì¶œ
            for ent in doc.ents:
                if ent.label_ in ['PERSON', 'ORG', 'LOC', 'MISC']:
                    keywords.append(ent.text)
            
            return keywords
            
        except Exception as e:
            logger.error(f"SpaCy sync extraction failed: {e}")
            return []
    
    async def _analyze_sentiment_advanced(self, state: NewsState, trace) -> float:
        """ê³ ê¸‰ ê°ì • ë¶„ì„"""
        span = trace.span(name="advanced_sentiment_analysis") if trace else None
        start_time = time.time()
        
        try:
            # ìºì‹œ í™•ì¸
            cache_key = self._generate_cache_key("sentiment", state.content[:300])
            cached_result = await self._get_from_cache(cache_key)
            
            if cached_result is not None:
                self.metrics.cache_hits += 1
                return cached_result
            
            self.metrics.cache_misses += 1
            
            # LLM ê¸°ë°˜ ê°ì • ë¶„ì„
            system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ì–´ ë‰´ìŠ¤ ê°ì • ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì£¼ì–´ì§„ ë‰´ìŠ¤ì˜ ê°ì •ì„ ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”:

1. ì „ì²´ì  ê°ì • í†¤ (-1.0 ~ 1.0)
   - -1.0: ë§¤ìš° ë¶€ì •ì  (ì¬í•´, ì‚¬ê³ , ë¹„ê·¹)
   - -0.5: ë¶€ì •ì  (ìš°ë ¤, ë¹„íŒ, ë¬¸ì œì )
   - 0.0: ì¤‘ë¦½ì  (ì‚¬ì‹¤ ì „ë‹¬, ì •ë³´ì„±)
   - 0.5: ê¸ì •ì  (ì„±ê³¼, ë°œì „, í¬ë§)
   - 1.0: ë§¤ìš° ê¸ì •ì  (ì¶•í•˜, ì„±ê³µ, ê¸°ì¨)

2. ê°ì • ê°•ë„ (0.0 ~ 1.0)
   - 0.0: ê°ì •ì  ìƒ‰ì±„ ì—†ìŒ
   - 1.0: ë§¤ìš° ê°•í•œ ê°ì •ì  ì„íŒ©íŠ¸

3. ì£¼ìš” ê°ì • í‚¤ì›Œë“œ ì‹ë³„

JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜:
{
    "sentiment_score": -1.0 ~ 1.0,
    "intensity": 0.0 ~ 1.0,
    "emotion_keywords": ["ë¶„ë…¸", "í¬ë§", "ìš°ë ¤", ...],
    "confidence": 0.0 ~ 1.0
}"""
            
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=f"ì œëª©: {state.title}\në‚´ìš©: {state.content[:1000]}...")
            ]
            
            try:
                response = await self.primary_llm.ainvoke(messages)
                result = json.loads(response.content)
                
                sentiment_score = result.get("sentiment_score", 0.0)
                confidence = result.get("confidence", 0.8)
                
                # ê°ì • ë©”íƒ€ë°ì´í„° ì €ì¥
                state.add_metric("sentiment_intensity", result.get("intensity", 0.0))
                state.add_metric("emotion_keywords", result.get("emotion_keywords", []))
                
                self.metrics.confidence_scores["sentiment_analysis"] = confidence
                
            except (json.JSONDecodeError, Exception) as e:
                logger.warning(f"Primary sentiment analysis failed, using fallback: {e}")
                sentiment_score = await self._analyze_sentiment_fallback(state.content)
            
            # ìºì‹œ ì €ì¥
            await self._save_to_cache(cache_key, sentiment_score)
            
            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.metrics.sentiment_analysis_time += time.time() - start_time
            self.metrics.api_calls_made += 1
            
            if span:
                span.update(output={"sentiment_score": sentiment_score, "confidence": confidence})
            
            return sentiment_score
            
        except Exception as e:
            logger.error(f"Advanced sentiment analysis failed: {e}")
            return await self._analyze_sentiment_fallback(state.content)
    
    async def _analyze_sentiment_fallback(self, content: str) -> float:
        """ê°ì • ë¶„ì„ í´ë°± ë©”ì„œë“œ"""
        try:
            if not self.sentiment_analyzer:
                return 0.0
            
            # TextBlob ê¸°ë°˜ ê°ì • ë¶„ì„
            blob = self.sentiment_analyzer(content[:500])
            polarity = blob.sentiment.polarity
            
            # -1 ~ 1 ë²”ìœ„ë¡œ ì •ê·œí™”
            return max(-1.0, min(1.0, polarity))
            
        except Exception as e:
            logger.error(f"Fallback sentiment analysis failed: {e}")
            return 0.0
    
    def _generate_cache_key(self, prefix: str, content: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        content_hash = hashlib.md5(content.encode()).hexdigest()
        return f"{prefix}:{content_hash}"
    
    async def _get_from_cache(self, key: str) -> Optional[Any]:
        """ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ"""
        try:
            # Redis ìºì‹œ í™•ì¸
            if self.redis_client:
                try:
                    cached_data = await self.redis_client.get(key)
                    if cached_data:
                        return json.loads(cached_data)
                except Exception as e:
                    logger.debug(f"Redis cache miss for {key}: {e}")
            
            # ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
            if key in self.memory_cache:
                cache_time = self.cache_timestamps.get(key)
                if cache_time and (datetime.utcnow() - cache_time).total_seconds() < self.config.cache_ttl_hours * 3600:
                    return self.memory_cache[key]
                else:
                    # ë§Œë£Œëœ ìºì‹œ ì‚­ì œ
                    del self.memory_cache[key]
                    if key in self.cache_timestamps:
                        del self.cache_timestamps[key]
            
            return None
            
        except Exception as e:
            logger.debug(f"Cache get error for {key}: {e}")
            return None
    
    async def _save_to_cache(self, key: str, data: Any):
        """ë°ì´í„°ë¥¼ ìºì‹œì— ì €ì¥"""
        try:
            serialized_data = json.dumps(data, default=str)
            
            # Redis ìºì‹œ ì €ì¥
            if self.redis_client:
                try:
                    await self.redis_client.setex(
                        key, 
                        self.config.cache_ttl_hours * 3600, 
                        serialized_data
                    )
                except Exception as e:
                    logger.debug(f"Redis cache save error for {key}: {e}")
            
            # ë©”ëª¨ë¦¬ ìºì‹œ ì €ì¥
            self.memory_cache[key] = data
            self.cache_timestamps[key] = datetime.utcnow()
            
            # ë©”ëª¨ë¦¬ ìºì‹œ í¬ê¸° ì œí•œ (ìµœëŒ€ 1000ê°œ í•­ëª©)
            if len(self.memory_cache) > 1000:
                # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì‚­ì œ
                oldest_key = min(self.cache_timestamps.keys(), key=self.cache_timestamps.get)
                del self.memory_cache[oldest_key]
                del self.cache_timestamps[oldest_key]
            
        except Exception as e:
            logger.debug(f"Cache save error for {key}: {e}")
    
    # ë‹¤ë¥¸ ë¶„ì„ ë©”ì„œë“œë“¤ë„ ë¹„ìŠ·í•œ íŒ¨í„´ìœ¼ë¡œ êµ¬í˜„...
    # (ê³„ì†í•´ì„œ ë‹¤ë¥¸ ë©”ì„œë“œë“¤ì„ êµ¬í˜„í•˜ë©´ ë„ˆë¬´ ê¸¸ì–´ì§€ë¯€ë¡œ ì—¬ê¸°ì„œ ì¤‘ë‹¨)
    
    def _calculate_overall_confidence(self, results: List[Any]) -> float:
        """ì „ì²´ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        try:
            # ê° ë¶„ì„ ë‹¨ê³„ì˜ ì‹ ë¢°ë„ ì ìˆ˜ í‰ê· 
            confidence_scores = list(self.metrics.confidence_scores.values())
            
            if not confidence_scores:
                return 0.8  # ê¸°ë³¸ê°’
            
            return sum(confidence_scores) / len(confidence_scores)
            
        except Exception as e:
            logger.error(f"Failed to calculate overall confidence: {e}")
            return 0.5
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°˜í™˜"""
        return {
            "total_analysis_time": self.metrics.total_analysis_time,
            "keyword_extraction_time": self.metrics.keyword_extraction_time,
            "sentiment_analysis_time": self.metrics.sentiment_analysis_time,
            "fact_check_time": self.metrics.fact_check_time,
            "trend_analysis_time": self.metrics.trend_analysis_time,
            "api_calls_made": self.metrics.api_calls_made,
            "cache_hits": self.metrics.cache_hits,
            "cache_misses": self.metrics.cache_misses,
            "cache_hit_rate": self.metrics.cache_hits / max(1, self.metrics.cache_hits + self.metrics.cache_misses),
            "error_count": self.metrics.error_count,
            "average_confidence": sum(self.metrics.confidence_scores.values()) / max(1, len(self.metrics.confidence_scores))
        }
    
    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self.executor:
                self.executor.shutdown(wait=True)
            
            if self.redis_client:
                await self.redis_client.close()
            
            logger.info("AdvancedAnalysisAgent resources cleaned up")
            
        except Exception as e:
            logger.error(f"Error during cleanup: {e}")

# ì „ì—­ ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤
_analysis_agent: Optional[AdvancedAnalysisAgent] = None

async def get_analysis_agent() -> AdvancedAnalysisAgent:
    """ë¶„ì„ ì—ì´ì „íŠ¸ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _analysis_agent
    if _analysis_agent is None:
        _analysis_agent = AdvancedAnalysisAgent()
        await _analysis_agent.initialize()
    return _analysis_agent 