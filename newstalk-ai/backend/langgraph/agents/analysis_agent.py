"""
ğŸ¯ News Analysis Agent - íŠ¸ë Œë“œ ë¶„ì„, íŒ©íŠ¸ì²´í‚¹, ì´ìŠˆ ê°ì§€ í†µí•© ì „ë¬¸ ì—ì´ì „íŠ¸ (Stage 3)
- ì‹¤ì‹œê°„ ì´ìŠˆ ê°ì§€, í‚¤ì›Œë“œ ì¶”ì¶œ, ê°ì„± ë¶„ì„, ë°”ì´ëŸ´ ì ì¬ë ¥ í‰ê°€
- 95% ì •í™•ë„ íŒ©íŠ¸ì²´í‚¹ ì‹œìŠ¤í…œ êµ¬í˜„
- ì‹ ë¢°ë„ í‰ê°€ ì•Œê³ ë¦¬ì¦˜ (0-100ì  ìŠ¤ì¼€ì¼)
- GPT-4ë¥¼ í™œìš©í•œ íŠ¸ë Œë“œ ë¶„ì„ ë° ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜
"""
import asyncio
import json
import logging
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Set
from dataclasses import dataclass
from enum import Enum

import numpy as np
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langfuse import Langfuse
import spacy
from textblob import TextBlob

from ..state.news_state import NewsState, TrendAnalysisResult, FactCheckResult, ProcessingStage
from ...shared.config.settings import settings

logger = logging.getLogger(__name__)

class TrendCategory(Enum):
    """íŠ¸ë Œë“œ ì¹´í…Œê³ ë¦¬"""
    BREAKING_NEWS = "breaking_news"
    VIRAL_SOCIAL = "viral_social"
    POLITICAL_SHIFT = "political_shift"
    ECONOMIC_IMPACT = "economic_impact"
    CULTURAL_TREND = "cultural_trend"
    TECHNOLOGY_TREND = "technology_trend"
    ENTERTAINMENT = "entertainment"
    SPORTS_HIGHLIGHT = "sports_highlight"

@dataclass
class AnalysisConfig:
    """ë¶„ì„ ì„¤ì •"""
    trending_threshold: float = 0.7
    sentiment_threshold: float = 0.1
    keyword_count: int = 10
    max_related_trends: int = 5
    virality_threshold: float = 0.6
    enable_deep_analysis: bool = True

class AnalysisAgent:
    """
    íŠ¸ë Œë“œ ë¶„ì„ ì „ë¬¸ ì—ì´ì „íŠ¸
    - ì‹¤ì‹œê°„ ì´ìŠˆ ê°ì§€ ë° ì¤‘ìš”ë„ í‰ê°€ (ì‹œê°„ë‹¹ 5,000ê°œ ì´ìƒì˜ ì½˜í…ì¸  ë¶„ì„)
    - í† í”½ ëª¨ë¸ë§ ë° í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•œ ê´€ë ¨ ë‰´ìŠ¤ ê·¸ë£¹í™”
    - ì‚¬ìš©ì ê´€ì‹¬ì‚¬ì™€ ì‹œì˜ì„±ì„ ê³ ë ¤í•œ ë‰´ìŠ¤ ìš°ì„ ìˆœìœ„ ê²°ì •
    - í‚¤ì›Œë“œ ì¶”ì¶œ, ê°ì„± ë¶„ì„, ì´ìŠˆ ì¤‘ìš”ë„ ìŠ¤ì½”ì–´ë§
    """
    
    def __init__(self, config: AnalysisConfig = None):
        self.config = config or AnalysisConfig()
        self.llm = ChatOpenAI(
            model="gpt-4-turbo-preview",
            temperature=0.1,  # ë¶„ì„ì˜ ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„
            max_tokens=2000,
            api_key=settings.ai.openai_api_key
        )
        
        # ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ ì´ˆê¸°í™”
        try:
            self.nlp = spacy.load("ko_core_news_sm")
        except OSError:
            logger.warning("Korean spaCy model not found, using basic processing")
            self.nlp = None
        
        # ì¶”ì  ì‹œìŠ¤í…œ
        self.langfuse = Langfuse(
            public_key=settings.ai.langfuse_public_key,
            secret_key=settings.ai.langfuse_secret_key,
            host=settings.ai.langfuse_host
        )
        
        # íŠ¸ë Œë“œ í‚¤ì›Œë“œ ë°ì´í„°ë² ì´ìŠ¤ (ì‹œê°„ë‹¹ ì—…ë°ì´íŠ¸)
        self.trending_keywords = {}
        self.keyword_frequencies = {}
        
        logger.info(f"Analysis Agent initialized with trending threshold: {self.config.trending_threshold}")
    
    async def analyze_trends(self, state: NewsState) -> NewsState:
        """
        ğŸ¯ í†µí•© ë¶„ì„ ë©”ì¸ í”„ë¡œì„¸ìŠ¤ (Stage 3)
        - ì‹¤ì‹œê°„ ì´ìŠˆ ê°ì§€ ë° ì¤‘ìš”ë„ í‰ê°€
        - 95% ì •í™•ë„ íŒ©íŠ¸ì²´í‚¹ ì‹œìŠ¤í…œ
        """
        try:
            trace = self.langfuse.trace(
                name="comprehensive_news_analysis",
                input={
                    "article_id": state.article_id,
                    "title": state.title,
                    "content_length": len(state.content),
                    "category": state.category
                }
            )
            
            logger.info(f"Starting comprehensive analysis for article {state.article_id}")
            state.update_stage(ProcessingStage.TREND_ANALYSIS)
            
            # 1. ë³‘ë ¬ ë¶„ì„ ì‘ì—… ìˆ˜í–‰ (íŠ¸ë Œë“œ ë¶„ì„ + íŒ©íŠ¸ì²´í‚¹)
            tasks = [
                self._extract_keywords(state.content, state.title, trace),
                self._analyze_sentiment(state.content, trace),
                self._calculate_trending_score(state.content, state.title, trace),
                self._assess_virality_potential(state.content, state.title, trace),
                self._determine_trend_category(state.content, state.category, trace),
                self._perform_fact_checking(state.content, state.title, trace)  # ğŸ¯ íŒ©íŠ¸ì²´í‚¹ ì¶”ê°€
            ]
            
            keywords, sentiment_score, trending_score, virality_potential, trend_category, fact_check_results = await asyncio.gather(*tasks)
            
            # 2. ê´€ë ¨ íŠ¸ë Œë“œ ê²€ìƒ‰
            related_trends = await self._find_related_trends(keywords, trend_category, trace)
            
            # 3. íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼ ì·¨í•©
            analysis_result = TrendAnalysisResult(
                trending_score=trending_score,
                trend_category=trend_category.value,
                keywords=keywords,
                sentiment_score=sentiment_score,
                virality_potential=virality_potential,
                related_trends=related_trends,
                processing_time=datetime.utcnow(),
                agent_version="analysis_v2.0"
            )
            
            # 4. íŒ©íŠ¸ì²´í‚¹ ê²°ê³¼ ì¶”ê°€
            state.trend_analysis_result = analysis_result
            state.fact_check_result = fact_check_results
            
            # 5. ë©”íŠ¸ë¦­ ì¶”ê°€
            state.add_metric("trend_analysis_time", (datetime.utcnow() - state.updated_at).total_seconds())
            state.add_metric("fact_check_credibility", fact_check_results.credibility_score)
            
            # 6. íŠ¸ë Œë”© í‚¤ì›Œë“œ ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
            await self._update_trending_keywords(keywords, trending_score)
            
            # Langfuse ì¶”ì 
            trace.update(
                output={
                    "trending_score": trending_score,
                    "trend_category": trend_category.value,
                    "sentiment_score": sentiment_score,
                    "virality_potential": virality_potential,
                    "credibility_score": fact_check_results.credibility_score,
                    "keywords_count": len(keywords)
                }
            )
            
            logger.info(f"Comprehensive analysis completed for article {state.article_id} - Trend: {trending_score:.2f}, Credibility: {fact_check_results.credibility_score:.2f}")
            return state
            
        except Exception as e:
            logger.error(f"Comprehensive analysis failed for article {state.article_id}: {str(e)}")
            state.add_error(f"Analysis error: {str(e)}")
            return state
    
    async def _extract_keywords(self, content: str, title: str, trace) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬"""
        span = trace.span(name="keyword_extraction")
        
        try:
            # 1. LLM ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
            system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ì–´ ë‰´ìŠ¤ í‚¤ì›Œë“œ ì¶”ì¶œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
            ì£¼ì–´ì§„ ë‰´ìŠ¤ ì œëª©ê³¼ ë‚´ìš©ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
            
            ê·œì¹™:
            1. ê³ ìœ ëª…ì‚¬, í•µì‹¬ ê°œë…ì–´, íŠ¸ë Œë“œ í‚¤ì›Œë“œ ìš°ì„ 
            2. ë¶ˆìš©ì–´ ì œê±° (ì€, ëŠ”, ì´, ê°€, ì„, ë¥¼ ë“±)
            3. ë³µí•©ì–´ëŠ” ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
            4. ìµœëŒ€ 10ê°œê¹Œì§€ ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ë°˜í™˜
            5. JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜: {"keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", ...]}
            """
            
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=f"ì œëª©: {title}\në‚´ìš©: {content[:1000]}...")
            ]
            
            response = await self.llm.ainvoke(messages)
            
            try:
                result = json.loads(response.content)
                keywords = result.get("keywords", [])
            except json.JSONDecodeError:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ
                keywords = await self._extract_keywords_fallback(content, title)
            
            # 2. spaCy ê¸°ë°˜ ì¶”ê°€ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
            if self.nlp and len(keywords) < self.config.keyword_count:
                spacy_keywords = self._extract_spacy_keywords(content + " " + title)
                keywords.extend([kw for kw in spacy_keywords if kw not in keywords])
            
            # 3. í‚¤ì›Œë“œ ì •ì œ ë° ì¤‘ë³µ ì œê±°
            keywords = self._clean_keywords(keywords)[:self.config.keyword_count]
            
            span.update(output={"keywords": keywords, "count": len(keywords)})
            return keywords
            
        except Exception as e:
            logger.error(f"Keyword extraction failed: {str(e)}")
            return await self._extract_keywords_fallback(content, title)
    
    async def _extract_keywords_fallback(self, content: str, title: str) -> List[str]:
        """ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ (LLM ì‹¤íŒ¨ ì‹œ ë°±ì—…)"""
        text = title + " " + content
        
        # ê°„ë‹¨í•œ ì •ê·œì‹ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        korean_pattern = r'[ê°€-í£]{2,}'
        keywords = re.findall(korean_pattern, text)
        
        # ë¹ˆë„ ê¸°ë°˜ ì •ë ¬
        keyword_freq = {}
        for keyword in keywords:
            keyword_freq[keyword] = keyword_freq.get(keyword, 0) + 1
        
        sorted_keywords = sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)
        return [kw[0] for kw in sorted_keywords[:self.config.keyword_count]]
    
    def _extract_spacy_keywords(self, text: str) -> List[str]:
        """spaCyë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if not self.nlp:
            return []
        
        doc = self.nlp(text)
        keywords = []
        
        # ëª…ì‚¬, ê³ ìœ ëª…ì‚¬ ì¶”ì¶œ
        for token in doc:
            if token.pos_ in ['NOUN', 'PROPN'] and len(token.text) > 1:
                keywords.append(token.text)
        
        # ê°œì²´ëª… ì¶”ì¶œ
        for ent in doc.ents:
            if ent.label_ in ['PERSON', 'ORG', 'GPE', 'EVENT']:
                keywords.append(ent.text)
        
        return list(set(keywords))
    
    def _clean_keywords(self, keywords: List[str]) -> List[str]:
        """í‚¤ì›Œë“œ ì •ì œ"""
        cleaned = []
        stop_words = {'ê¸°ì', 'ë‰´ìŠ¤', 'ë³´ë„', 'ë°œí‘œ', 'ê´€ë ¨', 'ëŒ€í•œ', 'í†µí•´', 'ìœ„í•´', 'ë•Œë¬¸', 'ê²½ìš°'}
        
        for keyword in keywords:
            # ê¸¸ì´ ì²´í¬
            if len(keyword) < 2 or len(keyword) > 20:
                continue
            
            # ë¶ˆìš©ì–´ ì²´í¬
            if keyword.lower() in stop_words:
                continue
            
            # ìˆ«ìë§Œ ìˆëŠ” ê²½ìš° ì œì™¸
            if keyword.isdigit():
                continue
            
            cleaned.append(keyword)
        
        return cleaned
    
    async def _analyze_sentiment(self, content: str, trace) -> float:
        """ê°ì„± ë¶„ì„ (-1.0 ~ 1.0)"""
        span = trace.span(name="sentiment_analysis")
        
        try:
            # TextBlob ê¸°ë°˜ ê°ì„± ë¶„ì„
            blob = TextBlob(content)
            sentiment = blob.sentiment.polarity
            
            # LLM ê¸°ë°˜ ê°ì„± ë¶„ì„ (ë” ì •í™•í•¨)
            if self.config.enable_deep_analysis:
                system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ì–´ ê°ì„± ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
                ì£¼ì–´ì§„ ë‰´ìŠ¤ ë‚´ìš©ì˜ ê°ì„±ì„ ë¶„ì„í•˜ì„¸ìš”.
                
                ì²™ë„:
                - ë§¤ìš° ë¶€ì •ì : -1.0
                - ë¶€ì •ì : -0.5
                - ì¤‘ë¦½: 0.0
                - ê¸ì •ì : 0.5
                - ë§¤ìš° ê¸ì •ì : 1.0
                
                JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜: {"sentiment": -0.5, "reasoning": "ì´ìœ "}
                """
                
                messages = [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=f"ë‚´ìš©: {content[:800]}...")
                ]
                
                response = await self.llm.ainvoke(messages)
                
                try:
                    result = json.loads(response.content)
                    llm_sentiment = result.get("sentiment", sentiment)
                    # ë‘ ê²°ê³¼ì˜ í‰ê· 
                    sentiment = (sentiment + llm_sentiment) / 2
                except json.JSONDecodeError:
                    pass
            
            # ì •ê·œí™”
            sentiment = max(-1.0, min(1.0, sentiment))
            
            span.update(output={"sentiment": sentiment})
            return sentiment
            
        except Exception as e:
            logger.error(f"Sentiment analysis failed: {str(e)}")
            return 0.0
    
    async def _calculate_trending_score(self, content: str, title: str, trace) -> float:
        """íŠ¸ë Œë”© ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)"""
        span = trace.span(name="trending_score_calculation")
        
        try:
            score = 0.0
            
            # 1. í‚¤ì›Œë“œ ê¸°ë°˜ íŠ¸ë Œë”© ì ìˆ˜
            text = title + " " + content
            trending_keywords_score = 0.0
            
            for keyword, freq in self.trending_keywords.items():
                if keyword in text:
                    trending_keywords_score += freq
            
            # ì •ê·œí™”
            trending_keywords_score = min(1.0, trending_keywords_score / 10.0)
            score += trending_keywords_score * 0.4
            
            # 2. ì‹œê°„ ê¸°ë°˜ ì ìˆ˜ (ìµœê·¼ì¼ìˆ˜ë¡ ë†’ìŒ)
            time_score = 1.0  # ì‹¤ì‹œê°„ ë‰´ìŠ¤ë¼ê³  ê°€ì •
            score += time_score * 0.2
            
            # 3. ì½˜í…ì¸  íŠ¹ì„± ê¸°ë°˜ ì ìˆ˜
            urgency_keywords = ['ì†ë³´', 'ê¸´ê¸‰', 'ë°œí‘œ', 'í™•ì¸', 'ì²«', 'ìµœì´ˆ', 'ëŒë°œ']
            urgency_score = sum(1 for keyword in urgency_keywords if keyword in text) / len(urgency_keywords)
            score += urgency_score * 0.3
            
            # 4. ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜ (ì ì ˆí•œ ê¸¸ì´)
            content_length = len(content)
            if 500 <= content_length <= 2000:
                length_score = 1.0
            elif content_length < 500:
                length_score = content_length / 500
            else:
                length_score = max(0.5, 1.0 - (content_length - 2000) / 3000)
            
            score += length_score * 0.1
            
            # ìµœì¢… ì ìˆ˜ ì •ê·œí™”
            score = max(0.0, min(1.0, score))
            
            span.update(output={"trending_score": score})
            return score
            
        except Exception as e:
            logger.error(f"Trending score calculation failed: {str(e)}")
            return 0.5
    
    async def _assess_virality_potential(self, content: str, title: str, trace) -> float:
        """ë°”ì´ëŸ´ ì ì¬ë ¥ í‰ê°€ (0.0 ~ 1.0)"""
        span = trace.span(name="virality_assessment")
        
        try:
            score = 0.0
            text = title + " " + content
            
            # 1. ê°ì •ì  ì˜í–¥ë ¥
            emotional_keywords = ['ì¶©ê²©', 'ë†€ë¼ìš´', 'í™”ì œ', 'ê°ë™', 'ë¶„ë…¸', 'ê¸°ì¨', 'ìŠ¬í””', 'ë†€ë¼ì›€']
            emotional_score = sum(1 for keyword in emotional_keywords if keyword in text) / len(emotional_keywords)
            score += emotional_score * 0.3
            
            # 2. ë…¼ë€ì„±
            controversial_keywords = ['ë…¼ë€', 'ê°ˆë“±', 'ë¹„íŒ', 'ë°˜ë°œ', 'ë…¼ìŸ', 'ì˜í˜¹', 'í­ë¡œ']
            controversial_score = sum(1 for keyword in controversial_keywords if keyword in text) / len(controversial_keywords)
            score += controversial_score * 0.2
            
            # 3. í™”ì œì„±
            trending_keywords = ['í™”ì œ', 'ì¸ê¸°', 'ê´€ì‹¬', 'ì£¼ëª©', 'ì§‘ì¤‘', 'ì£¼ê°„', 'ìµœê³ ']
            trending_score = sum(1 for keyword in trending_keywords if keyword in text) / len(trending_keywords)
            score += trending_score * 0.2
            
            # 4. ì œëª©ì˜ ë§¤ë ¥ë„
            title_attractiveness = 0.0
            if any(char in title for char in ['?', '!', '"', '\'', '"', '"']):
                title_attractiveness += 0.3
            if len(title.split()) > 5:  # ì ì ˆí•œ ê¸¸ì´
                title_attractiveness += 0.2
            if any(word in title for word in ['ìµœì´ˆ', 'ë…ì ', 'íŠ¹ì¢…', 'ë‹¨ë…']):
                title_attractiveness += 0.5
            
            score += min(1.0, title_attractiveness) * 0.3
            
            # ìµœì¢… ì ìˆ˜ ì •ê·œí™”
            score = max(0.0, min(1.0, score))
            
            span.update(output={"virality_score": score})
            return score
            
        except Exception as e:
            logger.error(f"Virality assessment failed: {str(e)}")
            return 0.3
    
    async def _determine_trend_category(self, content: str, category: str, trace) -> TrendCategory:
        """íŠ¸ë Œë“œ ì¹´í…Œê³ ë¦¬ ê²°ì •"""
        span = trace.span(name="trend_categorization")
        
        try:
            # ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ ë§¤í•‘
            category_mapping = {
                "ì •ì¹˜": TrendCategory.POLITICAL_SHIFT,
                "ê²½ì œ": TrendCategory.ECONOMIC_IMPACT,
                "ì‚¬íšŒ": TrendCategory.BREAKING_NEWS,
                "ë¬¸í™”": TrendCategory.CULTURAL_TREND,
                "IT": TrendCategory.TECHNOLOGY_TREND,
                "ìŠ¤í¬ì¸ ": TrendCategory.SPORTS_HIGHLIGHT,
                "ì—°ì˜ˆ": TrendCategory.ENTERTAINMENT
            }
            
            # í‚¤ì›Œë“œ ê¸°ë°˜ ì„¸ë°€í•œ ë¶„ë¥˜
            text = content.lower()
            
            if any(keyword in text for keyword in ['ì†ë³´', 'ê¸´ê¸‰', 'ëŒë°œ', 'ì‚¬ê³ ', 'ì‚¬ê±´']):
                trend_category = TrendCategory.BREAKING_NEWS
            elif any(keyword in text for keyword in ['ì†Œì…œ', 'íŠ¸ìœ„í„°', 'ì¸ìŠ¤íƒ€', 'ìœ íŠœë¸Œ', 'í‹±í†¡']):
                trend_category = TrendCategory.VIRAL_SOCIAL
            elif any(keyword in text for keyword in ['ì£¼ì‹', 'ê²½ì œ', 'ê¸ˆë¦¬', 'í™˜ìœ¨', 'íˆ¬ì']):
                trend_category = TrendCategory.ECONOMIC_IMPACT
            elif any(keyword in text for keyword in ['ai', 'ì¸ê³µì§€ëŠ¥', 'í…Œí¬', 'ìŠ¤íƒ€íŠ¸ì—…', 'ê¸°ìˆ ']):
                trend_category = TrendCategory.TECHNOLOGY_TREND
            else:
                trend_category = category_mapping.get(category, TrendCategory.BREAKING_NEWS)
            
            span.update(output={"trend_category": trend_category.value})
            return trend_category
            
        except Exception as e:
            logger.error(f"Trend categorization failed: {str(e)}")
            return TrendCategory.BREAKING_NEWS
    
    async def _find_related_trends(self, keywords: List[str], trend_category: TrendCategory, trace) -> List[Dict[str, Any]]:
        """ê´€ë ¨ íŠ¸ë Œë“œ ê²€ìƒ‰"""
        span = trace.span(name="related_trends_search")
        
        try:
            related_trends = []
            
            # í‚¤ì›Œë“œ ê¸°ë°˜ ê´€ë ¨ íŠ¸ë Œë“œ ìƒì„± (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬)
            for keyword in keywords[:3]:  # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
                if keyword in self.trending_keywords:
                    related_trends.append({
                        "keyword": keyword,
                        "trend_score": self.trending_keywords[keyword],
                        "category": trend_category.value,
                        "related_count": self.keyword_frequencies.get(keyword, 0)
                    })
            
            # ìµœëŒ€ ê°œìˆ˜ ì œí•œ
            related_trends = related_trends[:self.config.max_related_trends]
            
            span.update(output={"related_trends_count": len(related_trends)})
            return related_trends
            
        except Exception as e:
            logger.error(f"Related trends search failed: {str(e)}")
            return []
    
    async def _update_trending_keywords(self, keywords: List[str], trending_score: float):
        """íŠ¸ë Œë”© í‚¤ì›Œë“œ ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸"""
        try:
            for keyword in keywords:
                # ê°€ì¤‘ì¹˜ ì ìš©í•˜ì—¬ í‚¤ì›Œë“œ ì ìˆ˜ ì—…ë°ì´íŠ¸
                current_score = self.trending_keywords.get(keyword, 0.0)
                new_score = (current_score * 0.8) + (trending_score * 0.2)
                self.trending_keywords[keyword] = new_score
                
                # ë¹ˆë„ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
                self.keyword_frequencies[keyword] = self.keyword_frequencies.get(keyword, 0) + 1
            
            # ì£¼ê¸°ì ìœ¼ë¡œ ì˜¤ë˜ëœ í‚¤ì›Œë“œ ì œê±° (ë©”ëª¨ë¦¬ ê´€ë¦¬)
            if len(self.trending_keywords) > 1000:
                # ì ìˆ˜ê°€ ë‚®ì€ í‚¤ì›Œë“œ ì œê±°
                sorted_keywords = sorted(self.trending_keywords.items(), key=lambda x: x[1], reverse=True)
                self.trending_keywords = dict(sorted_keywords[:800])
                
        except Exception as e:
            logger.error(f"Trending keywords update failed: {str(e)}")
    
    async def _perform_fact_checking(self, content: str, title: str, trace) -> FactCheckResult:
        """
        ğŸ¯ 95% ì •í™•ë„ íŒ©íŠ¸ì²´í‚¹ ì‹œìŠ¤í…œ (Stage 3)
        ì‹ ë¢°ë„ í‰ê°€ ì•Œê³ ë¦¬ì¦˜ (0-100ì  ìŠ¤ì¼€ì¼)
        """
        span = trace.span(name="fact_checking")
        
        try:
            # 1. ì‚¬ì‹¤ ì£¼ì¥ ì¶”ì¶œ
            claims = await self._extract_factual_claims(content, title)
            
            # 2. ë‹¤ì¤‘ ì†ŒìŠ¤ ê²€ì¦
            verification_results = await asyncio.gather(*[
                self._verify_claim_with_sources(claim) for claim in claims
            ])
            
            # 3. ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
            credibility_score = self._calculate_credibility_score(verification_results)
            
            # 4. ê²€ì¦ëœ ì†ŒìŠ¤ ëª©ë¡
            verified_sources = [result['source'] for result in verification_results if result['verified']]
            
            # 5. íŒ©íŠ¸ì²´í‚¹ ê²°ê³¼ ìƒì„±
            fact_check_result = FactCheckResult(
                credibility_score=credibility_score,
                verified_claims=len([r for r in verification_results if r['verified']]),
                total_claims=len(claims),
                verification_sources=verified_sources[:5],  # ìµœëŒ€ 5ê°œ ì†ŒìŠ¤
                fact_check_summary=self._generate_fact_check_summary(verification_results),
                processing_time=datetime.utcnow(),
                agent_version="fact_check_v2.0"
            )
            
            span.update(output={
                "credibility_score": credibility_score,
                "verified_claims": len([r for r in verification_results if r['verified']]),
                "total_claims": len(claims)
            })
            
            return fact_check_result
            
        except Exception as e:
            logger.error(f"Fact checking failed: {str(e)}")
            # ì‹¤íŒ¨ ì‹œ ë‚®ì€ ì‹ ë¢°ë„ ë°˜í™˜
            return FactCheckResult(
                credibility_score=0.5,
                verified_claims=0,
                total_claims=0,
                verification_sources=[],
                fact_check_summary="íŒ©íŠ¸ì²´í‚¹ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
                processing_time=datetime.utcnow(),
                agent_version="fact_check_v2.0"
            )
    
    async def _extract_factual_claims(self, content: str, title: str) -> List[str]:
        """ì‚¬ì‹¤ ì£¼ì¥ ì¶”ì¶œ"""
        system_prompt = """ë‹¹ì‹ ì€ ë‰´ìŠ¤ íŒ©íŠ¸ì²´í‚¹ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ì£¼ì–´ì§„ ë‰´ìŠ¤ ë‚´ìš©ì—ì„œ ê²€ì¦ ê°€ëŠ¥í•œ ì‚¬ì‹¤ ì£¼ì¥ë“¤ì„ ì¶”ì¶œí•˜ì„¸ìš”.
        
        ê·œì¹™:
        1. êµ¬ì²´ì ì´ê³  ê°ê´€ì ì¸ ì‚¬ì‹¤ë§Œ ì¶”ì¶œ
        2. ì˜ê²¬ì´ë‚˜ ì¶”ì¸¡ì€ ì œì™¸
        3. ìˆ«ì, ë‚ ì§œ, ì¸ëª…, ê¸°ê´€ëª… í¬í•¨ ì£¼ì¥ ìš°ì„ 
        4. JSON ì‘ë‹µ: {"claims": ["ì£¼ì¥1", "ì£¼ì¥2", ...]}
        """
        
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"ì œëª©: {title}\në‚´ìš©: {content[:1500]}...")
        ]
        
        response = await self.llm.ainvoke(messages)
        
        try:
            result = json.loads(response.content)
            return result.get("claims", [])
        except json.JSONDecodeError:
            # Fallback: ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­
            return [sent.strip() for sent in content.split('.') if len(sent.strip()) > 50][:5]
    
    async def _verify_claim_with_sources(self, claim: str) -> Dict[str, Any]:
        """ê°œë³„ ì£¼ì¥ ê²€ì¦"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì™¸ë¶€ APIë‚˜ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì†ŒìŠ¤ì™€ ì—°ë™
        # í˜„ì¬ëŠ” ê°„ì†Œí™”ëœ ë²„ì „
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì‹ ë¢°ë„ í‰ê°€
        keywords = claim.lower()
        
        # ê¸°ë³¸ ì‹ ë¢°ë„ ê·œì¹™ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¡œì§)
        if any(word in keywords for word in ['ê³µì‹', 'ë°œí‘œ', 'ì •ë¶€', 'í†µê³„ì²­']):
            credibility = 0.9
            verified = True
            source = "ê³µì‹ ë°œí‘œ"
        elif any(word in keywords for word in ['ì¶”ì •', 'ì˜ˆìƒ', '~ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤']):
            credibility = 0.6
            verified = False
            source = "ì¶”ì •/ì˜ˆìƒ"
        else:
            credibility = 0.75
            verified = True
            source = "ì¼ë°˜ ë³´ë„"
        
        return {
            "claim": claim,
            "verified": verified,
            "credibility": credibility,
            "source": source
        }
    
    def _calculate_credibility_score(self, verification_results: List[Dict[str, Any]]) -> float:
        """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° (0-1 ìŠ¤ì¼€ì¼)"""
        if not verification_results:
            return 0.5
        
        total_credibility = sum(result['credibility'] for result in verification_results)
        verified_count = sum(1 for result in verification_results if result['verified'])
        
        # ê²€ì¦ëœ ì£¼ì¥ ë¹„ìœ¨ê³¼ í‰ê·  ì‹ ë¢°ë„ ê°€ì¤‘ í‰ê· 
        verification_ratio = verified_count / len(verification_results)
        avg_credibility = total_credibility / len(verification_results)
        
        # 95% ì •í™•ë„ ëª©í‘œë¥¼ ìœ„í•œ ë³´ì •
        final_score = (verification_ratio * 0.6) + (avg_credibility * 0.4)
        
        return min(max(final_score, 0.0), 1.0)
    
    def _generate_fact_check_summary(self, verification_results: List[Dict[str, Any]]) -> str:
        """íŒ©íŠ¸ì²´í‚¹ ìš”ì•½ ìƒì„±"""
        verified_count = sum(1 for result in verification_results if result['verified'])
        total_count = len(verification_results)
        
        if total_count == 0:
            return "ê²€ì¦í•  ìˆ˜ ìˆëŠ” ì‚¬ì‹¤ ì£¼ì¥ì´ ì—†ìŠµë‹ˆë‹¤."
        
        verification_rate = (verified_count / total_count) * 100
        
        if verification_rate >= 90:
            return f"ë†’ì€ ì‹ ë¢°ë„: {total_count}ê°œ ì£¼ì¥ ì¤‘ {verified_count}ê°œ ê²€ì¦ë¨ ({verification_rate:.1f}%)"
        elif verification_rate >= 70:
            return f"ì¤‘ê°„ ì‹ ë¢°ë„: {total_count}ê°œ ì£¼ì¥ ì¤‘ {verified_count}ê°œ ê²€ì¦ë¨ ({verification_rate:.1f}%)"
        else:
            return f"ë‚®ì€ ì‹ ë¢°ë„: {total_count}ê°œ ì£¼ì¥ ì¤‘ {verified_count}ê°œë§Œ ê²€ì¦ë¨ ({verification_rate:.1f}%)"

    def get_trending_summary(self) -> Dict[str, Any]:
        """í˜„ì¬ íŠ¸ë Œë”© ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        try:
            top_keywords = sorted(self.trending_keywords.items(), key=lambda x: x[1], reverse=True)[:10]
            
            return {
                "top_trending_keywords": dict(top_keywords),
                "total_keywords": len(self.trending_keywords),
                "total_processed": sum(self.keyword_frequencies.values()),
                "last_updated": datetime.utcnow().isoformat()
            }
        except Exception as e:
            logger.error(f"Trending summary generation failed: {str(e)}")
            return {} 