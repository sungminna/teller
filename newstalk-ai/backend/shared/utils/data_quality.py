"""
ğŸ¯ NewsTalk AI ê³ ê¸‰ ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ì‹œìŠ¤í…œ
==========================================

95% ë°ì´í„° í’ˆì§ˆ ë³´ì¥ì„ ìœ„í•œ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬:
- ì‹¤ì‹œê°„ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼
- ìë™ ë°ì´í„° ì •ì œ ë° ë³´ê°•
- ML ê¸°ë°˜ ì´ìƒ íƒì§€
- ìŠ¤í‚¤ë§ˆ ì§„í™” ì¶”ì 
- ë°ì´í„° ê³„ë³´ ê´€ë¦¬
- ìë™ í’ˆì§ˆ ê°œì„  ì œì•ˆ
"""

import asyncio
import hashlib
import logging
import re
import statistics
import time
from collections import Counter, defaultdict
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Callable, Dict, List, Optional, Tuple, Union


logger = logging.getLogger(__name__)


class QualityMetric(Enum):
    """ë°ì´í„° í’ˆì§ˆ ë©”íŠ¸ë¦­"""

    COMPLETENESS = "completeness"  # ì™„ì „ì„±
    ACCURACY = "accuracy"  # ì •í™•ì„±
    CONSISTENCY = "consistency"  # ì¼ê´€ì„±
    VALIDITY = "validity"  # ìœ íš¨ì„±
    UNIQUENESS = "uniqueness"  # ê³ ìœ ì„±
    TIMELINESS = "timeliness"  # ì ì‹œì„±
    RELEVANCE = "relevance"  # ê´€ë ¨ì„±


class ValidationSeverity(Enum):
    """ê²€ì¦ ì‹¬ê°ë„"""

    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


@dataclass
class QualityRule:
    """ë°ì´í„° í’ˆì§ˆ ê·œì¹™"""

    name: str
    metric: QualityMetric
    severity: ValidationSeverity
    description: str
    validation_func: Callable[[Any], bool]
    auto_fix_func: Optional[Callable[[Any], Any]] = None
    threshold: float = 0.95  # í’ˆì§ˆ ì„ê³„ê°’ (95%)
    enabled: bool = True


@dataclass
class QualityIssue:
    """í’ˆì§ˆ ì´ìŠˆ"""

    rule_name: str
    severity: ValidationSeverity
    field_name: str
    issue_description: str
    sample_value: Any
    suggestion: Optional[str] = None
    auto_fixable: bool = False
    timestamp: datetime = field(default_factory=datetime.utcnow)


@dataclass
class DataQualityReport:
    """ë°ì´í„° í’ˆì§ˆ ë¦¬í¬íŠ¸"""

    dataset_name: str
    total_records: int
    quality_score: float
    metrics_scores: Dict[QualityMetric, float]
    issues: List[QualityIssue]
    improvement_suggestions: List[str]
    timestamp: datetime = field(default_factory=datetime.utcnow)


class AdvancedDataQualityManager:
    """
    ê³ ê¸‰ ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ì

    ì£¼ìš” ê¸°ëŠ¥:
    - ì‹¤ì‹œê°„ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
    - ìë™ ë°ì´í„° ì •ì œ
    - ML ê¸°ë°˜ ì´ìƒ íƒì§€
    - í’ˆì§ˆ íŠ¸ë Œë“œ ë¶„ì„
    - ìë™ í’ˆì§ˆ ê°œì„ 
    """

    def __init__(self):
        # í’ˆì§ˆ ê·œì¹™ ì €ì¥ì†Œ
        self.quality_rules: Dict[str, QualityRule] = {}

        # í’ˆì§ˆ ì´ë ¥ ê´€ë¦¬
        self.quality_history: List[DataQualityReport] = []
        self.anomaly_patterns: Dict[str, List[Dict]] = defaultdict(list)

        # ìë™ ì •ì œ í†µê³„
        self.auto_fix_stats = {
            "total_fixes": 0,
            "successful_fixes": 0,
            "failed_fixes": 0,
            "fixes_by_type": defaultdict(int),
        }

        # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
        self._monitoring_task: Optional[asyncio.Task] = None
        self._quality_cache: Dict[str, Dict] = {}

        # ML ê¸°ë°˜ ì´ìƒ íƒì§€ ëª¨ë¸
        self._anomaly_detector = None
        self._initialize_anomaly_detector()

        logger.info("AdvancedDataQualityManager initialized")

    def _initialize_anomaly_detector(self):
        """ì´ìƒ íƒì§€ ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            # ê°„ë‹¨í•œ í†µê³„ ê¸°ë°˜ ì´ìƒ íƒì§€ ëª¨ë¸
            self._anomaly_detector = StatisticalAnomalyDetector()
            logger.info("Statistical anomaly detector initialized")
        except Exception as e:
            logger.warning(f"Failed to initialize anomaly detector: {e}")

    async def initialize_quality_rules(self):
        """í’ˆì§ˆ ê·œì¹™ ì´ˆê¸°í™”"""
        # ë‰´ìŠ¤ ê¸°ì‚¬ í’ˆì§ˆ ê·œì¹™ë“¤
        news_rules = [
            # ì™„ì „ì„± ê·œì¹™
            QualityRule(
                name="title_completeness",
                metric=QualityMetric.COMPLETENESS,
                severity=ValidationSeverity.ERROR,
                description="ë‰´ìŠ¤ ì œëª©ì€ í•„ìˆ˜ì…ë‹ˆë‹¤",
                validation_func=lambda x: bool(x and x.get("title", "").strip()),
                auto_fix_func=self._generate_title_from_content,
            ),
            QualityRule(
                name="content_completeness",
                metric=QualityMetric.COMPLETENESS,
                severity=ValidationSeverity.ERROR,
                description="ë‰´ìŠ¤ ë³¸ë¬¸ì€ í•„ìˆ˜ì…ë‹ˆë‹¤",
                validation_func=lambda x: bool(x and x.get("content", "").strip()),
            ),
            QualityRule(
                name="published_date_completeness",
                metric=QualityMetric.COMPLETENESS,
                severity=ValidationSeverity.WARNING,
                description="ë°œí–‰ì¼ì€ í•„ìˆ˜ì…ë‹ˆë‹¤",
                validation_func=lambda x: bool(x and x.get("published_at")),
                auto_fix_func=self._estimate_publish_date,
            ),
            # ì •í™•ì„± ê·œì¹™
            QualityRule(
                name="title_length_accuracy",
                metric=QualityMetric.ACCURACY,
                severity=ValidationSeverity.WARNING,
                description="ì œëª© ê¸¸ì´ëŠ” 10-200ì ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤",
                validation_func=lambda x: 10 <= len(x.get("title", "")) <= 200,
                auto_fix_func=self._fix_title_length,
            ),
            QualityRule(
                name="content_length_accuracy",
                metric=QualityMetric.ACCURACY,
                severity=ValidationSeverity.WARNING,
                description="ë³¸ë¬¸ ê¸¸ì´ëŠ” 100ì ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤",
                validation_func=lambda x: len(x.get("content", "")) >= 100,
            ),
            QualityRule(
                name="url_validity",
                metric=QualityMetric.VALIDITY,
                severity=ValidationSeverity.WARNING,
                description="URL í˜•ì‹ì´ ìœ íš¨í•´ì•¼ í•©ë‹ˆë‹¤",
                validation_func=self._validate_url_format,
                auto_fix_func=self._fix_url_format,
            ),
            # ì¼ê´€ì„± ê·œì¹™
            QualityRule(
                name="category_consistency",
                metric=QualityMetric.CONSISTENCY,
                severity=ValidationSeverity.WARNING,
                description="ì¹´í…Œê³ ë¦¬ëŠ” ì •ì˜ëœ ê°’ ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤",
                validation_func=self._validate_category_consistency,
                auto_fix_func=self._fix_category_consistency,
            ),
            QualityRule(
                name="source_consistency",
                metric=QualityMetric.CONSISTENCY,
                severity=ValidationSeverity.INFO,
                description="ì†ŒìŠ¤ëª…ì€ ì¼ê´€ëœ í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤",
                validation_func=self._validate_source_consistency,
                auto_fix_func=self._fix_source_consistency,
            ),
            # ì ì‹œì„± ê·œì¹™
            QualityRule(
                name="content_timeliness",
                metric=QualityMetric.TIMELINESS,
                severity=ValidationSeverity.WARNING,
                description="ë‰´ìŠ¤ëŠ” 7ì¼ ì´ë‚´ ë°œí–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤",
                validation_func=self._validate_content_timeliness,
            ),
            # ê³ ìœ ì„± ê·œì¹™
            QualityRule(
                name="content_uniqueness",
                metric=QualityMetric.UNIQUENESS,
                severity=ValidationSeverity.ERROR,
                description="ì¤‘ë³µ ê¸°ì‚¬ê°€ ì—†ì–´ì•¼ í•©ë‹ˆë‹¤",
                validation_func=self._validate_content_uniqueness,
            ),
            # ê´€ë ¨ì„± ê·œì¹™
            QualityRule(
                name="content_relevance",
                metric=QualityMetric.RELEVANCE,
                severity=ValidationSeverity.INFO,
                description="ì œëª©ê³¼ ë³¸ë¬¸ì´ ê´€ë ¨ì„±ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤",
                validation_func=self._validate_content_relevance,
            ),
        ]

        # ê·œì¹™ ë“±ë¡
        for rule in news_rules:
            self.quality_rules[rule.name] = rule

        logger.info(f"Initialized {len(news_rules)} quality rules")

    async def validate_data_quality(
        self, data: Union[Dict, List[Dict]], dataset_name: str = "unknown"
    ) -> DataQualityReport:
        """
        ë°ì´í„° í’ˆì§ˆ ê²€ì¦

        Args:
            data: ê²€ì¦í•  ë°ì´í„°
            dataset_name: ë°ì´í„°ì…‹ ì´ë¦„

        Returns:
            ë°ì´í„° í’ˆì§ˆ ë¦¬í¬íŠ¸
        """
        start_time = time.time()

        # ë°ì´í„° ì •ê·œí™”
        if isinstance(data, dict):
            data_list = [data]
        else:
            data_list = data

        total_records = len(data_list)
        all_issues = []
        metric_scores = {metric: [] for metric in QualityMetric}

        # ê° ë ˆì½”ë“œë³„ í’ˆì§ˆ ê²€ì¦
        for i, record in enumerate(data_list):
            record_issues = await self._validate_single_record(record, i)
            all_issues.extend(record_issues)

            # ë©”íŠ¸ë¦­ë³„ ì ìˆ˜ ê³„ì‚°
            for metric in QualityMetric:
                metric_issues = [
                    issue
                    for issue in record_issues
                    if issue.rule_name in self._get_rules_by_metric(metric)
                ]
                score = 1.0 - (len(metric_issues) / max(1, len(self._get_rules_by_metric(metric))))
                metric_scores[metric].append(max(0.0, score))

        # ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        overall_scores = []
        for metric, scores in metric_scores.items():
            if scores:
                overall_scores.append(statistics.mean(scores))

        quality_score = statistics.mean(overall_scores) if overall_scores else 0.0

        # ë©”íŠ¸ë¦­ í‰ê·  ì ìˆ˜ ê³„ì‚°
        avg_metric_scores = {
            metric: statistics.mean(scores) if scores else 0.0
            for metric, scores in metric_scores.items()
        }

        # ê°œì„  ì œì•ˆ ìƒì„±
        improvement_suggestions = self._generate_improvement_suggestions(
            all_issues, avg_metric_scores
        )

        # í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±
        quality_report = DataQualityReport(
            dataset_name=dataset_name,
            total_records=total_records,
            quality_score=quality_score,
            metrics_scores=avg_metric_scores,
            issues=all_issues,
            improvement_suggestions=improvement_suggestions,
        )

        # ì´ìƒ íƒì§€ ì‹¤í–‰
        await self._detect_anomalies(quality_report)

        # í’ˆì§ˆ ì´ë ¥ ì €ì¥
        self.quality_history.append(quality_report)

        # ì„±ëŠ¥ ë¡œê¹…
        execution_time = time.time() - start_time
        logger.info(
            f"Quality validation completed for {dataset_name}: "
            f"Score={quality_score:.3f}, Issues={len(all_issues)}, "
            f"Time={execution_time:.3f}s"
        )

        return quality_report

    async def _validate_single_record(self, record: Dict, record_index: int) -> List[QualityIssue]:
        """ë‹¨ì¼ ë ˆì½”ë“œ í’ˆì§ˆ ê²€ì¦"""
        issues = []

        for rule_name, rule in self.quality_rules.items():
            if not rule.enabled:
                continue

            try:
                is_valid = rule.validation_func(record)

                if not is_valid:
                    # ìë™ ìˆ˜ì • ì‹œë„
                    if rule.auto_fix_func:
                        try:
                            fixed_value = rule.auto_fix_func(record)
                            if fixed_value is not None:
                                # ìˆ˜ì • ì„±ê³µ
                                self.auto_fix_stats["successful_fixes"] += 1
                                self.auto_fix_stats["fixes_by_type"][rule.metric.value] += 1

                                issues.append(
                                    QualityIssue(
                                        rule_name=rule_name,
                                        severity=ValidationSeverity.INFO,
                                        field_name=self._extract_field_name(rule_name),
                                        issue_description=f"{rule.description} (ìë™ ìˆ˜ì •ë¨)",
                                        sample_value=str(fixed_value)[:100],
                                        auto_fixable=True,
                                    )
                                )
                                continue
                        except Exception as e:
                            self.auto_fix_stats["failed_fixes"] += 1
                            logger.warning(f"Auto-fix failed for rule {rule_name}: {e}")

                    # ì´ìŠˆ ê¸°ë¡
                    issues.append(
                        QualityIssue(
                            rule_name=rule_name,
                            severity=rule.severity,
                            field_name=self._extract_field_name(rule_name),
                            issue_description=rule.description,
                            sample_value=str(record.get(self._extract_field_name(rule_name), ""))[
                                :100
                            ],
                            suggestion=self._generate_fix_suggestion(rule_name, record),
                            auto_fixable=rule.auto_fix_func is not None,
                        )
                    )

            except Exception as e:
                logger.error(f"Rule validation failed for {rule_name}: {e}")
                issues.append(
                    QualityIssue(
                        rule_name=rule_name,
                        severity=ValidationSeverity.ERROR,
                        field_name="validation_error",
                        issue_description=f"ê²€ì¦ ì˜¤ë¥˜: {str(e)}",
                        sample_value="N/A",
                    )
                )

        return issues

    def _get_rules_by_metric(self, metric: QualityMetric) -> List[str]:
        """ë©”íŠ¸ë¦­ë³„ ê·œì¹™ ì´ë¦„ ëª©ë¡ ë°˜í™˜"""
        return [name for name, rule in self.quality_rules.items() if rule.metric == metric]

    def _extract_field_name(self, rule_name: str) -> str:
        """ê·œì¹™ ì´ë¦„ì—ì„œ í•„ë“œëª… ì¶”ì¶œ"""
        # "title_completeness" -> "title"
        return rule_name.split("_")[0] if "_" in rule_name else rule_name

    def _generate_improvement_suggestions(
        self, issues: List[QualityIssue], metric_scores: Dict[QualityMetric, float]
    ) -> List[str]:
        """í’ˆì§ˆ ê°œì„  ì œì•ˆ ìƒì„±"""
        suggestions = []

        # ì‹¬ê°í•œ ì´ìŠˆë³„ ì œì•ˆ
        critical_issues = [
            issue for issue in issues if issue.severity == ValidationSeverity.CRITICAL
        ]
        if critical_issues:
            suggestions.append(f"ğŸ”´ {len(critical_issues)}ê°œì˜ ì¹˜ëª…ì  ì´ìŠˆ ì¦‰ì‹œ í•´ê²° í•„ìš”")

        error_issues = [issue for issue in issues if issue.severity == ValidationSeverity.ERROR]
        if error_issues:
            suggestions.append(f"ğŸŸ  {len(error_issues)}ê°œì˜ ì˜¤ë¥˜ ì´ìŠˆ ìš°ì„  í•´ê²° ê¶Œì¥")

        # ë©”íŠ¸ë¦­ë³„ ê°œì„  ì œì•ˆ
        for metric, score in metric_scores.items():
            if score < 0.8:  # 80% ë¯¸ë§Œì¸ ê²½ìš°
                if metric == QualityMetric.COMPLETENESS:
                    suggestions.append("ğŸ“ í•„ìˆ˜ í•„ë“œ ëˆ„ë½ ê°œì„ : ë°ì´í„° ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ ì ê²€ í•„ìš”")
                elif metric == QualityMetric.ACCURACY:
                    suggestions.append("ğŸ¯ ë°ì´í„° ì •í™•ì„± ê°œì„ : ì…ë ¥ ê²€ì¦ ê°•í™” í•„ìš”")
                elif metric == QualityMetric.CONSISTENCY:
                    suggestions.append("ğŸ”„ ë°ì´í„° ì¼ê´€ì„± ê°œì„ : í‘œì¤€í™” ê·œì¹™ ì ìš© í•„ìš”")
                elif metric == QualityMetric.VALIDITY:
                    suggestions.append("âœ… ë°ì´í„° ìœ íš¨ì„± ê°œì„ : í˜•ì‹ ê²€ì¦ ê°•í™” í•„ìš”")
                elif metric == QualityMetric.UNIQUENESS:
                    suggestions.append("ğŸ” ì¤‘ë³µ ë°ì´í„° ì œê±°: ì¤‘ë³µ íƒì§€ ì•Œê³ ë¦¬ì¦˜ ê°œì„  í•„ìš”")
                elif metric == QualityMetric.TIMELINESS:
                    suggestions.append("â° ë°ì´í„° ì ì‹œì„± ê°œì„ : ì‹¤ì‹œê°„ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ìµœì í™” í•„ìš”")
                elif metric == QualityMetric.RELEVANCE:
                    suggestions.append("ğŸ¨ ë°ì´í„° ê´€ë ¨ì„± ê°œì„ : ì½˜í…ì¸  í•„í„°ë§ ì•Œê³ ë¦¬ì¦˜ ê°œì„  í•„ìš”")

        # ìë™ ìˆ˜ì • ì œì•ˆ
        auto_fixable_issues = [issue for issue in issues if issue.auto_fixable]
        if auto_fixable_issues:
            suggestions.append(f"ğŸ”§ {len(auto_fixable_issues)}ê°œ ì´ìŠˆ ìë™ ìˆ˜ì • ê°€ëŠ¥")

        return suggestions[:10]  # ìƒìœ„ 10ê°œ ì œì•ˆë§Œ

    async def _detect_anomalies(self, quality_report: DataQualityReport):
        """ì´ìƒ íƒì§€ ì‹¤í–‰"""
        try:
            if self._anomaly_detector:
                anomalies = await self._anomaly_detector.detect_anomalies(quality_report)

                for anomaly in anomalies:
                    logger.warning(f"Quality anomaly detected: {anomaly}")

                    # ì´ìƒ íŒ¨í„´ ì €ì¥
                    self.anomaly_patterns[quality_report.dataset_name].append(
                        {
                            "timestamp": datetime.utcnow(),
                            "anomaly": anomaly,
                            "quality_score": quality_report.quality_score,
                        }
                    )

        except Exception as e:
            logger.error(f"Anomaly detection failed: {e}")

    def _generate_fix_suggestion(self, rule_name: str, record: Dict) -> str:
        """ìˆ˜ì • ì œì•ˆ ìƒì„±"""
        if rule_name == "title_completeness":
            return "ë³¸ë¬¸ ì²« ë¬¸ì¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì œëª© ìƒì„±ì„ ê³ ë ¤í•˜ì„¸ìš”"
        elif rule_name == "content_length_accuracy":
            return "ë³¸ë¬¸ì„ ë” ìƒì„¸íˆ ì‘ì„±í•˜ê±°ë‚˜ ìš”ì•½ë¬¸ì„ ì¶”ê°€í•˜ì„¸ìš”"
        elif rule_name == "url_validity":
            return "ì˜¬ë°”ë¥¸ URL í˜•ì‹(http://ë˜ëŠ” https://)ìœ¼ë¡œ ìˆ˜ì •í•˜ì„¸ìš”"
        elif rule_name == "category_consistency":
            return "ì •ì˜ëœ ì¹´í…Œê³ ë¦¬ ëª©ë¡ì—ì„œ ì„ íƒí•˜ì„¸ìš”"
        else:
            return "ë°ì´í„° ê²€ì¦ ê·œì¹™ì„ í™•ì¸í•˜ê³  ìˆ˜ì •í•˜ì„¸ìš”"

    # ìë™ ìˆ˜ì • í•¨ìˆ˜ë“¤
    def _generate_title_from_content(self, record: Dict) -> Optional[str]:
        """ë³¸ë¬¸ì—ì„œ ì œëª© ìƒì„±"""
        content = record.get("content", "").strip()
        if not content:
            return None

        # ì²« ë¬¸ì¥ ì¶”ì¶œ (ê°„ë‹¨í•œ êµ¬í˜„)
        sentences = re.split(r"[.!?]", content)
        first_sentence = sentences[0].strip() if sentences else ""

        if len(first_sentence) > 10:
            # 200ìë¡œ ì œí•œ
            title = first_sentence[:200] if len(first_sentence) > 200 else first_sentence
            record["title"] = title
            return title

        return None

    def _estimate_publish_date(self, record: Dict) -> Optional[str]:
        """ë°œí–‰ì¼ ì¶”ì •"""
        # í˜„ì¬ ì‹œê°„ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
        estimated_date = datetime.utcnow()
        record["published_at"] = estimated_date.isoformat()
        return estimated_date.isoformat()

    def _fix_title_length(self, record: Dict) -> Optional[str]:
        """ì œëª© ê¸¸ì´ ìˆ˜ì •"""
        title = record.get("title", "").strip()

        if len(title) < 10:
            # ë„ˆë¬´ ì§§ì€ ê²½ìš°: ë³¸ë¬¸ì—ì„œ ë³´ê°•
            content = record.get("content", "").strip()
            if content:
                additional_text = content.split(".")[0]
                fixed_title = f"{title} - {additional_text}"[:200]
                record["title"] = fixed_title
                return fixed_title

        elif len(title) > 200:
            # ë„ˆë¬´ ê¸´ ê²½ìš°: ì¤„ì„
            fixed_title = title[:197] + "..."
            record["title"] = fixed_title
            return fixed_title

        return title

    def _fix_url_format(self, record: Dict) -> Optional[str]:
        """URL í˜•ì‹ ìˆ˜ì •"""
        url = record.get("url", "").strip()
        if url and not url.startswith(("http://", "https://")):
            fixed_url = f"https://{url}"
            record["url"] = fixed_url
            return fixed_url
        return url

    def _fix_category_consistency(self, record: Dict) -> Optional[str]:
        """ì¹´í…Œê³ ë¦¬ ì¼ê´€ì„± ìˆ˜ì •"""
        category = record.get("category", "").strip().lower()

        # ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        category_mapping = {
            "ì •ì¹˜": "politics",
            "ê²½ì œ": "economy",
            "ì‚¬íšŒ": "society",
            "êµ­ì œ": "international",
            "ìŠ¤í¬ì¸ ": "sports",
            "ì—°ì˜ˆ": "entertainment",
            "ê¸°ìˆ ": "technology",
            "tech": "technology",
            "it": "technology",
            "ê³¼í•™": "science",
        }

        if category in category_mapping:
            fixed_category = category_mapping[category]
            record["category"] = fixed_category
            return fixed_category

        return None

    def _fix_source_consistency(self, record: Dict) -> Optional[str]:
        """ì†ŒìŠ¤ëª… ì¼ê´€ì„± ìˆ˜ì •"""
        source = record.get("source_name", "").strip()

        # ì†ŒìŠ¤ëª… ì •ê·œí™”
        if source:
            # ê³µí†µ ì ‘ë¯¸ì‚¬ ì œê±°
            cleaned_source = re.sub(r"\s*(ë‰´ìŠ¤|ì‹ ë¬¸|ë°©ì†¡|ë¯¸ë””ì–´|ì–¸ë¡ )$", "", source)
            # ê³µë°± ì •ê·œí™”
            cleaned_source = re.sub(r"\s+", " ", cleaned_source).strip()

            if cleaned_source != source:
                record["source_name"] = cleaned_source
                return cleaned_source

        return source

    # ê²€ì¦ í•¨ìˆ˜ë“¤
    def _validate_url_format(self, record: Dict) -> bool:
        """URL í˜•ì‹ ê²€ì¦"""
        url = record.get("url", "")
        if not url:
            return True  # URLì´ ì—†ëŠ” ê²ƒì€ í—ˆìš©

        url_pattern = re.compile(
            r"^https?://"  # http:// ë˜ëŠ” https://
            r"(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|"  # ë„ë©”ì¸
            r"localhost|"  # localhost
            r"\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})"  # IP
            r"(?::\d+)?"  # í¬íŠ¸
            r"(?:/?|[/?]\S+)$",
            re.IGNORECASE,
        )

        return bool(url_pattern.match(url))

    def _validate_category_consistency(self, record: Dict) -> bool:
        """ì¹´í…Œê³ ë¦¬ ì¼ê´€ì„± ê²€ì¦"""
        category = record.get("category", "").strip().lower()

        valid_categories = {
            "politics",
            "economy",
            "society",
            "international",
            "sports",
            "entertainment",
            "technology",
            "science",
            "ì •ì¹˜",
            "ê²½ì œ",
            "ì‚¬íšŒ",
            "êµ­ì œ",
            "ìŠ¤í¬ì¸ ",
            "ì—°ì˜ˆ",
            "ê¸°ìˆ ",
            "ê³¼í•™",
        }

        return category in valid_categories

    def _validate_source_consistency(self, record: Dict) -> bool:
        """ì†ŒìŠ¤ëª… ì¼ê´€ì„± ê²€ì¦"""
        source = record.get("source_name", "").strip()

        if not source:
            return False

        # ì†ŒìŠ¤ëª…ì€ 2-50ì ì‚¬ì´, íŠ¹ìˆ˜ë¬¸ì ì œí•œ
        if not (2 <= len(source) <= 50):
            return False

        # ê¸°ë³¸ì ì¸ í˜•ì‹ ê²€ì¦
        if re.search(r"[<>{}]", source):  # HTML íƒœê·¸ ê°™ì€ ë¬¸ì ì œì™¸
            return False

        return True

    def _validate_content_timeliness(self, record: Dict) -> bool:
        """ì½˜í…ì¸  ì ì‹œì„± ê²€ì¦"""
        published_at = record.get("published_at")

        if not published_at:
            return False

        try:
            if isinstance(published_at, str):
                pub_date = datetime.fromisoformat(published_at.replace("Z", "+00:00"))
            else:
                pub_date = published_at

            # 7ì¼ ì´ë‚´ ë°œí–‰ëœ ë‰´ìŠ¤ë§Œ í—ˆìš©
            days_old = (datetime.utcnow() - pub_date.replace(tzinfo=None)).days
            return days_old <= 7

        except Exception:
            return False

    def _validate_content_uniqueness(self, record: Dict) -> bool:
        """ì½˜í…ì¸  ê³ ìœ ì„± ê²€ì¦"""
        title = record.get("title", "").strip()
        content = record.get("content", "").strip()

        if not title or not content:
            return True  # ë‹¤ë¥¸ ê·œì¹™ì—ì„œ ì²˜ë¦¬

        # ê°„ë‹¨í•œ ì¤‘ë³µ ê²€ì‚¬ (ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ í•„ìš”)
        content_hash = hashlib.md5(f"{title}{content}".encode()).hexdigest()

        # ìºì‹œì—ì„œ ì¤‘ë³µ í™•ì¸
        if content_hash in self._quality_cache.get("content_hashes", set()):
            return False

        # ìºì‹œì— ì¶”ê°€
        if "content_hashes" not in self._quality_cache:
            self._quality_cache["content_hashes"] = set()
        self._quality_cache["content_hashes"].add(content_hash)

        return True

    def _validate_content_relevance(self, record: Dict) -> bool:
        """ì½˜í…ì¸  ê´€ë ¨ì„± ê²€ì¦"""
        title = record.get("title", "").strip().lower()
        content = record.get("content", "").strip().lower()

        if not title or not content:
            return True  # ë‹¤ë¥¸ ê·œì¹™ì—ì„œ ì²˜ë¦¬

        # ì œëª©ì˜ ì£¼ìš” í‚¤ì›Œë“œê°€ ë³¸ë¬¸ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        title_words = set(re.findall(r"\w+", title))
        content_words = set(re.findall(r"\w+", content))

        # êµì§‘í•© ë¹„ìœ¨ ê³„ì‚°
        if len(title_words) == 0:
            return True

        intersection_ratio = len(title_words.intersection(content_words)) / len(title_words)
        return intersection_ratio >= 0.3  # 30% ì´ìƒ ì¼ì¹˜

    async def get_quality_trends(self, dataset_name: str = None, days: int = 7) -> Dict[str, Any]:
        """í’ˆì§ˆ íŠ¸ë Œë“œ ë¶„ì„"""
        cutoff_date = datetime.utcnow() - timedelta(days=days)

        # í•„í„°ë§ëœ ë¦¬í¬íŠ¸ ê°€ì ¸ì˜¤ê¸°
        filtered_reports = [
            report
            for report in self.quality_history
            if report.timestamp >= cutoff_date
            and (dataset_name is None or report.dataset_name == dataset_name)
        ]

        if not filtered_reports:
            return {"error": "No quality data available for the specified period"}

        # íŠ¸ë Œë“œ ê³„ì‚°
        trend_data = {
            "period": f"Last {days} days",
            "total_reports": len(filtered_reports),
            "average_quality_score": statistics.mean([r.quality_score for r in filtered_reports]),
            "quality_trend": self._calculate_quality_trend(filtered_reports),
            "top_issues": self._get_top_issues(filtered_reports),
            "metric_trends": self._calculate_metric_trends(filtered_reports),
            "improvement_rate": self._calculate_improvement_rate(filtered_reports),
        }

        return trend_data

    def _calculate_quality_trend(self, reports: List[DataQualityReport]) -> str:
        """í’ˆì§ˆ íŠ¸ë Œë“œ ê³„ì‚°"""
        if len(reports) < 2:
            return "insufficient_data"

        # ì‹œê°„ìˆœ ì •ë ¬
        sorted_reports = sorted(reports, key=lambda x: x.timestamp)

        # ì²« ì ˆë°˜ê³¼ í›„ ì ˆë°˜ì˜ í‰ê·  ë¹„êµ
        mid_point = len(sorted_reports) // 2
        first_half_avg = statistics.mean([r.quality_score for r in sorted_reports[:mid_point]])
        second_half_avg = statistics.mean([r.quality_score for r in sorted_reports[mid_point:]])

        improvement = second_half_avg - first_half_avg

        if improvement > 0.05:
            return "improving"
        elif improvement < -0.05:
            return "declining"
        else:
            return "stable"

    def _get_top_issues(self, reports: List[DataQualityReport], limit: int = 5) -> List[Dict]:
        """ìƒìœ„ ì´ìŠˆ ëª©ë¡ ìƒì„±"""
        issue_counter = Counter()

        for report in reports:
            for issue in report.issues:
                issue_counter[issue.rule_name] += 1

        top_issues = []
        for rule_name, count in issue_counter.most_common(limit):
            rule = self.quality_rules.get(rule_name)
            top_issues.append(
                {
                    "rule_name": rule_name,
                    "description": rule.description if rule else "Unknown rule",
                    "count": count,
                    "severity": rule.severity.value if rule else "unknown",
                }
            )

        return top_issues

    def _calculate_metric_trends(self, reports: List[DataQualityReport]) -> Dict[str, float]:
        """ë©”íŠ¸ë¦­ë³„ íŠ¸ë Œë“œ ê³„ì‚°"""
        metric_trends = {}

        for metric in QualityMetric:
            scores = [report.metrics_scores.get(metric, 0.0) for report in reports]
            if scores:
                metric_trends[metric.value] = statistics.mean(scores)

        return metric_trends

    def _calculate_improvement_rate(self, reports: List[DataQualityReport]) -> float:
        """ê°œì„ ìœ¨ ê³„ì‚°"""
        if len(reports) < 2:
            return 0.0

        sorted_reports = sorted(reports, key=lambda x: x.timestamp)
        first_score = sorted_reports[0].quality_score
        last_score = sorted_reports[-1].quality_score

        if first_score == 0:
            return 0.0

        return (last_score - first_score) / first_score * 100

    async def auto_fix_issues(self, data: Union[Dict, List[Dict]]) -> Tuple[Any, int]:
        """ìë™ ì´ìŠˆ ìˆ˜ì •"""
        if isinstance(data, dict):
            data_list = [data]
            single_record = True
        else:
            data_list = data
            single_record = False

        fixes_applied = 0

        for record in data_list:
            for rule_name, rule in self.quality_rules.items():
                if not rule.enabled or not rule.auto_fix_func:
                    continue

                try:
                    is_valid = rule.validation_func(record)
                    if not is_valid:
                        fixed_value = rule.auto_fix_func(record)
                        if fixed_value is not None:
                            fixes_applied += 1
                            self.auto_fix_stats["total_fixes"] += 1
                            logger.debug(f"Auto-fixed {rule_name} for record")

                except Exception as e:
                    logger.error(f"Auto-fix failed for {rule_name}: {e}")

        result = data_list[0] if single_record else data_list
        return result, fixes_applied

    def get_auto_fix_stats(self) -> Dict[str, Any]:
        """ìë™ ìˆ˜ì • í†µê³„ ë°˜í™˜"""
        stats = self.auto_fix_stats.copy()

        if stats["total_fixes"] > 0:
            stats["success_rate"] = stats["successful_fixes"] / stats["total_fixes"]
        else:
            stats["success_rate"] = 0.0

        return stats


class StatisticalAnomalyDetector:
    """í†µê³„ ê¸°ë°˜ ì´ìƒ íƒì§€ê¸°"""

    def __init__(self):
        self.baseline_stats = {}
        self.anomaly_threshold = 2.0  # í‘œì¤€í¸ì°¨ ì„ê³„ê°’

    async def detect_anomalies(self, quality_report: DataQualityReport) -> List[str]:
        """ì´ìƒ íƒì§€ ì‹¤í–‰"""
        anomalies = []

        try:
            # í’ˆì§ˆ ì ìˆ˜ ì´ìƒ íƒì§€
            if await self._is_score_anomaly(quality_report.quality_score):
                anomalies.append(f"Quality score anomaly: {quality_report.quality_score:.3f}")

            # ì´ìŠˆ ê°œìˆ˜ ì´ìƒ íƒì§€
            if await self._is_issue_count_anomaly(len(quality_report.issues)):
                anomalies.append(f"Issue count anomaly: {len(quality_report.issues)}")

            # ë©”íŠ¸ë¦­ë³„ ì´ìƒ íƒì§€
            for metric, score in quality_report.metrics_scores.items():
                if await self._is_metric_anomaly(metric, score):
                    anomalies.append(f"Metric anomaly {metric.value}: {score:.3f}")

        except Exception as e:
            logger.error(f"Anomaly detection error: {e}")

        return anomalies

    async def _is_score_anomaly(self, score: float) -> bool:
        """í’ˆì§ˆ ì ìˆ˜ ì´ìƒ ì—¬ë¶€ í™•ì¸"""
        # ê°„ë‹¨í•œ ì„ê³„ê°’ ê¸°ë°˜ ì´ìƒ íƒì§€
        return score < 0.5  # 50% ë¯¸ë§Œì€ ì´ìƒìœ¼ë¡œ ê°„ì£¼

    async def _is_issue_count_anomaly(self, issue_count: int) -> bool:
        """ì´ìŠˆ ê°œìˆ˜ ì´ìƒ ì—¬ë¶€ í™•ì¸"""
        return issue_count > 100  # 100ê°œ ì´ˆê³¼ëŠ” ì´ìƒìœ¼ë¡œ ê°„ì£¼

    async def _is_metric_anomaly(self, metric: QualityMetric, score: float) -> bool:
        """ë©”íŠ¸ë¦­ ì´ìƒ ì—¬ë¶€ í™•ì¸"""
        # ë©”íŠ¸ë¦­ë³„ ì„ê³„ê°’
        thresholds = {
            QualityMetric.COMPLETENESS: 0.8,
            QualityMetric.ACCURACY: 0.7,
            QualityMetric.CONSISTENCY: 0.7,
            QualityMetric.VALIDITY: 0.8,
            QualityMetric.UNIQUENESS: 0.9,
            QualityMetric.TIMELINESS: 0.6,
            QualityMetric.RELEVANCE: 0.6,
        }

        threshold = thresholds.get(metric, 0.7)
        return score < threshold


# ì „ì—­ í’ˆì§ˆ ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤
_quality_manager: Optional[AdvancedDataQualityManager] = None


async def get_quality_manager() -> AdvancedDataQualityManager:
    """í’ˆì§ˆ ê´€ë¦¬ì ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _quality_manager
    if _quality_manager is None:
        _quality_manager = AdvancedDataQualityManager()
        await _quality_manager.initialize_quality_rules()
    return _quality_manager


# í¸ì˜ í•¨ìˆ˜ë“¤
async def validate_news_quality(data: Union[Dict, List[Dict]]) -> DataQualityReport:
    """ë‰´ìŠ¤ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ í¸ì˜ í•¨ìˆ˜"""
    manager = await get_quality_manager()
    return await manager.validate_data_quality(data, "news_articles")


async def auto_fix_news_data(data: Union[Dict, List[Dict]]) -> Tuple[Any, int]:
    """ë‰´ìŠ¤ ë°ì´í„° ìë™ ìˆ˜ì • í¸ì˜ í•¨ìˆ˜"""
    manager = await get_quality_manager()
    return await manager.auto_fix_issues(data)
