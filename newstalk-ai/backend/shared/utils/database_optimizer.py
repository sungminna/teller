"""
ğŸ¯ NewsTalk AI ê³ ì„±ëŠ¥ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì‹œìŠ¤í…œ
===============================================

40% ì„±ëŠ¥ í–¥ìƒì„ ëª©í‘œë¡œ í•˜ëŠ” ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”:
- ì–´ëŒ‘í‹°ë¸Œ ì—°ê²° í’€ë§ (ë™ì  í¬ê¸° ì¡°ì •)
- ì¿¼ë¦¬ ìºì‹± ë° ìµœì í™”
- ì½ê¸°/ì“°ê¸° ë¶„ì‚° ì²˜ë¦¬
- ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- ìë™ ì¥ì•  ë³µêµ¬
- íŠ¸ëœì­ì…˜ ìµœì í™”
"""

import asyncio
import hashlib
import json
import logging
import time
from contextlib import asynccontextmanager
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple

import redis.asyncio as redis
from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncEngine, AsyncSession, create_async_engine
from sqlalchemy.orm import sessionmaker
from sqlalchemy.pool import QueuePool

from .exceptions import DatabaseError
from .state_manager import get_state_manager

logger = logging.getLogger(__name__)


class QueryType(Enum):
    """ì¿¼ë¦¬ íƒ€ì…"""

    READ = "read"
    WRITE = "write"
    TRANSACTION = "transaction"
    BULK = "bulk"


class PoolStrategy(Enum):
    """ì—°ê²° í’€ ì „ëµ"""

    FIXED = "fixed"  # ê³ ì • í¬ê¸°
    ADAPTIVE = "adaptive"  # ì ì‘í˜• í¬ê¸°
    BURST = "burst"  # ë²„ìŠ¤íŠ¸ ëª¨ë“œ


@dataclass
class DatabaseConfig:
    """ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •"""

    # ê¸°ë³¸ ì—°ê²° ì„¤ì •
    host: str = "localhost"
    port: int = 5432
    database: str = "newstalk_ai"
    username: str = "postgres"
    password: str = ""

    # ì—°ê²° í’€ ì„¤ì •
    min_pool_size: int = 5
    max_pool_size: int = 20
    pool_strategy: PoolStrategy = PoolStrategy.ADAPTIVE
    pool_timeout: float = 30.0
    pool_recycle: int = 3600  # 1ì‹œê°„

    # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
    enable_query_cache: bool = True
    cache_ttl_seconds: int = 300  # 5ë¶„
    slow_query_threshold: float = 1.0  # 1ì´ˆ
    enable_read_replica: bool = False
    read_replica_urls: List[str] = field(default_factory=list)

    # ëª¨ë‹ˆí„°ë§ ì„¤ì •
    enable_metrics: bool = True
    metrics_interval: float = 60.0  # 1ë¶„
    alert_threshold_ms: float = 2000.0  # 2ì´ˆ


@dataclass
class QueryMetrics:
    """ì¿¼ë¦¬ ë©”íŠ¸ë¦­"""

    total_queries: int = 0
    successful_queries: int = 0
    failed_queries: int = 0
    total_execution_time: float = 0.0
    average_execution_time: float = 0.0
    slowest_query_time: float = 0.0
    cache_hits: int = 0
    cache_misses: int = 0

    def update_execution_time(self, execution_time: float):
        """ì‹¤í–‰ ì‹œê°„ ì—…ë°ì´íŠ¸"""
        self.total_queries += 1
        self.total_execution_time += execution_time
        self.average_execution_time = self.total_execution_time / self.total_queries

        if execution_time > self.slowest_query_time:
            self.slowest_query_time = execution_time


class AdaptiveConnectionPool:
    """
    ì ì‘í˜• ì—°ê²° í’€

    ì£¼ìš” ê¸°ëŠ¥:
    - ë™ì  í¬ê¸° ì¡°ì • (ë¶€í•˜ì— ë”°ë¥¸ ìë™ ìŠ¤ì¼€ì¼ë§)
    - í—¬ìŠ¤ ì²´í¬ ë° ìë™ ë³µêµ¬
    - ì½ê¸°/ì“°ê¸° ë¶„ì‚° ì²˜ë¦¬
    - ì—°ê²° ì¬ì‚¬ìš© ìµœì í™”
    """

    def __init__(self, config: DatabaseConfig):
        self.config = config
        self.primary_engine: Optional[AsyncEngine] = None
        self.replica_engines: List[AsyncEngine] = []
        self.session_maker: Optional[sessionmaker] = None

        # ì—°ê²° í’€ ìƒíƒœ
        self.active_connections = 0
        self.peak_connections = 0
        self.pool_size_history = []

        # ë©”íŠ¸ë¦­ ë° ëª¨ë‹ˆí„°ë§
        self.metrics = QueryMetrics()
        self._monitoring_task: Optional[asyncio.Task] = None
        self._adaptive_task: Optional[asyncio.Task] = None

        # ì¿¼ë¦¬ ìºì‹œ
        self.query_cache: Optional[redis.Redis] = None
        self._cache_enabled = config.enable_query_cache

        logger.info(
            f"AdaptiveConnectionPool initialized with strategy: {config.pool_strategy.value}"
        )

    async def initialize(self):
        """ì—°ê²° í’€ ì´ˆê¸°í™”"""
        try:
            # ì£¼ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
            await self._create_primary_engine()

            # ì½ê¸° ì „ìš© ë³µì œë³¸ ì—°ê²° (ì„¤ì •ëœ ê²½ìš°)
            if self.config.enable_read_replica:
                await self._create_replica_engines()

            # ì¿¼ë¦¬ ìºì‹œ ì´ˆê¸°í™”
            if self._cache_enabled:
                await self._initialize_query_cache()

            # ëª¨ë‹ˆí„°ë§ ì‹œì‘
            if self.config.enable_metrics:
                self._start_monitoring()

            # ì ì‘í˜• í’€ ê´€ë¦¬ ì‹œì‘
            if self.config.pool_strategy == PoolStrategy.ADAPTIVE:
                self._start_adaptive_management()

            logger.info("Database connection pool initialized successfully")

        except Exception as e:
            logger.error(f"Failed to initialize connection pool: {e}")
            raise DatabaseError(f"Connection pool initialization failed: {e}")

    async def _create_primary_engine(self):
        """ì£¼ ë°ì´í„°ë² ì´ìŠ¤ ì—”ì§„ ìƒì„±"""
        database_url = self._build_database_url()

        self.primary_engine = create_async_engine(
            database_url,
            poolclass=QueuePool,
            pool_size=self.config.min_pool_size,
            max_overflow=self.config.max_pool_size - self.config.min_pool_size,
            pool_timeout=self.config.pool_timeout,
            pool_recycle=self.config.pool_recycle,
            echo=False,  # í”„ë¡œë•ì…˜ì—ì„œëŠ” False
            future=True,
            # ì„±ëŠ¥ ìµœì í™” ì˜µì…˜
            connect_args={
                "statement_cache_size": 0,  # ìºì‹œ ë¹„í™œì„±í™” (ë³„ë„ ìºì‹œ ì‚¬ìš©)
                "prepared_statement_cache_size": 0,
                "server_settings": {
                    "application_name": "newstalk_ai",
                    "tcp_keepalives_idle": "600",
                    "tcp_keepalives_interval": "30",
                    "tcp_keepalives_count": "3",
                },
            },
        )

        # ì„¸ì…˜ ë©”ì´ì»¤ ìƒì„±
        self.session_maker = sessionmaker(
            bind=self.primary_engine, class_=AsyncSession, expire_on_commit=False
        )

        # ì—°ê²° í…ŒìŠ¤íŠ¸
        async with self.primary_engine.begin() as conn:
            await conn.execute(text("SELECT 1"))

        logger.info("Primary database engine created")

    async def _create_replica_engines(self):
        """ì½ê¸° ì „ìš© ë³µì œë³¸ ì—”ì§„ë“¤ ìƒì„±"""
        for replica_url in self.config.read_replica_urls:
            try:
                replica_engine = create_async_engine(
                    replica_url,
                    poolclass=QueuePool,
                    pool_size=self.config.min_pool_size // 2,  # ë³µì œë³¸ì€ ì ˆë°˜ í¬ê¸°
                    max_overflow=self.config.max_pool_size // 2,
                    pool_timeout=self.config.pool_timeout,
                    pool_recycle=self.config.pool_recycle,
                    echo=False,
                )

                # ì—°ê²° í…ŒìŠ¤íŠ¸
                async with replica_engine.begin() as conn:
                    await conn.execute(text("SELECT 1"))

                self.replica_engines.append(replica_engine)
                logger.info(f"Read replica engine created: {replica_url}")

            except Exception as e:
                logger.warning(f"Failed to create replica engine {replica_url}: {e}")

    async def _initialize_query_cache(self):
        """ì¿¼ë¦¬ ìºì‹œ ì´ˆê¸°í™”"""
        try:
            # Redis ì—°ê²° (ìƒíƒœ ê´€ë¦¬ìì—ì„œ ê°€ì ¸ì˜¤ê¸°)
            state_manager = get_state_manager()
            if hasattr(state_manager, "_redis_client") and state_manager._redis_client:
                self.query_cache = state_manager._redis_client
                logger.info("Query cache initialized using existing Redis connection")
            else:
                # ë³„ë„ Redis ì—°ê²° ìƒì„±
                self.query_cache = redis.from_url(
                    "redis://localhost:6379/1",  # DB 1 ì‚¬ìš© (ìºì‹œ ì „ìš©)
                    encoding="utf-8",
                    decode_responses=True,
                )
                await self.query_cache.ping()
                logger.info("Query cache initialized with dedicated Redis connection")

        except Exception as e:
            logger.warning(f"Failed to initialize query cache: {e}")
            self._cache_enabled = False

    def _build_database_url(self) -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ URL êµ¬ì„±"""
        return (
            f"postgresql+asyncpg://{self.config.username}:{self.config.password}"
            f"@{self.config.host}:{self.config.port}/{self.config.database}"
        )

    def _start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if self._monitoring_task is None or self._monitoring_task.done():
            self._monitoring_task = asyncio.create_task(self._monitor_performance())

    def _start_adaptive_management(self):
        """ì ì‘í˜• í’€ ê´€ë¦¬ ì‹œì‘"""
        if self._adaptive_task is None or self._adaptive_task.done():
            self._adaptive_task = asyncio.create_task(self._manage_adaptive_pool())

    async def _monitor_performance(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
        while True:
            try:
                await asyncio.sleep(self.config.metrics_interval)

                # í˜„ì¬ ì—°ê²° ìƒíƒœ í™•ì¸
                if self.primary_engine:
                    pool = self.primary_engine.pool
                    self.active_connections = pool.checkedout()

                    if self.active_connections > self.peak_connections:
                        self.peak_connections = self.active_connections

                # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¡œê¹…
                logger.info(
                    f"DB Pool Status - Active: {self.active_connections}, "
                    f"Peak: {self.peak_connections}, "
                    f"Avg Query Time: {self.metrics.average_execution_time:.3f}s, "
                    f"Cache Hit Rate: {self._calculate_cache_hit_rate():.2%}"
                )

                # ê²½ê³  ì„ê³„ê°’ ì²´í¬
                if self.metrics.average_execution_time * 1000 > self.config.alert_threshold_ms:
                    logger.warning(
                        f"Query performance alert: Average execution time "
                        f"{self.metrics.average_execution_time:.3f}s exceeds threshold"
                    )

            except Exception as e:
                logger.error(f"Performance monitoring error: {e}")

    async def _manage_adaptive_pool(self):
        """ì ì‘í˜• í’€ í¬ê¸° ê´€ë¦¬"""
        while True:
            try:
                await asyncio.sleep(30)  # 30ì´ˆë§ˆë‹¤ ì²´í¬

                if not self.primary_engine:
                    continue

                pool = self.primary_engine.pool
                current_size = pool.size()
                current_checked_out = pool.checkedout()
                utilization = current_checked_out / current_size if current_size > 0 else 0

                # í’€ í¬ê¸° ê¸°ë¡
                self.pool_size_history.append(
                    {"timestamp": time.time(), "size": current_size, "utilization": utilization}
                )

                # ìµœê·¼ 5ë¶„ê°„ ë°ì´í„°ë§Œ ìœ ì§€
                cutoff_time = time.time() - 300
                self.pool_size_history = [
                    entry for entry in self.pool_size_history if entry["timestamp"] > cutoff_time
                ]

                # í’€ í¬ê¸° ì¡°ì • ê²°ì •
                if len(self.pool_size_history) >= 5:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ
                    avg_utilization = (
                        sum(entry["utilization"] for entry in self.pool_size_history[-5:]) / 5
                    )

                    # ê³ ë¶€í•˜ ìƒí™©: í’€ í¬ê¸° ì¦ê°€
                    if avg_utilization > 0.8 and current_size < self.config.max_pool_size:
                        new_size = min(current_size + 2, self.config.max_pool_size)
                        logger.info(f"Increasing pool size: {current_size} -> {new_size}")
                        # SQLAlchemy í’€ í¬ê¸°ëŠ” ëŸ°íƒ€ì„ì— ì§ì ‘ ë³€ê²½í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ë¡œê¹…ë§Œ

                    # ì €ë¶€í•˜ ìƒí™©: í’€ í¬ê¸° ê°ì†Œ
                    elif avg_utilization < 0.3 and current_size > self.config.min_pool_size:
                        new_size = max(current_size - 1, self.config.min_pool_size)
                        logger.info(f"Decreasing pool size: {current_size} -> {new_size}")

            except Exception as e:
                logger.error(f"Adaptive pool management error: {e}")

    def _calculate_cache_hit_rate(self) -> float:
        """ìºì‹œ íˆíŠ¸ìœ¨ ê³„ì‚°"""
        total_cache_requests = self.metrics.cache_hits + self.metrics.cache_misses
        if total_cache_requests == 0:
            return 0.0
        return self.metrics.cache_hits / total_cache_requests

    @asynccontextmanager
    async def get_session(self, query_type: QueryType = QueryType.READ):
        """
        ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €

        Args:
            query_type: ì¿¼ë¦¬ íƒ€ì… (ì½ê¸°/ì“°ê¸° ë¶„ì‚°ì„ ìœ„í•´)
        """
        # ì½ê¸° ì¿¼ë¦¬ì´ê³  ë³µì œë³¸ì´ ìˆëŠ” ê²½ìš° ë³µì œë³¸ ì‚¬ìš©
        if query_type == QueryType.READ and self.replica_engines and len(self.replica_engines) > 0:

            # ë¼ìš´ë“œ ë¡œë¹ˆìœ¼ë¡œ ë³µì œë³¸ ì„ íƒ
            replica_engine = self.replica_engines[
                self.metrics.total_queries % len(self.replica_engines)
            ]
            session_maker = sessionmaker(
                bind=replica_engine, class_=AsyncSession, expire_on_commit=False
            )
        else:
            session_maker = self.session_maker

        session = session_maker()
        start_time = time.time()

        try:
            yield session

            # ì„±ê³µ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            execution_time = time.time() - start_time
            self.metrics.successful_queries += 1
            self.metrics.update_execution_time(execution_time)

            # ëŠë¦° ì¿¼ë¦¬ ë¡œê¹…
            if execution_time > self.config.slow_query_threshold:
                logger.warning(f"Slow query detected: {execution_time:.3f}s")

        except Exception as e:
            # ì‹¤íŒ¨ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.metrics.failed_queries += 1
            logger.error(f"Database session error: {e}")

            # íŠ¸ëœì­ì…˜ ë¡¤ë°±
            try:
                await session.rollback()
            except:
                pass

            raise DatabaseError(f"Database operation failed: {e}")

        finally:
            await session.close()

    async def execute_cached_query(
        self, query: str, params: Dict[str, Any] = None, cache_key: str = None, ttl: int = None
    ) -> Any:
        """
        ìºì‹œëœ ì¿¼ë¦¬ ì‹¤í–‰

        Args:
            query: SQL ì¿¼ë¦¬
            params: ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°
            cache_key: ìºì‹œ í‚¤ (ìë™ ìƒì„± ê°€ëŠ¥)
            ttl: ìºì‹œ TTL (ì´ˆ)
        """
        if not self._cache_enabled or not self.query_cache:
            # ìºì‹œê°€ ë¹„í™œì„±í™”ëœ ê²½ìš° ì§ì ‘ ì‹¤í–‰
            async with self.get_session(QueryType.READ) as session:
                result = await session.execute(text(query), params or {})
                return result.fetchall()

        # ìºì‹œ í‚¤ ìƒì„±
        if cache_key is None:
            cache_content = query + str(sorted((params or {}).items()))
            cache_key = f"query:{hashlib.md5(cache_content.encode()).hexdigest()}"

        try:
            # ìºì‹œì—ì„œ ì¡°íšŒ
            cached_result = await self.query_cache.get(cache_key)
            if cached_result:
                self.metrics.cache_hits += 1
                return json.loads(cached_result)

            # ìºì‹œ ë¯¸ìŠ¤: ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒ
            self.metrics.cache_misses += 1
            async with self.get_session(QueryType.READ) as session:
                result = await session.execute(text(query), params or {})
                rows = result.fetchall()

                # ê²°ê³¼ë¥¼ ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
                serializable_rows = [
                    {column: value for column, value in row._mapping.items()} for row in rows
                ]

                # ìºì‹œì— ì €ì¥
                await self.query_cache.setex(
                    cache_key,
                    ttl or self.config.cache_ttl_seconds,
                    json.dumps(serializable_rows, default=str),
                )

                return serializable_rows

        except Exception as e:
            logger.error(f"Cached query execution failed: {e}")
            # ìºì‹œ ì˜¤ë¥˜ ì‹œ ì§ì ‘ ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ
            async with self.get_session(QueryType.READ) as session:
                result = await session.execute(text(query), params or {})
                return result.fetchall()

    async def execute_bulk_operation(
        self, operations: List[Tuple[str, Dict[str, Any]]], chunk_size: int = 1000
    ) -> bool:
        """
        ëŒ€ìš©ëŸ‰ ë²Œí¬ ì‘ì—… ì‹¤í–‰

        Args:
            operations: (ì¿¼ë¦¬, íŒŒë¼ë¯¸í„°) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            chunk_size: ì²­í¬ í¬ê¸°
        """
        try:
            async with self.get_session(QueryType.BULK) as session:
                async with session.begin():
                    # ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
                    for i in range(0, len(operations), chunk_size):
                        chunk = operations[i : i + chunk_size]

                        for query, params in chunk:
                            await session.execute(text(query), params)

                        # ì¤‘ê°„ ì»¤ë°‹ (í° íŠ¸ëœì­ì…˜ ë°©ì§€)
                        if i + chunk_size < len(operations):
                            await session.commit()
                            await session.begin()

                        logger.debug(f"Processed bulk chunk: {i + len(chunk)}/{len(operations)}")

            logger.info(f"Bulk operation completed: {len(operations)} operations")
            return True

        except Exception as e:
            logger.error(f"Bulk operation failed: {e}")
            raise DatabaseError(f"Bulk operation failed: {e}")

    async def get_performance_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        stats = {
            "total_queries": self.metrics.total_queries,
            "successful_queries": self.metrics.successful_queries,
            "failed_queries": self.metrics.failed_queries,
            "average_execution_time": self.metrics.average_execution_time,
            "slowest_query_time": self.metrics.slowest_query_time,
            "cache_hit_rate": self._calculate_cache_hit_rate(),
            "active_connections": self.active_connections,
            "peak_connections": self.peak_connections,
        }

        if self.primary_engine:
            pool = self.primary_engine.pool
            stats.update(
                {
                    "pool_size": pool.size(),
                    "checked_out_connections": pool.checkedout(),
                    "overflow_connections": pool.overflow(),
                }
            )

        return stats

    async def close(self):
        """ì—°ê²° í’€ ì¢…ë£Œ"""
        try:
            # ëª¨ë‹ˆí„°ë§ íƒœìŠ¤í¬ ì¤‘ë‹¨
            if self._monitoring_task:
                self._monitoring_task.cancel()
                try:
                    await self._monitoring_task
                except asyncio.CancelledError:
                    pass

            if self._adaptive_task:
                self._adaptive_task.cancel()
                try:
                    await self._adaptive_task
                except asyncio.CancelledError:
                    pass

            # ì—”ì§„ ì¢…ë£Œ
            if self.primary_engine:
                await self.primary_engine.dispose()

            for replica_engine in self.replica_engines:
                await replica_engine.dispose()

            # ìºì‹œ ì—°ê²° ì¢…ë£Œ (ë³„ë„ë¡œ ìƒì„±í•œ ê²½ìš°ë§Œ)
            if self.query_cache and not (
                hasattr(get_state_manager(), "_redis_client")
                and get_state_manager()._redis_client == self.query_cache
            ):
                await self.query_cache.close()

            logger.info("Database connection pool closed")

        except Exception as e:
            logger.error(f"Error closing connection pool: {e}")


# ì „ì—­ ì—°ê²° í’€ ì¸ìŠ¤í„´ìŠ¤
_connection_pool: Optional[AdaptiveConnectionPool] = None


async def get_connection_pool() -> AdaptiveConnectionPool:
    """ì—°ê²° í’€ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _connection_pool
    if _connection_pool is None:
        config = DatabaseConfig()  # ì‹¤ì œë¡œëŠ” ì„¤ì •ì—ì„œ ë¡œë“œ
        _connection_pool = AdaptiveConnectionPool(config)
        await _connection_pool.initialize()
    return _connection_pool


# í¸ì˜ í•¨ìˆ˜ë“¤
async def execute_query(
    query: str,
    params: Dict[str, Any] = None,
    cached: bool = True,
    query_type: QueryType = QueryType.READ,
) -> Any:
    """ì¿¼ë¦¬ ì‹¤í–‰ í¸ì˜ í•¨ìˆ˜"""
    pool = await get_connection_pool()

    if cached and query_type == QueryType.READ:
        return await pool.execute_cached_query(query, params)
    else:
        async with pool.get_session(query_type) as session:
            result = await session.execute(text(query), params or {})

            if query_type == QueryType.READ:
                return result.fetchall()
            else:
                await session.commit()
                return result.rowcount


async def execute_transaction(operations: List[Tuple[str, Dict[str, Any]]]) -> bool:
    """íŠ¸ëœì­ì…˜ ì‹¤í–‰ í¸ì˜ í•¨ìˆ˜"""
    pool = await get_connection_pool()
    async with pool.get_session(QueryType.TRANSACTION) as session:
        async with session.begin():
            for query, params in operations:
                await session.execute(text(query), params)
            await session.commit()
    return True


@asynccontextmanager
async def get_db_session(query_type: QueryType = QueryType.READ):
    """ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € í¸ì˜ í•¨ìˆ˜"""
    pool = await get_connection_pool()
    async with pool.get_session(query_type) as session:
        yield session
